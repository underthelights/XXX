<h1 id="14-reinforcement-learning">14. Reinforcement Learning</h1>

<ul>
  <li>Sutton RL</li>
</ul>

<h1 id="characteristics-of-reinforcement-learning">Characteristics of Reinforcement Learning</h1>

<p><img src="14/Untitled.png" alt="Untitled" /></p>

<ul>
  <li>What makes reinforcement learning different from other machine learning paradigms?
    <ul>
      <li>supervised l. vs unsupervised l. vs. RL
        <ul>
          <li>supervised : label + data</li>
          <li>Unsupervised : just use given data</li>
          <li>RL : data + reward - Reward에 해당하는 추가적인 input이 존재함</li>
        </ul>
      </li>
    </ul>

    <p>→ There is no supervisor, only a reward signal</p>
  </li>
  <li>Feedback is delayed, not instantaneous</li>
  <li>Time really matters (sequential, non i.i.d. data)
    <ul>
      <li>시간이 중요한 요소 중 하나</li>
      <li>sequential : 전반의 선택이 후반의 선택에 영향
  iid = independent identically distributed - 상호 연관</li>
    </ul>
  </li>
  <li>Agent’s actions affect the subsequent data it receives
    <ul>
      <li>agent action이 이후 data에 영향을 미친다.</li>
    </ul>
  </li>
</ul>

<h2 id="examples-of-reinforcement-learning">Examples of Reinforcement Learning</h2>

<ul>
  <li>Fly stunt manoeuvres in a helicopter
    <ul>
      <li>헬리콥터의 비행 모형</li>
    </ul>
  </li>
  <li>Defeat the world champion at Backgammon
    <ul>
      <li>backgammon 게임에서의 응용</li>
    </ul>
  </li>
  <li>Manageaninvestmentportfolio</li>
  <li>Controlapowerstation</li>
  <li>Makeahumanoidrobotwalk</li>
  <li>Play many different Atari games better than humans
    <ul>
      <li>로봇, 투자 포트폴리오, 아타리 게임에서의 학습</li>
    </ul>

    <p><img src="14/Untitled_1.png" alt="Untitled" /></p>

    <p><img src="14/Untitled_2.png" alt="Untitled" /></p>
  </li>
</ul>

<h1 id="rewards">Rewards</h1>

<ul>
  <li>A reward 𝒕 is a scalar feedback signal</li>
  <li>Indicate show well agent is doing at step t &amp; The agent’s job is to maximize cumulative reward</li>
  <li>각각의 시간에 얼마나 잘 행동 했는지 보고 reward 최대화되는 방향으로 행동하도록 학습</li>
  <li>Reinforcementlearning is based on the reward hypothesis
    <ul>
      <li>reward = 사람이 만든 기준
  ex. Atari game : target 별 최대한의 점수를 학습할 수 있도록 학습이 되기도 함. 점수가 많은 쪽을 더 빨리 얻을 수 있도록 학습시키는 양상이 생길 수 있다,</li>
      <li>Reward hypothesis: all goals can be described by the m<strong>aximization of expected cumulative reward</strong></li>
    </ul>
  </li>
</ul>

<h2 id="examples-of-rewards">Examples of Rewards</h2>

<ul>
  <li>
    <p>Fly stunt manoeuvres in a helicopter</p>

    <p>(+) : 원하는 궤적을 그리며 날아갈 때
  (-) : crashing 시 마이너스 ㅎ과</p>

    <ul>
      <li>+ve reward for following desired trajectory</li>
      <li>−ve reward for crashing</li>
    </ul>
  </li>
  <li>Defeat the world champion at Backgammon
    <ul>
      <li>+/−ve reward for winning/losing a game</li>
    </ul>
  </li>
  <li>
    <p>Manage an investment portfolio</p>

    <p>(+) : 원하는 이익
  (-) : 손실</p>

    <ul>
      <li>+ve reward for each $ in bank</li>
    </ul>
  </li>
  <li>
    <p>Control a power station</p>

    <p>(+) : 적절한 전력 공급
  (-) :</p>

    <ul>
      <li>+ve reward for producing power</li>
      <li>−ve reward for exceeding safety thresholds</li>
    </ul>
  </li>
  <li>
    <p>Make a humanoid robot walk</p>

    <p>(+) : 주어진 환경에서 target 물질을 확보에서 mission 잘 수행
  (-) : 넘어짐</p>

    <ul>
      <li>+ve reward for forward motion</li>
      <li>−ve reward for falling over</li>
    </ul>
  </li>
  <li>
    <p>Play many different Atari games better than humans</p>

    <p>(+) : 점수 얻거나
  (-) : 점수 잃거나
  -&gt; 빠른 시간 안에 점수를 많이 얻는 방향으로</p>

    <ul>
      <li>+/−ve reward for increasing/decreasing score</li>
    </ul>
  </li>
</ul>

<h1 id="sequential-decision-making">Sequential Decision Making</h1>

<ul>
  <li>현재의 action이 다음 턴 action에 영향을 미치는데, 오랜 turn에 대해 영향을 끼칠수도 있음.</li>
  <li>Goal: select actions to maximize total future reward
    <ul>
      <li>일련의 행동에 따른 reward가 최대가 되도록 학습한다</li>
    </ul>
  </li>
  <li>Actions may have long term consequences
    <ul>
      <li>state가 있고 action을 취해서 s1-(a1)-&gt;s2-(a2)-&gt;s3</li>
    </ul>
  </li>
  <li>Reward may be delayed reward는 delay를 수반하여 주어질 수 있다</li>
  <li>현재 action으로 인한 reward에 더 중점을 둘 것인지, 미래의 reward에 중점을 더 둘 것인지 : user setting할 수도 있고 학습 단계에서 어떻게 parameter를 설정했는지에 따라 / 학습이 잘 효과적으로 이루어질수 있는지를 고려하여 모수 조정
    <ul>
      <li>(greedy) 현재 reward에 초점을 맞추는 경우 - current reward</li>
      <li>(optimal) 전체 reward에 초점을 맞추는 경우 - total reward
        <ul>
          <li>Itmay be better to sacrifice immediate reward to gain more long-term reward (greedy optimal)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Examples:</p>

    <p>Ex.
  -투자 :당장은 손해가 나더라도 미래 시점에 수익
  -헬리콥터 주행 중 연료 주입 : crash하면 negative penalty하기에 현재로서는 reward 줄지만 optimal하게는 늘어나는 reward
  -체스에서 상대방 이동 : 본인 점수 취하는 것보다 상대방 방해가 전체적으로 더 이득일수도 있는 경우</p>

    <ul>
      <li>A financial investment (may take months to be mature)</li>
      <li>Refueling a helicopter (might prevent a crash in several hours)</li>
      <li>Blocking opponent moves (might help winning chances many moves from now)</li>
    </ul>
  </li>
</ul>

<h1 id="agent-and-environment">Agent and Environment</h1>

<p><img src="14/Untitled_3.png" alt="Untitled" /></p>

<ul>
  <li>At each stept the agent: agent가 주변을 관찰하고, reward를 받아 action을 취함
    <ul>
      <li>Executes action At</li>
      <li>Receives observation Ot</li>
      <li>Receives scalar reward Rt</li>
    </ul>
  </li>
</ul>

<p><img src="14/Untitled_4.png" alt="Untitled" /></p>

<ul>
  <li>The environment:
    <ul>
      <li>Receives action At</li>
      <li>Emits observation Ot+1</li>
      <li>Emits scalar reward Rt+1</li>
    </ul>
  </li>
  <li>t increments at env. step</li>
</ul>

<p>&lt;agent, environment의 상호작용&gt;
agent는 action을 취하고 state에 따라 Reward를 받게 됨
env는 action을 받아들여서 agent에게 주고 변환된 statement를 agent에게 줌</p>

<ul>
  <li>t타임으로 이루어지는 요소들</li>
</ul>

<p>action에 대해서 reward와 statement의 변화</p>

<p><img src="14%20Reinforcement%20Learning%20e7de39297aeb4f5dab69c0ae48410224/IMG_0328.jpg" alt="IMG_0328.jpg" /></p>

<h1 id="major-components-of-an-rl-agent">Major Components of an RL Agent</h1>

<ul>
  <li>An RL agent may include one or more of these components:
    <ul>
      <li>Policy: agent’s behavior function 행동 정의</li>
      <li>Value function: how good is each state and/or action 얼마나 좋은가</li>
      <li>Model: agent’s representation of the environment  학습 모델</li>
    </ul>
  </li>
</ul>

<h2 id="example---maze">Example - Maze</h2>

<p><img src="14/Untitled_5.png" alt="Untitled" /></p>

<ul>
  <li>Agent: explores environment and gets reward</li>
  <li>Environment: agent 돌아다니는 환경 situation being explored by the agent</li>
  <li>States: 위치 - positions/locations in the environment</li>
  <li>Actions: 상하좌우 - allowed movements for the agent</li>
  <li>Reward: what agent gets as it moves</li>
</ul>

<p><img src="14/Untitled_6.png" alt="Untitled" /></p>

<ul>
  <li>
    <p>For example, bomb has reward -10, germ has reward 10, every other move has rewards -1</p>

    <p>→ 불필요한 이동을 최소화시키기 위한 장치</p>
  </li>
</ul>

<p>s6 is blocked</p>

<p><img src="14/Untitled_7.png" alt="Untitled" /></p>

<p><img src="14/Untitled_8.png" alt="Untitled" /></p>

<h1 id="bellman-equation">Bellman equation</h1>

\[V(s) = max_a(R(s,a) + \gamma V(s'))\]

<ul>
  <li>$R(s,a)$ : reward: state에서 취한 action에 따른 reward</li>
  <li>$V(s)$ : is the value function - value function:전체 reward 를 어떻게 표현할 것인가</li>
  <li>$\gamma$ : is the discounting factor
    <ul>
      <li>현재-미래 reward중 어느 것에 초점을 맞출 것인지 중요도 맞추는 상수</li>
    </ul>
  </li>
  <li>$s’$ : is the next state agent can go from
    <ul>
      <li>s : 현재 state, s’ : next state</li>
    </ul>
  </li>
  <li>Bellman equation is used to calculate the value function 
→ 각 state에 대한 value function값으로 주어지게 됨 : 환경이 바뀌면 결과가 바뀌게 됨
R(s,a) : current reward
V(s’) : all futer reward</li>
  <li>일반적인 규칙:폭탄,보석이 있고 env가 달려젔을 때 학습을 더 잘 할것인가-&gt;bellman eq로 value fn으로 하는거는 환경 바뀌면 다시 적용해야 함</li>
</ul>

<p><img src="14/Untitled_9.png" alt="Untitled" /></p>

<ul>
  <li>T calculate V(1) ,consider a path s1- s2-s3-s7-s11-s12</li>
</ul>

<p>s1에 대해 가장 큰 state function의 결과를 만드는 값을 취하도록 했다.</p>

<ul>
  <li>(assume $\gamma = 1$)
    <ul>
      <li>V(1) = R(s1, →) + V(2) = -1+V(2)</li>
      <li>V(2) = R(s2, →) + V(3) = -1+V(3)</li>
      <li>V(3) = R(s3, $\downarrow$) + V(7) = -1+V(7)</li>
      <li>V(7) = R(s7, $\downarrow$) + V(11) = -1+V(11)</li>
      <li>V(11) = R(s11, →) + V(12) = -1+V(12)</li>
    </ul>
  </li>
  <li>Since V(12) = 10
    <ul>
      <li>We can get V(11)=9, V(7)=8, V(3)=7, V(2) = 6, V(1) = 5</li>
    </ul>
  </li>
  <li>We can consider other path, s1-s2-s3-s4- s3-s7-s11-s12 to calculate V(1), in which case V(1) will be less than 5 14</li>
</ul>

<p><img src="14/Untitled_10.png" alt="Untitled" /></p>

\[V(s) = max_a(R(s,a) + \gamma V(s'))\]

<ul>
  <li>By calculating V(s) for all states
    <ul>
      <li>Agent can move to the state with larger state value</li>
    </ul>
  </li>
  <li>임의의 출발점에서 state function 커지는 쪽으로 action을 취하면 된다
→ equation을 이용해서 value funcition을 구한후 최적의 path를 구할 수 있다</li>
</ul>

<h1 id="12-clustering">12. Clustering</h1>

<h1 id="clustering">Clustering</h1>

<ul>
  <li>Unsupervised learning</li>
  <li>Requires data, but no labels
    <ul>
      <li>label이 없다</li>
    </ul>
  </li>
  <li>Detect patterns e.g. in
    <ul>
      <li>data pattern을 인식하려고 한다 : email / 검색 결과 / 쇼핑 패턴 / 이미지 리전으로부터</li>
    </ul>
  </li>
  <li>Useful when don’t know what you’re looking for Clustering
    <ul>
      <li>일단 clustering 수행하고 나면 해당 data가 어떻게 이루어지는지는 알 수 있다</li>
    </ul>
  </li>
  <li>Basic idea: group together similar instances</li>
  <li>Example: 2D point patterns Clustering</li>
</ul>

<p><img src="12/Untitled.png" alt="Untitled" /></p>

<p><img src="12/Untitled_1.png" alt="Untitled" /></p>

<p><img src="12/Untitled_2.png" alt="Untitled" /></p>

<ul>
  <li>What could “similar” mean?
    <ul>
      <li>
        <p>One option: small Euclidean distance</p>

        <p>‘ similarity’를 어떻게 판단할 수 있는가</p>

        <ul>
          <li>2 dim 공간 상 거리 -&gt; Euclidean distance
  Similarity 는 distance에 반비례</li>
        </ul>
      </li>
      <li>
        <p>Clustering results are crucially dependent on the measure of similarity (or distance) between “points” to be clustered</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="12/Untitled_3.png" alt="Untitled" /></p>

<ul>
  <li>Partition algorithms
    <ul>
      <li>K-means</li>
      <li>Mixture of Gaussian</li>
      <li>Spectral clustering</li>
    </ul>
  </li>
  <li>Hierarchical algorithms
    <ul>
      <li>Bottom up : agglomerative</li>
      <li>Top down : divisive</li>
    </ul>
  </li>
</ul>

<h2 id="clustering-examples">Clustering Examples</h2>

<ul>
  <li>Image segmentation
    <ul>
      <li>Image segmentation : 인접한 유사한 feature를 가진 pixel 끼리 group으로 분할</li>
      <li>Goal: Break up the image into meaningful or perceptually similar regions</li>
    </ul>

    <p><img src="12/Untitled_4.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>Clustering gene data</p>

    <p>Hierarchical clustering : gene group에 큰 두 개의 gruop이 있음을 판단할 수 있다.</p>

    <ul>
      <li>나중에 들어오는 data에 대해서도 두 가지 class 대상으로 labeling할 수도 있고 두 가지 label로 supervised learning도 가능</li>
    </ul>

    <p><img src="12/Untitled_5.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>Cluster news articles</p>

    <p><img src="12/Untitled_6.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>Cluster people by space and time</p>

    <p><img src="12/Untitled_7.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>Cluster languages</p>

    <p><img src="12/Untitled_8.png" alt="Untitled" /></p>

    <p><img src="12/Untitled_9.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>Cluster species (phylogeny)</p>

    <p><img src="12/Untitled_10.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>Cluster search queries</p>

    <p><img src="12/Untitled_11.png" alt="Untitled" /></p>
  </li>
</ul>

<h1 id="k-means-clustering">K-Means Clustering</h1>

<p><img src="12/Untitled_12.png" alt="Untitled" /></p>

<p>An iterative clustering algorithm</p>

<ul>
  <li>
    <p>Initialize: Pick K random points as cluster centers</p>

    <ol>
      <li>cluster의 개수 가정 : k개의 cluster = k개의 random point 초깃값 assume</li>
    </ol>
  </li>
  <li>Alternate:
    <ol>
      <li>
        <p>Assign data points to closest cluster center</p>

        <ol>
          <li>k개의 pt에 대해서 각각의 data들을 가장 가까운 cluster center에 할당하고</li>
        </ol>
      </li>
      <li>
        <p>Change the cluster center to the average of its assigned points</p>

        <ol>
          <li>Cluster center update : 각 iteration마다 update</li>
        </ol>

        <ul>
          <li>더 이상 update되지 않는 시점에서 cluster 중단</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p>Stop when no point assignments change</p>

    <p>초깃값에 대해서 (initial center point) partition 나눔</p>

    <ul>
      <li>그 data들의 center mean을 구해서 이로 center를 update함</li>
      <li>이를 기반으로 update된 값들의 member들을 재할당</li>
      <li>반복하다 보면 각각의 cluster의 center로 이동하게 됨</li>
    </ul>
  </li>
</ul>

<p><img src="12/Untitled_13.png" alt="Untitled" /></p>

<h2 id="k-means-clustering-example">K-Means Clustering: Example</h2>

<p><img src="12/Untitled_14.png" alt="Untitled" /></p>

<ul>
  <li>Pick K random points as cluster centers (means)</li>
  <li>Shown here for K=2</li>
</ul>

<h3 id="iterative-step-1">Iterative Step 1</h3>

<ul>
  <li>Assign data points to closest cluster center
    <ul>
      <li>mean값에 대해서 다시 partition./ center값 할당하며 반복하게 되면
  K means clustering 작업에 의한 clustering 결과를 얻게 됨.</li>
    </ul>
  </li>
</ul>

<p><img src="12/Untitled_15.png" alt="Untitled" /></p>

<h3 id="iterative-step-2">Iterative Step 2</h3>

<ul>
  <li>Change the cluster center to the average of the assigned points</li>
</ul>

<p><img src="12/Untitled_16.png" alt="Untitled" /></p>

<h2 id="properties-of-k-means-algorithm">Properties of K-means algorithm</h2>

<ul>
  <li>
    <p>Guaranteed to converge in a finite number of iterations</p>

    <p>k값이 대략적인 cluster개수와 일치해야지, 그렇지 않으면 만족하지 못할 결과가 나올 수도 있음</p>
  </li>
  <li>
    <p>Running time per iteration:</p>
    <ol>
      <li>
        <p>Assign data points to closest cluster center : O(KN)</p>

        <p>k개의 point, n samples -&gt; kn</p>
      </li>
      <li>
        <p>Change the cluster center to the average of its assigned points : O(N)</p>

        <p>Cluster center average -&gt; n</p>
      </li>
    </ol>
  </li>
</ul>

<h2 id="properties-of-k-means-algorithm-convergence">Properties of K-means algorithm (convergence)</h2>

<ul>
  <li>
    <p>Objective</p>

    <p>모든 sample들에 대해서 center값을 계산 :sample들의 center 값으로부터의 거리 제곱의 합이
  k개의 cluster에 대해서 최소</p>

    <ul>
      <li>cluster center에 잘 일치하게 되면, 해당 cluster에 속하는 sample들이 그 cluster center 와 이루는 거리의 합이 모든 cluster에 대해서 minimum이 됨</li>
    </ul>

\[\min_{\mu} \min_{c}  \Sigma_{i=1}^{k} {\Sigma_{x\in C_i}|x-\mu_i|^2}\]
  </li>
  <li>
    <p>Fix $\mu$, optimize $C$</p>

    <p>iteration작업- partition update</p>

    <ul>
      <li>mu 고정, C optimize</li>
      <li>center까지의 모든 sample들의 거리를 최소화하는 작업</li>
    </ul>

\[\min_{c}  \Sigma_{i=1}^{k} {\Sigma_{x\in C_i}|x-\mu_i|^2} = \min_{c}  \Sigma_{i=1}^{n} {|x_i-\mu_{xi}|^2}\]
  </li>
  <li>
    <p>Fix $\mu$, optimize $\mu$</p>

    <p>Partition update후 center값 update</p>

    <ul>
      <li>각 cluster sample들에서 sample들로부터 center 거리를 최소화시킬 수 있도록 평균값 설정</li>
      <li>mu에 대해서 미분하고ㅡ 이를 0으로 setting하면 평균값이라는 것은 각 cluster에 속한 sample들의 값의 합을 그 cluster에 속한 sample들의 값의 합을 그 cluster에 속한 sample들의 개수로 나눈 값이 그 center 값 : center mean</li>
    </ul>

\[\min_{\mu} \Sigma_{i=1}^{k} {\Sigma_{x\in C_i}|x-\mu_i|^2}\]

    <ul>
      <li>Take partial derivative with respect to $\mu_i$and sets to zero, we have $\mu_i = \frac 1 {C_i} \Sigma_{x\in C_i} x$</li>
    </ul>
  </li>
  <li>
    <p>K-means takes an alternating optimization, each step is guaranteed to decrease the objective – thus guaranteed to converge</p>
  </li>
</ul>

<h2 id="example-k-means-for-image-segmentation">Example: K-Means for Image Segmentation</h2>

<p><img src="12/Untitled_17.png" alt="Untitled" /></p>

<ul>
  <li>
    <p>Goal of image segmentation is to partition an image into regions each of which has reasonably homogenous visual appearance.</p>

    <p>K means clustering segmentation</p>

    <ul>
      <li>clustering : space상에서 한 게 아니고 color값들이 r,g,b의 값을 갖는데 256^3 의 dimension 3 histogram</li>
      <li>k개의 color로 clustering</li>
    </ul>
  </li>
</ul>

<h1 id="k-means-algorithm---initialization">K-means algorithm - initialization</h1>

<p>K-means algorithm is a heuristic</p>

<ul>
  <li>converging하기도 하고 완전 heuristic하지는 않고 완전 이상한 initial value로 하면 아예 못 구하기도 함</li>
  <li>Requires initial means
    <ul>
      <li>초기 center값에 따라 최종 결과가 잘못될수도 있음</li>
      <li>It does matter what you pick!</li>
    </ul>
  </li>
  <li>What can go wrong?</li>
  <li>
    <p>Various schemes for preventing this kind of thing: variance-based split / merge, initialization heuristics</p>
  </li>
  <li>
    <p>A local optimum</p>

    <p><img src="12/Untitled_18.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>Would be better to have one cluster here</p>

    <p><img src="12/Untitled_19.png" alt="Untitled" /></p>
  </li>
</ul>

<h1 id="k-means-algorithm">K-means algorithm</h1>

<ul>
  <li>Not able to cluster properly</li>
</ul>

<p><img src="12/Untitled_20.png" alt="Untitled" /></p>

<p><img src="12/Untitled_21.png" alt="Untitled" /></p>

<ul>
  <li>Changing the features (distance function) can help</li>
</ul>

<h1 id="clustering-hierarchical-clustering">Clustering Hierarchical Clustering</h1>

<p>Two main types of hierarchical clustering</p>

<ul>
  <li>Agglomerative: 각각의 개별 sample로 시작해서 cluster로 간주하여 각각을 merge하여 최종적으로 1~k개의 cluster가 되기까지 merge
    <ul>
      <li>Start with the points as individual clusters</li>
      <li>At each step, merge the closest pair of clusters until only one cluster (or k clusters) left</li>
    </ul>
  </li>
  <li>Divisive:하나에서 개별 cluster로 divide
    <ul>
      <li>Start with one, all-inclusive cluster</li>
      <li>At each step, split a cluster until each cluster contains a point (or there are k clusters)</li>
    </ul>
  </li>
  <li>
    <p>Traditional hierarchical algorithms use a similarity or distance matrix</p>

    <p>similarity와 distance는 서로 역수 관계</p>

    <ul>
      <li>
        <p>Merge or split one cluster at a time</p>

        <p>Merge/split하는 방식을 hierarchical clustering이라고 함</p>
      </li>
    </ul>
  </li>
</ul>

<h1 id="agglomerative-clustering">Agglomerative clustering</h1>

<ul>
  <li>supervised : data label 제공 / unsupervised : data label 미제공
    <ul>
      <li>→ labeling해서 classification, regression 수행</li>
    </ul>
  </li>
  <li>clustering : hierarchical process로 data 분석하기도 하고</li>
</ul>

<p>이상 전기 신호를 어떻게 구분할 것인가 (classification) 해야 하는데 이에 대한 label이 없음</p>

<p>몇 년치 data를 labeling하기도, 분석도 바로 못 들어감.</p>

<p>→ 평균치보다 크거나 작은 값을 군집화하여 이상신호를 보고 판단하고 labeling 수작업</p>

<p>정상과 비정상의 label에 대하여 이후 들어오는 data들에 대하여 수작업 (supervised)</p>

<ul>
  <li>Agglomerative clustering
    <ul>
      <li>First merge very similar instances
        <ul>
          <li>비슷한 data끼리 grouping : 처음에는 모든 data pt가 개별 cluster로 되고 점차 하나의 group이 될 때까지 group화</li>
        </ul>
      </li>
      <li>Incrementally build larger clusters out of smaller clusters
        <ul>
          <li>가장 similarity가 큰 data끼리 group화함 :
            <ul>
              <li>1,3을 하나의 group으로 묶고 2, 5 group으로 묶이게 되면 1st-2nd group간 거리는 가장 가까운 거리로 할 것인지 (1-2) 먼 거리로 할 것인지 (3-5) 평균으로 할 것인지에 따라, similarity 판단 기준에 따라 clustering이 다르게 됨</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Algorithm:
    <ul>
      <li>Maintain a set of clusters, Initially, each instance in its own cluster</li>
      <li>Repeat:
        <ul>
          <li>Pick the two closest clusters</li>
          <li>Merge them into a new cluster</li>
          <li>Stop when there’s only one cluster left</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Produces not one cluster, but a family of clusters represented by a dendrogram</li>
</ul>

<p><img src="12/Untitled_22.png" alt="Untitled" /></p>

<ul>
  <li>How should we define “close” for clusters with multiple elements?
    <ul>
      <li>similarity를 어떻게 판단할 것인가  =(distance를 어떻게 계산할 것인가)</li>
      <li>
        <p>closest / Farthest / Average → cluster가 달라지게 됨</p>

        <p><img src="12/Untitled_23.png" alt="Untitled" /></p>
      </li>
    </ul>
  </li>
  <li>Many options:
    <ul>
      <li>Closest pair (single-link clustering)</li>
      <li>Farthest pair (complete-link clustering)</li>
      <li>Average of all pairs</li>
    </ul>
  </li>
  <li>Different choices create different clustering behaviors</li>
</ul>

<h2 id="strengths-of-hierarchical-clustering">Strengths of Hierarchical Clustering</h2>

<ul>
  <li>No assumptions on the number of clusters
    <ul>
      <li>cluster 개수 가정하고 수행하게 됨</li>
      <li>단) cluster 개수 잘못 예측하면 algorithm 좋지 않은 결과를 낼 것</li>
      <li>Any desired number of clusters can be obtained by ‘cutting’ the dendogram at the proper level</li>
    </ul>
  </li>
  <li>Hierarchical clusterings may correspond to meaningful taxonomies
    <ul>
      <li>cluster 몇 개인지 모르는 상태에서 clustering</li>
      <li>clustering : dendrogram 그려서 duration 긴 구간을 판단하여 cluster 개수 결정 / 적절한 상태에서 dendrogram cutting</li>
      <li>대칭적 분류 : phylogeny. Catalog</li>
      <li>Example in biological sciences (e.g., phylogeny reconstruction, etc), web (e.g., product catalogs), etc.</li>
    </ul>
  </li>
</ul>

<h2 id="complexity-of-hierarchical-clustering">Complexity of hierarchical clustering</h2>

<ul>
  <li>Distance matrix is used for deciding which clusters to merge/split
    <ul>
      <li>distance &lt;-&gt; proximity</li>
    </ul>
  </li>
  <li>data point n개가 있다면, n by n개 distance를 모두 계산함 : n^2 연산
    <ul>
      <li>At least quadratic in the number of data points
        <ul>
          <li>Not usable for large datasets</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>group화가 진행되며 Matrix 크기가 점차 줄어들게 되어 최종적으로 하나의 block만 남게 됨</p>

<h1 id="agglomerative-clustering-algorithm">Agglomerative clustering algorithm</h1>

<p>Data point distance metric계산하고 각각의 data point가 개별 cluster로 여겨지고 Closest cluster끼리 merge되고 (방법에는 최단/최대/평균)</p>

<p>Compute the distance matrix between the input data points Let each data point be a cluster Repeat Merge the two closest clusters Update the distance matrix Until only a single cluster remains</p>

<ul>
  <li>Key operation is the computation of the distance between two clusters</li>
  <li>Different definitions of the distance between clusters lead to different algorithms Clustering</li>
  <li>Most popular hierarchical clustering technique</li>
  <li>Basic algorithm</li>
</ul>

<h2 id="input-initial-setting">Input/ Initial setting</h2>

<p><img src="12/Untitled_24.png" alt="Untitled" /></p>

<p>n Start with clusters of individual points and a distance/proximity matrix</p>

<h2 id="intermediate-state">Intermediate State</h2>

<ul>
  <li>After some merging steps, we have some clusters
    <ul>
      <li></li>
      <li>clustering이 진행되면 group화 진행</li>
      <li>처음에 data point 12개여서 12<em>12
  -&gt; clustering되어 5</em>5</li>
      <li>data dimension이 커지면 거리 계산에 어려움</li>
      <li>dimension별 scaling : K nearest neighbor에서 고려해야 함</li>
    </ul>
  </li>
</ul>

<p><img src="12/Untitled_25.png" alt="Untitled" /></p>

<ul>
  <li>
    <p>Merge the two closest clusters (C2 and C5) and update the distance matrix</p>

    <p><img src="12/Untitled_26.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>How do we update the distance matrix?</p>

    <p><img src="12/Untitled_27.png" alt="Untitled" /></p>
  </li>
</ul>

<h1 id="distance-between-two-clusters">Distance between two clusters</h1>

<ul>
  <li>Each cluster is a set of points</li>
  <li>How do we define distance between two sets of points</li>
  <li>Lots of alternatives</li>
  <li>
    <p>Not an easy task</p>
  </li>
  <li>Single-link distance between clusters Ci and Cj is the minimum distance between any object in Ci and any object in Cj
    <ul>
      <li>Single link : shortest distance</li>
    </ul>
  </li>
  <li>
    <p>The distance is defined by the two most similar objects</p>

\[D _{s l} ( C _i , C _j ) = \min _{x , y} (d ( x , y )| x \in C _i , y \in C _j )\]
  </li>
</ul>

<h1 id="single-link-clustering-example">Single-link clustering: example</h1>

<ul>
  <li>
    <p>Determined by one pair of points, i.e., by one link in the proximity graph</p>

    <p>Diagonal value를 보고 판단할 수 있음</p>

    <p>같은 요소에 대한 값이 1-&gt; similarity</p>

    <ul>
      <li>data 분포에 의하면 1,2,3,4,5 clustering</li>
      <li>symmetric : 대각선 아래 부분은 크게 의미가 없음</li>
      <li>값이 높은 순대로 먼저 cluster를 형성하게 됨</li>
    </ul>
  </li>
</ul>

<p><img src="12/Untitled_28.png" alt="Untitled" /></p>

<p><img src="12/Untitled_29.png" alt="Untitled" /></p>

<ul>
  <li>
    <p>Can handle non-elliptical shapes</p>

    <p><img src="12/Untitled_30.png" alt="Untitled" /></p>
  </li>
  <li>Sensitive to noise and outliers</li>
  <li>It produces long, elongated clusters</li>
</ul>

<p><img src="12/Untitled_31.png" alt="Untitled" /></p>

<h2 id="distance-between-two-clusters-1">Distance between two clusters</h2>

<ul>
  <li>Complete-link distance between clusters Ci and Cj is the maximum distance between any object in Ci and any object in Cj</li>
  <li>
    <p>The distance is defined by the two most dissimilar objects</p>

    <p>가장 먼 거리의 simple pair에 대해서 data를 clustering (Most dissimilar)</p>

    <p>Most similar</p>
  </li>
</ul>

\[D _{s l} ( C _i , C _j ) = \max _{x , y} (d ( x , y )| x \in C _i , y \in C _j )\]

<h2 id="complete-link-clustering-example">Complete-link clustering: example</h2>

<ul>
  <li>
    <p>Distance between clusters is determined by the two most distant points in the different clusters</p>

    <p>거리 판단한 이후(차이-dissimilar)
  cluster끼리 병합할 때는 가장 가까운 것 끼리 (동일)</p>
  </li>
</ul>

<p><img src="12/Untitled_32.png" alt="Untitled" /></p>

<p><img src="12/Untitled_33.png" alt="Untitled" /></p>

<ul>
  <li>
    <p>More balanced clusters (with equal diameter)</p>

    <p>Balance : separation이 조금 더 명확해졌고 cluster의 크기가 비슷하게 분류됐고, noise에 대해서 약간 덜 예민해짐</p>
  </li>
  <li>
    <p>Less susceptible to noise</p>
  </li>
</ul>

<p><img src="12/Untitled_34.png" alt="Untitled" /></p>

<ul>
  <li>Tends to break large clusters</li>
  <li>
    <p>All clusters tend to have the same diameter – small clusters are merged with larger ones</p>

    <p>큰 cluster를 작게 쪼개려고 하는 성질</p>

    <ul>
      <li>cluster크기가 커지면 다른 sample들과 계산하건 거리가 크게 나옴 : 큰 덩어리가 분할되는 결과를 가져옴</li>
      <li>전체 cluster들이 비슷한 결과</li>
      <li>작은 cluster들이 큰 것을 흡수</li>
    </ul>
  </li>
</ul>

<p><img src="12/Untitled_35.png" alt="Untitled" /></p>

<h1 id="distance-between-two-clusters-2">Distance between two clusters</h1>

<ul>
  <li>Group average distance between clusters Ci and Cj is the average distance between any object in Ci and any object in Cj
    <ul>
      <li>전체 data pair의 average 이용</li>
      <li>I cluster , j cluster : 평균 distance 계산한 후 shortest path 결정</li>
    </ul>
  </li>
</ul>

\[D_{avg} (C_i, C_j) = \frac 1 {|C_i| \times |C_j| } \Sigma_{x\in C_i, y \in C_j} d(x,y)\]

<h1 id="average-link-clustering-example">Average-link clustering: example</h1>

<ul>
  <li>Proximity of two clusters is the average of pairwise proximity between points in the two clusters.</li>
</ul>

<p><img src="12/Untitled_36.png" alt="Untitled" /></p>

<p>Average distance의 길이가 짧을수록 stable cluster</p>

<p><img src="12/Untitled_37.png" alt="Untitled" /></p>

<h1 id="average-link-clustering-discussion">Average-link clustering: discussion</h1>

<ul>
  <li>Compromise between Single and Complete Link</li>
  <li>Strengths
    <ul>
      <li>Less susceptible to noise and outliers</li>
    </ul>
  </li>
  <li>Limitations
    <ul>
      <li>Biased towards globular clusters 각각의 cluster를 구 형태로 만드려는 특성</li>
    </ul>
  </li>
</ul>

<h1 id="hierarchical-clustering-comparison">Hierarchical Clustering: Comparison</h1>

<ul>
  <li>최종 cluster는 동일</li>
  <li>중간 cluster에서는 계산 방식이 다르기 때문에 다른 형태</li>
</ul>

<p><img src="12/Untitled_38.png" alt="Untitled" /></p>

<h1 id="divisive-hierarchical-clustering">Divisive hierarchical clustering</h1>

<p>Top-down : global (optimal)</p>

<ul>
  <li>한 번에 하나의 cluster를 split : which to divide</li>
  <li>calculation 많아짐</li>
</ul>

<p>Bottom-up : local (heuristic, greedy)</p>

<ul>
  <li>개별 data를 각각의 cluster -&gt; 점차 Merge하며 하나의cluster</li>
  <li></li>
</ul>

<p>Divisive top down</p>

<ul>
  <li>detailed cluster로 split recursively</li>
  <li>사용할 dimension, variable의 개수 정도</li>
</ul>

<p>monotheistic : 한 번에 한 개</p>

<p>polytheistic : 여러 dim/var을 한 번에</p>

<p>(Univariate / multivariate)</p>

<ul>
  <li>distance measure metric : 수치 등의 기준</li>
</ul>

<p>1 : n-1 (n)/ 2: n-2 (nC2)/ … 경우의 수를 따져볼 수 있고 최종적으로 하나의 Cluster</p>

<ul>
  <li>가장 적합한 split을 선택</li>
  <li>모든 경우의수 따져보면 bottom up보다 더 많이 연산을 요구할 것인지는 차이가 있을 수 있음</li>
  <li>일반적으로 agglomerative bottom-up방식 사용 : dendrogram방식으로 다양한 단계에서의 Clustering을 수행하며 정보를 얻고 취합하여 최종 Cluster 결과 선정 가능</li>
  <li>
    <p>현재 상태의 cluster에서 가장 가짜운 Cluster과 Merge : optimal보다는 heuristic, Greedy (전체 objective를 최적화하는 global의 개념은 아님-너무 연산 많아짐)</p>
  </li>
  <li>Start with a single cluster composed of all data points</li>
  <li>Split this into components</li>
  <li>Continue recursively</li>
  <li>Monothetic divisive methods split clusters using one variable/dimension at a time</li>
  <li>Polythetic divisive methods make splits on the basis of all variables together</li>
  <li>Any intercluster distance measure can be used</li>
  <li>Computationally intensive, less widely used than agglomerative methods</li>
</ul>

<p><img src="12/Untitled_39.png" alt="Untitled" /></p>

<ul>
  <li>Initially, all points in the dataset belong to one single cluster</li>
  <li>Partition the cluster into two least similar cluster</li>
  <li>Proceed recursively to form new clusters until the desired number of clusters is obtained Clustering Divisive hierarchical 53</li>
</ul>

<p><img src="12/Untitled_40.png" alt="Untitled" /></p>

<ul>
  <li>Check the sum of squared errors of each cluster and choose the one with the largest value.</li>
  <li>In the dataset below, the data points are separated into 2 clusters</li>
  <li>For further separating it to form the 3rd cluster, find the sum of squared errors (SSE) for each cluster</li>
  <li>The cluster with the largest SSE value is separated into 2 clusters</li>
</ul>

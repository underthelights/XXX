<h1 id="9-thread-level-parallelism">9. Thread-Level Parallelism</h1>

<h1 id="today">Today</h1>

<ul>
  <li>Parallel Computing Hardware
    <ul>
      <li>Multicore</li>
      <li>Multiple separate processors on single chip</li>
      <li>Hyperthreading</li>
      <li>Efficient execution of multiple threads on single core</li>
    </ul>
  </li>
  <li>Thread-Level Parallelism
    <ul>
      <li>Splitting program into independent tasks</li>
      <li>Example: Parallel summation</li>
    </ul>
  </li>
  <li>Consistency Models
    <ul>
      <li>What happens when multiple threads are reading</li>
      <li>writing shared state  2</li>
    </ul>
  </li>
</ul>

<h1 id="exploiting-parallel-execution">Exploiting parallel execution</h1>

<ul>
  <li>
    <p>So far, we’ve used threads to deal with I/O delays</p>

    <p>(기존) single.multi,manicore 신경 안쓰고 풀었음</p>

    <ul>
      <li>지금까지 봤던 것 : CS에 여러가지가 들어갈 수 없으니까 한 thread가 들어가 있을 때 다른 thread가 들어가지 못하게 하여 sync문제 해결</li>
      <li>e.g., one thread per client to prevent one from delaying another -</li>
    </ul>
  </li>
  <li>Multi-core/Hyperthreaded CPUs offer another opportunity
    <ul>
      <li>Spread work over threads executing in parallel</li>
      <li>Happens automatically, if many independent tasks 
  independent task를 가지고 있어 실행하게 함
        <ul>
          <li>e.g., running many applications or serving many clients</li>
        </ul>
      </li>
      <li>Can also write code to make one big task go faster 
  thread끼리 공유하는 자료구조 : parallel하게 완전히 실행되기는 어려운 면 존재
        <ul>
          <li>by organizing it as multiple parallel sub-tasks  3</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>multicore
    <ul>
      <li>single chip 안에 여러가지의 processor : cpu 여러 개</li>
      <li>hypertheading되건 안되건 cpu 여러개 환경 가정,</li>
      <li>thread실행하게 되면 물리적으로 각각 mapping되어 실행 가능</li>
    </ul>
  </li>
  <li>Hyperthreading cpu :
    <ul>
      <li>하이퍼스레딩 가능하다고 하면 multicore 4인게 hyperthreaded되어 8개처럼 보인다</li>
      <li>CPU안에 instruction 처리하는 unit (control) : register 동일한 것들을 두개를 복제해 놓고 실제 계산하는 ALU FPU, 이런 것들을 공유하는 방식</li>
      <li>
        <p>hyperthreading 안 되던 시절 : pc 1개, register set도 결국 여러 개 inst 공유하도록 되어있지 않아 불가능</p>

        <p>→ load store, alu, fpu등 여러개가 있다 보니까 동시에 사용할 수 있게 해 줌으로서 pc 2개, register set 2개 있어 실제 function unit을 interleaving하며 동시에 사용할 수 있도록 함.</p>
      </li>
      <li>물리적 cpu 8개인것과 완전히 같지는 않지만 유사한 기능</li>
    </ul>
  </li>
</ul>

<h1 id="typical-multicore-processor">Typical Multicore Processor</h1>

<p><img src="https://user-images.githubusercontent.com/46957634/183250383-15541cd5-8047-4f77-a44b-4e6e7bc2e7d7.png" alt="Untitled" /></p>

<p>6.38.</p>

<ul>
  <li>cpu cache : processor 내 processor cache
    <ul>
      <li>CPU chip - (bus) - main memory</li>
    </ul>
  </li>
  <li>Private cahce : L1 cache, l2 cahce
    <ul>
      <li>자기만 접근할수 있는 cache</li>
      <li>실제 data 접근할 때 register/l1/l2 cache 뒤져 데이터 접근</li>
      <li>다른 core에서의 register/l1/l2 접근 불가</li>
      <li>cpu 쪽에 가까워질수록 access latency가 짧음
        <ul>
          <li>: (1) register에서 가져오는 것이 제일 빠름 - (2) l1 cache  - (3) l2 cache</li>
        </ul>
      </li>
      <li>L1 cache : instruction-data cache (computer architecture)</li>
    </ul>
  </li>
  <li>Shared cache : L3 cache
    <ul>
      <li>core들이 동시에 접근할 수 있음</li>
    </ul>
  </li>
  <li>main memory : DRAM - cache는 SM으로 만듬</li>
  <li>storage hierarchy 의 이해를 해야 최적화된 코드를 작성할 수 있음.</li>
</ul>

<h1 id="benchmark-machine">Benchmark Machine</h1>

<ul>
  <li>Get data about machine from /proc/cpuinfo
    <ul>
      <li>Shark Machines</li>
      <li>Intel Xeon E5520 @ 2.27 GHz</li>
      <li>Nehalem, ca. 2010</li>
      <li>8 Cores</li>
      <li>Each can do 2x hyperthreading  5</li>
    </ul>
  </li>
</ul>

<h2 id="example-1-parallel-summation">Example 1: Parallel Summation</h2>

<ul>
  <li>Sum numbers 0, …, n-1
    <ul>
      <li>Should add up to ((n-1)*n)/2</li>
    </ul>
  </li>
  <li>Partition values 1, …, n-1 into t ranges
    <ul>
      <li>$\frac n t$ values in each range</li>
      <li>Each of t threads processes 1 range</li>
      <li>For simplicity, assume n is a multiple of t</li>
    </ul>
  </li>
  <li>
    <p>Let’s consider different ways that multiple threads might work on their assigned ranges in parallel</p>
  </li>
  <li>0 ~ n-1까지의 합을 구하는 간단한 프로그램
    <ul>
      <li>→ 병렬적으로 계산한다고 하면 0 ,1, 2, … n-1</li>
    </ul>
  </li>
  <li>T라는 range를 갖는 partition을 만들어보자 : t개의 element partition
    <ul>
      <li>partition당 개수 = 전체 개수 / partition개수</li>
      <li>n의 값이 굉장히 크다면 Thread를 cpu 개수만큼 띄운 다음 일부 덧셈하고 각각을 aggregation하는 방법 취하기</li>
    </ul>
  </li>
</ul>

<h2 id="first-attempt-psum-mutex">First attempt: psum-mutex</h2>

<ul>
  <li>Simplest approach:
    <ul>
      <li>Threads sum into a global variable protected by a semaphore mutex.</li>
    </ul>

    <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kt">void</span> <span class="o">*</span><span class="nf">sum_mutex</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">vargp</span><span class="p">);</span> <span class="cm">/* Thread routine */</span>
    
  <span class="cm">/* Global shared variables */</span>
  <span class="kt">long</span> <span class="n">gsum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>          <span class="cm">/* Global sum */</span>
  <span class="kt">long</span> <span class="n">nelems_per_thread</span><span class="p">;</span> <span class="cm">/* Number of elements to sum */</span>
  <span class="n">sem_t</span> <span class="n">mutex</span><span class="p">;</span>            <span class="cm">/* Mutex to protect global sum */</span>
    
  <span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">**</span><span class="n">argv</span><span class="p">)</span>
  <span class="p">{</span>
      <span class="kt">long</span> <span class="n">i</span><span class="p">,</span> <span class="n">nelems</span><span class="p">,</span> <span class="n">log_nelems</span><span class="p">,</span> <span class="n">nthreads</span><span class="p">,</span> <span class="n">myid</span><span class="p">[</span><span class="n">MAXTHREADS</span><span class="p">];</span>
      <span class="n">pthread_t</span> <span class="n">tid</span><span class="p">[</span><span class="n">MAXTHREADS</span><span class="p">];</span>
    
      <span class="cm">/* Get input arguments */</span>
      <span class="n">nthreads</span> <span class="o">=</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
      <span class="n">log_nelems</span> <span class="o">=</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
      <span class="n">nelems</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1L</span> <span class="o">&lt;&lt;</span> <span class="n">log_nelems</span><span class="p">);</span>
      <span class="n">nelems_per_thread</span> <span class="o">=</span> <span class="n">nelems</span> <span class="o">/</span> <span class="n">nthreads</span><span class="p">;</span>
      <span class="n">sem_init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
      <span class="cm">/* Create peer threads and wait for them to finish */</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nthreads</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="p">{</span>
          <span class="n">myid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
          <span class="n">Pthread_create</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tid</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">NULL</span><span class="p">,</span> <span class="n">sum_mutex</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myid</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
      <span class="p">}</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nthreads</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
          <span class="n">Pthread_join</span><span class="p">(</span><span class="n">tid</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">NULL</span><span class="p">);</span>
    
      <span class="cm">/* Check final answer */</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">gsum</span> <span class="o">!=</span> <span class="p">(</span><span class="n">nelems</span> <span class="o">*</span> <span class="p">(</span><span class="n">nelems</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
          <span class="n">printf</span><span class="p">(</span><span class="s">"Error: result=%ld</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">gsum</span><span class="p">);</span>
    
      <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
  <span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>thread 개수를 nthread 개수만큼 띄운다.</li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Thread routine for psum-mutex.c */</span>
<span class="kt">void</span> <span class="o">*</span><span class="nf">sum_mutex</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">vargp</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">long</span> <span class="n">myid</span> <span class="o">=</span> <span class="o">*</span><span class="p">((</span><span class="kt">long</span> <span class="o">*</span><span class="p">)</span><span class="n">vargp</span><span class="p">);</span>          <span class="cm">/* Extract thread ID */</span>
    <span class="kt">long</span> <span class="n">start</span> <span class="o">=</span> <span class="n">myid</span> <span class="o">*</span> <span class="n">nelems_per_thread</span><span class="p">;</span> <span class="cm">/* Start element index */</span>
    <span class="kt">long</span> <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">nelems_per_thread</span><span class="p">;</span>  <span class="cm">/* End element index */</span>
    <span class="kt">long</span> <span class="n">i</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">P</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
        <span class="n">gsum</span> <span class="o">+=</span> <span class="n">i</span><span class="p">;</span>
        <span class="n">V</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="psum-mutex-thread-routine">psum-mutex Thread Routine</h2>

<ul>
  <li>
    <p>Simplest approach: Threads sum into a global variable protected by a semaphore mutex.</p>
  </li>
  <li>여러 개 thread spawn한 다음
    <ul>
      <li>gsum이라는 global variable을 만들어 놓고 이를 주기적으로 update
        <ul>
          <li>→ 모든 thread들이 각각 얼마만큼 실행하면 매번 iteration 돌 때마다 gsum update</li>
        </ul>
      </li>
      <li>0~n-1까지의 값을 가지게 될 것이다</li>
    </ul>
  </li>
  <li>mutex :
    <ul>
      <li>이렇게 코딩하면 여러 thread들이 동시에 접근하는 global variable이기 때문에 mutex를 걸어 두어 동시에 update하지 않도록 함</li>
      <li>expensive하지만 cs 보호를 위해 잡아 둠</li>
    </ul>
  </li>
</ul>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Thread routine for psum-mutex.c */</span>
<span class="kt">void</span> <span class="o">*</span><span class="nf">sum_mutex</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">vargp</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">long</span> <span class="n">myid</span> <span class="o">=</span> <span class="o">*</span><span class="p">((</span><span class="kt">long</span> <span class="o">*</span><span class="p">)</span><span class="n">vargp</span><span class="p">);</span>          <span class="cm">/* Extract thread ID */</span>
    <span class="kt">long</span> <span class="n">start</span> <span class="o">=</span> <span class="n">myid</span> <span class="o">*</span> <span class="n">nelems_per_thread</span><span class="p">;</span> <span class="cm">/* Start element index */</span>
    <span class="kt">long</span> <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">nelems_per_thread</span><span class="p">;</span>  <span class="cm">/* End element index */</span>
    <span class="kt">long</span> <span class="n">i</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">P</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
        <span class="n">gsum</span> <span class="o">+=</span> <span class="n">i</span><span class="p">;</span>
        <span class="n">V</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="psum-mutex-performance">psum-mutex Performance</h2>

<ul>
  <li>Shark machine with 8 cores, $n=2^{31}$</li>
  <li>Nasty surprise:
    <ul>
      <li>Single thread is very slow</li>
      <li>Gets slower as we use more cores  10
        <ul>
          <li>계속해서 slow down</li>
        </ul>

        <p>single cpu 자체도 느림 : 혼자 돈다 하더라도 mutex lock을 계속 잡고 풀고 overhead</p>

        <p>→ multicore또한 더 느림→ 병렬 프로그래밍 어려움</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/46957634/183250355-cf35ed84-88d9-4284-9ac2-eb43897f7fbb.png" alt="Untitled 1" /></p>

<h2 id="next-attempt-psum-array">Next Attempt: psum-array</h2>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Thread routine for psum-array.c */</span>
<span class="kt">void</span> <span class="o">*</span><span class="nf">sum_array</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">vargp</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">long</span> <span class="n">myid</span> <span class="o">=</span> <span class="o">*</span><span class="p">((</span><span class="kt">long</span> <span class="o">*</span><span class="p">)</span><span class="n">vargp</span><span class="p">);</span>          <span class="cm">/* Extract thread ID */</span>
    <span class="kt">long</span> <span class="n">start</span> <span class="o">=</span> <span class="n">myid</span> <span class="o">*</span> <span class="n">nelems_per_thread</span><span class="p">;</span> <span class="cm">/* Start element index */</span>
    <span class="kt">long</span> <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">nelems_per_thread</span><span class="p">;</span>  <span class="cm">/* End element index */</span>
    <span class="kt">long</span> <span class="n">i</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">psum</span><span class="p">[</span><span class="n">myid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">i</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<ul>
  <li>partial sum array 만들기
    <ul>
      <li>다 끝나면 처음부터 k개 element까지 더함</li>
      <li>마지막에 array를 더하기하고</li>
      <li>자기가 중간에 계산한 값을 gsum에서 sum하지 않고 각자 자신의 array 공간을 가지고 수행</li>
    </ul>
  </li>
  <li>앞서 사용했던 mutex가 불필요 : 각자 자기 변수의 작업을 하기 때문에 <code class="language-plaintext highlighter-rouge">psum[myid] += i</code></li>
  <li>
    <p>앞에서 보여준 것처럼 thread끼리 compend하는 경우가 없을 것</p>
  </li>
  <li>Peer thread i sums into global array element <code class="language-plaintext highlighter-rouge">psum[i]</code></li>
  <li>Main waits for theads to finish, then sums elements of psum</li>
  <li>Eliminates need for mutex synchronization</li>
</ul>

<h2 id="psum-array-performance">psum-array Performance</h2>

<ul>
  <li>Orders of magnitude faster than <code class="language-plaintext highlighter-rouge">psum-array</code>
    <ul>
      <li>thread개수가 줄어들며 실행 시간이 쭉 쭉 줄어듦.</li>
      <li>cpu mutex lock unlock할 때는 시간이 너무 많이 들었으나 1/5수준으로 성능이 향상됨</li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/46957634/183250360-2ca4f49f-581b-46b5-ba8a-552f94f03d47.png" alt="Untitled 2" /></p>

<h2 id="next-attempt-psum-local">Next Attempt: psum-local</h2>

<ul>
  <li>
    <p>Reduce memory references by having peer thread i sum into a local variable (register)</p>
  </li>
  <li>→ 개선 :
    <ul>
      <li>array를 gloabl var로 쓰는게 아닌, 각 thread가 자신의 local var을 가지고 중간값을 저장할 것
  → compiler가 register에 있는 값으로 memory 값 update</li>
    </ul>
  </li>
  <li>memory가 아닌 실제 register에서 update되는 local variable</li>
  <li>iteration을 돌며 중간값 sum (@register) → psum이라는 array에 최종 update</li>
  <li>즉 본인 register에서 update하고 최종적으로 local psum에서 update
(장점) Memory reference overhead 줄일 수 있음 (local var의 활용) memory lv까지 가지 않고</li>
</ul>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Thread routine for psum-local.c */</span>
<span class="kt">void</span> <span class="o">*</span><span class="nf">sum_local</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">vargp</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">long</span> <span class="n">myid</span> <span class="o">=</span> <span class="o">*</span><span class="p">((</span><span class="kt">long</span> <span class="o">*</span><span class="p">)</span><span class="n">vargp</span><span class="p">);</span>          <span class="cm">/* Extract thread ID */</span>
    <span class="kt">long</span> <span class="n">start</span> <span class="o">=</span> <span class="n">myid</span> <span class="o">*</span> <span class="n">nelems_per_thread</span><span class="p">;</span> <span class="cm">/* Start element index */</span>
    <span class="kt">long</span> <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">nelems_per_thread</span><span class="p">;</span>  <span class="cm">/* End element index */</span>   
    <span class="kt">long</span> <span class="n">i</span><span class="p">,</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>        
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">i</span><span class="p">;</span>                          
    <span class="p">}</span>
    <span class="n">psum</span><span class="p">[</span><span class="n">myid</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="psum-local-performance">psum-local Performance</h2>

<ul>
  <li>Significantly faster than <code class="language-plaintext highlighter-rouge">psum-array</code></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/46957634/183250362-3c807d3f-b2ab-4284-bac0-0bec7099bd89.png" alt="Untitled 3" /></p>

<h1 id="characterizing-parallel-program-performance">Characterizing Parallel Program Performance</h1>

<ul>
  <li>characterize : 얼마만큼 성능을 내는지 분석
    <ul>
      <li>p=8 / 8개 core를 활용하는 CPU</li>
      <li>k&lt;p, k = 4 / thread를 4개 띄운 형태</li>
    </ul>
  </li>
  <li>p processor cores, Tk is the running time using k cores</li>
  <li>Def. <strong>Speedup</strong>: $S_p=\frac{T_1} {T_p }$
    <ul>
      <li>relative speedup if T1 is running time of parallel version of the code running on 1 core.</li>
      <li>absolute speedup if T1 is running time of sequential version of code running on 1 core.</li>
      <li>
        <p>Absolute speedup is a much truer measure of the benefits of parallelism.</p>
      </li>
      <li>S4 : 4개의 cpu를 활용하여 얼만큼 속도가 빨라지는지 = T1/T4
        <ul>
          <li>T1 : 순수하게 하나의 CPU에서 sequential -&gt; absolute speedup : 혼자 돌았을 때</li>
          <li>T4개 돌았을 때 시간이 있다면 S4 = T1/T4</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Def. <strong>Efficiency</strong> $E_p = \frac{S_p}{p} = \frac{T_1} {pT_p}$ = speedup / process 개수
    <ul>
      <li>Reported as a percentage in the range (0, 100].</li>
      <li>Measures the overhead due to parallelization</li>
    </ul>
  </li>
  <li>ex
    <ul>
      <li>순수하게 thread 1개 넣은 t1 = 1, t4 = 0.25
        <ul>
          <li>→ S4 = 1/0.25 = 4이므로</li>
        </ul>
      </li>
      <li>E4 = 4/4 = 1 -&gt; 결국 100% (손실이 전혀 없다)
        <ul>
          <li>이상적으로 효율적으로 parallellize</li>
        </ul>
      </li>
      <li>
        <p>t1 = 1인데 t4=0.5</p>

        <p>→ S4 = 1/0.5 = 2 :process는 1개~4개했지만 speedup은 2개</p>

        <p>→ efficiency = 2/4 = 50%만큼의 성능 손실</p>

        <p>이상적으로 바라는 100%까지는 도달하지 못함</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="performance-of-psum">Performance of psum</h2>

<p><img src="https://user-images.githubusercontent.com/46957634/183250364-831bcd48-8098-4352-9a86-ccecc1435c0e.png" alt="Untitled 4" /></p>

<ul>
  <li>loss가 커지는 게 Sp가 어느정도 가다가 성능은 더 좋아졌지만 도움이 되지 않는다</li>
  <li>효율이 떨어지기 시작한다 → 가장 최적의 성능을 갖는 8 core까지 돌림</li>
  <li>
    <p>pt : 성능을 가장 잘 낼 수 있는 point</p>
  </li>
  <li>Efficiencies OK, not great -</li>
  <li>Our example is easily parallelizable -</li>
  <li>Real codes are often much harder to parallelize  16</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/46957634/183250366-04f93967-d513-4389-8f17-ca156587b856.png" alt="Untitled 5" /></p>

<h1 id="memory-consistency">Memory Consistency</h1>

<ul>
  <li>memory consistency
    <ul>
      <li>memory에 두 변수가 있고 Thread1 : a -&gt; 2, print / Thread2 : b -&gt; 200, print</li>
      <li>execution order가 이미 정해져 있음</li>
      <li>값이 deterministic하지 않음 : thread1&lt;-&gt;thread2 순서 interleaving 가능</li>
      <li>각각 실행되는 순서에 ordering을 강제화하지 않는다면 값을 알수 없음
        <ul>
          <li>→ 실행되는 순서에 따라 달라지는 값</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/46957634/183250368-89f8373c-98c3-40a9-a80c-0b3b7bd0f12e.png" alt="Untitled 6" />
<img src="https://user-images.githubusercontent.com/46957634/183250369-e981dc13-1ed8-4ee4-9b5e-cb0568a2ceee.png" alt="Untitled 7" /></p>

<ul>
  <li>What are the possible values printed?
    <ul>
      <li>Depends on memory consistency model</li>
      <li>Abstract model of how hardware handles concurrent accesses</li>
    </ul>
  </li>
  <li>Sequential consistency : 순서에 따라 실행될 수 있게
    <ul>
      <li>만일 순서에 따라 실행되지 않는다고 하면, 나올 수 있는 결과값을 보장할 수 없음.</li>
      <li>전체적인 결과가 각 thread의 실행 순서에 따라 결정되고 interleaving에 따라 변화</li>
      <li>Overall effect consistent with each individual thread</li>
      <li>Otherwise, arbitrary interleaving  17 Thread2: Wb: b = 200; Ra: print(a);</li>
    </ul>
  </li>
</ul>

<h2 id="sequential-consistency-example">Sequential Consistency Example</h2>

<p><img src="https://user-images.githubusercontent.com/46957634/183250370-b4d7ed7e-5db9-42b3-a528-206a34942588.png" alt="Untitled 8" /></p>

<p><img src="https://user-images.githubusercontent.com/46957634/183250372-57156631-4bcf-4201-aa5e-79feffb5fa50.png" alt="Untitled 9" />
<img src="https://user-images.githubusercontent.com/46957634/183250374-abae0b75-e5c6-42f2-8686-0efabca16cbd.png" alt="Untitled 10" /></p>

<ul>
  <li>Impossible outputs
    <ul>
      <li>100, 1 and 1, 100</li>
      <li>Would require reaching both Ra and Rb before Wa and Wb</li>
    </ul>
  </li>
</ul>

<h1 id="non-coherent-cache-scenario">Non-Coherent Cache Scenario</h1>

<p><img src="https://user-images.githubusercontent.com/46957634/183250375-c664baf4-26c6-42d3-a2f1-eb9943db362c.png" alt="Untitled 11" />
<img src="https://user-images.githubusercontent.com/46957634/183250377-419627a5-a187-489e-9619-07b41eefb358.png" alt="Untitled 12" /></p>

<ul>
  <li>
    <p>Write-back caches, without coordination between them</p>
  </li>
  <li>cache → 더 복잡해짐/ a=1, b=100
    <ul>
      <li>thread 1 : a =2 @ cache</li>
      <li>thread 2 : b =200 @ cache 라고 하자.</li>
    </ul>
  </li>
  <li>memory의 값</li>
  <li>cpu에 register만 있는 것이 아니라 memory까지 가는 길목에 cache 존재
    <ul>
      <li>
        <blockquote>
          <p>a,b의 contents copy본이 어디엔가 존재</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>thread가 실행하며 update했는데 그 값이 main memory의 original copy본과 과 다름
    <ul>
      <li>memory에서는 복제 값을 가지고 있고 thread로부터 최종 update하지 않은 상태</li>
    </ul>
  </li>
  <li>→ b=100 print</li>
  <li>
    <p>문제 상황 : 이렇게 하면 안됨 (문제상황)</p>

    <p>thread 1번은 thread 2번이 b라는 것을 cpu cache에서 b=200으로 바꾼 것을 모름</p>

    <p>thread 2번은 thread 1번이 a라는 것을 cpu cache에서 a=2으로 바꾼 것을 모름</p>
  </li>
  <li>각 cahce에 복제본을 가지고 update한 다음 실행하게 되면 a =1 프린트하고 b=100을 print하게 됨 (최신의 값으로 update되지 않음)</li>
</ul>

<h1 id="snoopy-caches">Snoopy Caches</h1>

<p><img src="https://user-images.githubusercontent.com/46957634/183250378-3f6c8182-a96b-4e05-91f5-b83139f85bda.png" alt="Untitled 13" /></p>

<p><img src="https://user-images.githubusercontent.com/46957634/183250379-33cd6cec-0ce1-423b-a708-634f9abb654d.png" alt="Untitled 14" /></p>

<p>| <Invalid> | Cannot use value |
| --- | --- |
| <Shared> | Readable copy |
| <Exclusive> | Writable copy |</Exclusive></Shared></Invalid></p>
<ul>
  <li>각 CPU에서 돌아가는 memory contents에 cacheing될 때 tag를 붙여 둠</li>
  <li>main memory로부터 cache로 copy본을 가지고 있어 값을 바꾼 후 update되지 않은 상태
→ exclusive tag</li>
</ul>

<p>만약, thread2번이 a를 접근하는 경우에</p>

<ul>
  <li>snoopy cahces :
    <ul>
      <li>a라는 copy본을 가지고 있지 않지만 다른 cpu cache에 a라는 값이 복제본 가짐을 알 수 있는 방법</li>
      <li>읽을 때 thread 1의 contents를 shared로, thread 2의 contents 를 sahre</li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/46957634/183250381-778aad25-ab96-48a6-9948-bc4ea7a35bae.png" alt="Untitled 15" /></p>

<p><img src="https://user-images.githubusercontent.com/46957634/183250382-a2c4b318-030b-4d31-8d81-0b4cddbd9859.png" alt="Untitled 16" /></p>

<ul>
  <li>When cache sees request for one of its E-tagged blocks
    <ul>
      <li>Supply value from cache</li>
      <li>Set tag to S</li>
    </ul>
  </li>
  <li>누가 어떤 contents를 가지고, tag를 가지는지를 서로 확인할 수 있게 하는 방법이 hw적으로 구현
    <ul>
      <li>
        <blockquote>
          <p>thread 1에서 b를 접근할 때 thread 1에서 exclusive tag를 가지고 있다면 이를 가져와 print</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>SP하는 입장에서 보면 HW적으로 메커니즘이 구현되어 있기에</li>
  <li>Cache coherensive protocol을 hw적으로 implement (by snoopy)
    <ul>
      <li>→ 실제 순수하게 cache간 값을 share하는 overhead는 감수한다. (0은 아님)</li>
    </ul>
  </li>
  <li>내부적인 multicore로 봤을 때의 개념 기억하기, 성능 상의 차이</li>
</ul>

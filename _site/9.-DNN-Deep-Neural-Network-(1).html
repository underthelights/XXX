<!DOCTYPE html>
<html lang="en"><head>
  
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         TeX: {
         equationNumbers: {
             autoNumber: "AMS"
         }
         },
         tex2jax: {
         inlineMath: [ ['$', '$'] ],
         displayMath: [ ['$$', '$$'] ],
         processEscapes: true,
     }
     });
     MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
         alert("Math Processing Error: "+message[1]);
         });
     MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
         alert("Math Processing Error: "+message[1]);
         });
     </script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>9. DNN Deep Neural Network (1) ✱ Kyuhwan Shim</title>

  <link rel="stylesheet" href="/assets/css/style.css">

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Barlow:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800&family=Noto+Sans+TC&display=swap" rel="stylesheet">

  <link href="/assets/fontawesome/all.min.css" rel="stylesheet">

  <!-- Bootstrap -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js'></script>

  <!-- GA -->
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VG8LB4J4EL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VG8LB4J4EL');
</script>


  <!-- favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/images/main/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/images/main/favicon/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/images/main/favicon/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/images/main/favicon/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/images/main/favicon/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/images/main/favicon/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/images/main/favicon/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/images/main/favicon/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/images/main/favicon/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/main/favicon/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/images/main/favicon/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/main/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/images/main/favicon/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/main/favicon/favicon-16x16.png">
  <link rel="manifest" href="">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/images/main/favicon/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  <!-- Opengraph -->
  <meta property="og:type" content="website">
  <meta property="og:image" content="/assets/images/main/favicon/og.png9.-DNN-Deep-Neural-Network-(1).jpg">
  <meta property="og:title" content="9. DNN Deep Neural Network (1) | Kyuhwan Shim">
  <meta property="og:description" content="Paper overview of  et al., ">

  <!-- Google scholar -->
  
  <meta name="citation_title" content="9. DNN Deep Neural Network (1)">
  
  <meta name="citation_publication_date" content="">
  
  <meta name="citation_conference_title" content="">
  
  <meta name="citation_pdf_url" content="https://underthelights.github.io/assets/pdf/9.-DNN-Deep-Neural-Network-(1).pdf">
  
</head>
<body><header class="page-content">
  <nav class="navbar navbar-expand-md navbar-light py-4">
    <div class="container-fluid">
      <button class="navbar-toggler ms-auto" type="button" data-bs-toggle="collapse" data-bs-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
        <ul class="navbar-nav ms-auto mt-4 mt-lg-0 navbar-nav-scroll">
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/news">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/publication">publication</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/project">Project</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/oconnect">oconnect</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/cv">CV</a>
          </li>
          <li class="nav-item dropdown">
              <a href="#" class="nav-link dropdown-toggle " data-bs-toggle="dropdown">fun</a>
              <div class="dropdown-menu">
                  <a href="/music" class="dropdown-item">music</a>
                  <a href="/artwork" class="dropdown-item">artwork</a>
              </div>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" target="_blank" rel="noopener noreferrer" href="https://underthelights.github.io/blog/">Blog<i class="fa-regular fa-arrow-up-right-from-square" style="padding-left: 5px;"></i></a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</header>

<style>
  .dropdown-toggle {
    background-color: transparent;
    border-color: #fff;
    border-style: solid;
    border-top: none;
    border-right: none;
    border-left: none;
    transition: color .15s ease-in-out, background-color .15s ease-in-out,border-color .15s ease-in-out;
  }
  .dropdown-toggle:hover {
    font-weight: 500
  }
  .dropdown:hover .dropdown-menu {
    display: block;
    margin-top: 0;
 }
 .dropdown-menu {
    --bs-dropdown-border-radius: 0 !important;
    padding-top: 0 !important;
    padding-bottom: 0 !important
 }
 .dropdown-item {
  font-size: .95rem !important;
  padding: .3rem .75rem !important;
 }
 .dropdown-item:active {
  background-color: #999 !important
 }
</style><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <style>
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    
      font-family: 'Arial', sans-serif;
      /* Use a modern font */
      margin-top: 20px;
      margin-bottom: 10px;
  }
  /* 포스트 헤더에 적용되는 여백 제거 */
  .post-header, h1, h2, h3, h4, h5, h6 {
      margin-left: 0 !important; 
      padding-left: 0 !important;
  }


  .post-title {
      font-size: 2em; /* 폰트 크기 증가 */
      font-weight: bold; /* 볼드체 */
      color: #333; /* 색상 변경 */
      box-shadow: inset 0 -20px 0 #bbb7e8;
      max-width: max-content;
    }

  /* Updated styling for h1 */
  h1 {
    font-size: 1.5em;
    /* Increase font size for main titles */
    box-shadow: inset 0 -10px 0 #bbb7e8;
    max-width: max-content; 
    font-weight: bold; /* 볼드체 */
    border-bottom: none;
    /* Remove bottom border */
    padding-bottom: 0;
    /* Adjust padding if needed */
  }

  h2 {
    font-size: 1.2em;
    margin-left: none;
    font-weight: bold; /* 볼드체 */
    max-width: max-content; 
    background-color: #cdcbe9;
  }

  h3 {
    font-size: 1.1em;
    max-width: max-content; 
    
    max-width: max-content; 
    background-color: #bbb7e8;
  }

  h4 {
    font-size: 0.75em;
    color: #888888;
  }

  h5 {
    font-size: 0.5em;
    color: #a6a6a6;
  }

  h6 {
    font-size: 1em;
    color: #bcbcbc;
  }

  /* Add some additional styling if needed */
  .post-content {
    margin-left: 20px; /* 왼쪽 여백 추가 */
    /* margin-right: 20px; 오른쪽에 TOC 너비 + 여백 만큼 추가 */
    line-height: 1.6;/* Improve readability */
    color: #333;/* Dark grey color for text */
  }
  /* Category Chart Style */
  .category-chart {
    font-family: 'Arial', sans-serif;
    /* width: 100%; */
    /* height: 100px; Adjust as needed */
  }

  .category-list li {
    display: inline-block;
    margin-right: 20px;
  }
  /* 이미지 스타일 */
  img {
    display: block; /* 이미지를 블록 레벨로 설정 */
    width: 50%; /* 또는 원하는 너비 */
    height: auto; /* 비율 유지 */
    margin-bottom: 5px; /* 이미지와 캡션 사이의 간격 */
    text-align: center;
  }

  /* 이미지 캡션 스타일링 */
  .em {
    color: #757575;
    font-size: 0.8em;
    
    /* margin-top: 5px; */
    display: block; /* 캡션을 블록 요소로 설정, 이미지 아래로 강제 배치 */
    text-align: center;
    margin-bottom: 5px;
  }
  /* Blockquote 스타일 */
  blockquote {
      margin-left: 20px;
      border-left: 1.5px solid #ccc; /* 좌측 선 */
      border-top: 1.5px solid #ccc; /* 좌측 선 */
      border-right: 1.5px solid #ccc; /* 좌측 선 */
      border-bottom: 1.5px solid #ccc; /* 좌측 선 */

      padding-left: 15px;
      color: #666; /* 글자 색상 */
  }



</style>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <!-- Table of Contents Container -->
<div class="toc-container">
    <nav class="toc">
      <strong>Table of Contents</strong>
      <!--  -->
      <!-- Jekyll TOC Liquid Code -->
      <ul><li><a href="#deep-learning">Deep Learning</a><ul><li><a href="#history">History</a></li><li><a href="#structure">Structure</a></li></ul></li><li><a href="#dropout">Dropout</a><ul><li><a href="#training-stage">Training stage</a><ul><li><a href="#testing-stage">Testing stage</a></li></ul></li></ul></li><li><a href="#convolutional-neural-network">Convolutional Neural Network</a><ul><li><a href="#fully-connected-layer">Fully connected layer</a></li><li><a href="#locally-connected-layer">Locally Connected Layer</a></li><li><a href="#convolution-operation">Convolution operation</a></li><li><a href="#pooling">Pooling</a><ul><li><a href="#max-pooling">Max-pooling</a></li><li><a href="#average-pooling">Average-pooling</a></li><li><a href="#l2-pooling">L2-pooling</a></li></ul></li><li><a href="#convolution-kernel-filter-examples">Convolution kernel (filter) examples</a></li><li><a href="#lenet-5-1998">LeNet-5 (1998)</a></li><li><a href="#backpropagation-in-cnn">Backpropagation in CNN</a></li></ul></li><li><a href="#from-handwritten-digits-to-3-d-objects">From Handwritten Digits to 3-D objects</a></li><li><a href="#the-ilsvrc-2012-competition-on-imagenet">The ILSVRC-2012 Competition on ImageNet</a><ul><li><a href="#examples-from-the-test-set-with-the-networks-guesses">Examples from the test set (with the network’s guesses)</a></li></ul></li><li><a href="#a-cnn-for-imagenet">A CNN for ImageNet</a><ul><li><a href="#more-examples-from-alexnet">More examples from AlexNet</a></li><li><a href="#hardware-for-alexnet">Hardware for AlexNet</a></li></ul></li><li><a href="#evolution-of-the-dnn">Evolution of the DNN</a></li><li><a href="#fully-convolutional-networks">Fully Convolutional Networks</a></li><li><a href="#machine-learning-deep-learning-data-mining-big-data">Machine Learning, Deep Learning, Data Mining, Big data</a><ul><li><a href="#big-data">Big Data</a></li><li><a href="#machine-learning">Machine Learning</a></li><li><a href="#deep-learning-1">Deep learning</a></li><li><a href="#data-mining">Data Mining</a></li><li><a href="#some-history">Some history</a><ul><li><a href="#timelines">timelines</a></li></ul></li><li><a href="#limitations-of-deep-learning">Limitations of Deep Learning</a></li><li><a href="#statics-of-acceptance-rate-neurips">Statics of acceptance rate NeurIPS</a></li><li><a href="#deep-learning-frameworktoolkits">Deep Learning Framework/Toolkits</a></li><li><a href="#alexnet">AlexNet</a></li></ul></li></ul>

      <!-- <ul><li><a href="#deep-learning">Deep Learning</a><ul><li><a href="#history">History</a></li><li><a href="#structure">Structure</a></li></ul></li><li><a href="#dropout">Dropout</a><ul><li><a href="#training-stage">Training stage</a><ul><li><a href="#testing-stage">Testing stage</a></li></ul></li></ul></li><li><a href="#convolutional-neural-network">Convolutional Neural Network</a><ul><li><a href="#fully-connected-layer">Fully connected layer</a></li><li><a href="#locally-connected-layer">Locally Connected Layer</a></li><li><a href="#convolution-operation">Convolution operation</a></li><li><a href="#pooling">Pooling</a><ul><li><a href="#max-pooling">Max-pooling</a></li><li><a href="#average-pooling">Average-pooling</a></li><li><a href="#l2-pooling">L2-pooling</a></li></ul></li><li><a href="#convolution-kernel-filter-examples">Convolution kernel (filter) examples</a></li><li><a href="#lenet-5-1998">LeNet-5 (1998)</a></li><li><a href="#backpropagation-in-cnn">Backpropagation in CNN</a></li></ul></li><li><a href="#from-handwritten-digits-to-3-d-objects">From Handwritten Digits to 3-D objects</a></li><li><a href="#the-ilsvrc-2012-competition-on-imagenet">The ILSVRC-2012 Competition on ImageNet</a><ul><li><a href="#examples-from-the-test-set-with-the-networks-guesses">Examples from the test set (with the network’s guesses)</a></li></ul></li><li><a href="#a-cnn-for-imagenet">A CNN for ImageNet</a><ul><li><a href="#more-examples-from-alexnet">More examples from AlexNet</a></li><li><a href="#hardware-for-alexnet">Hardware for AlexNet</a></li></ul></li><li><a href="#evolution-of-the-dnn">Evolution of the DNN</a></li><li><a href="#fully-convolutional-networks">Fully Convolutional Networks</a></li><li><a href="#machine-learning-deep-learning-data-mining-big-data">Machine Learning, Deep Learning, Data Mining, Big data</a><ul><li><a href="#big-data">Big Data</a></li><li><a href="#machine-learning">Machine Learning</a></li><li><a href="#deep-learning-1">Deep learning</a></li><li><a href="#data-mining">Data Mining</a></li><li><a href="#some-history">Some history</a><ul><li><a href="#timelines">timelines</a></li></ul></li><li><a href="#limitations-of-deep-learning">Limitations of Deep Learning</a></li><li><a href="#statics-of-acceptance-rate-neurips">Statics of acceptance rate NeurIPS</a></li><li><a href="#deep-learning-frameworktoolkits">Deep Learning Framework/Toolkits</a></li><li><a href="#alexnet">AlexNet</a></li></ul></li></ul> -->
    </nav>
</div>

<style>
    /* TOC 스타일 */
.toc-container {
  position: fixed;
  right: 10px;
  top: 100px;
  z-index: 1000;
  /* max-width: 50px; 가로 너비 조정, 필요에 따라 값을 변경하세요 */
  font-size: 0.75rem;
}

.toc {
  border: 1px solid #ddd;
  padding: 10px;
  border-radius: 5px;
  background-color: white;
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  max-width: 200px; /* 가로 너비 조정, 필요에 따라 값을 변경하세요 */
}


.toc strong {
  display: block;
  margin-bottom: 10px;
}

.toc ul {
  list-style: none;
  padding: 0;
  margin: 0;
}

.toc ul li a {
  text-decoration: none;
  color: #007bff;
  display: block;
  padding: 5px 0;
}

.toc ul li a:hover,
.toc ul li a.active { /* 추가된 active 클래스 스타일 */
  font-weight: bold;
  /* box-shadow: inset 0 -10px 0 #bbb7e8; */
  background-color: #bbb7e8;
}



  button {
    padding: 5px 10px;
    /* 상하 10px, 좌우 20px 패딩으로 텍스트 주변 여유 공간 추가 */
    font-size: 0.87rem;
    /* 글자 크기 */
    color: white;
    /* 글자 색상 */
    background-color: #007bff;
    /* 배경 색상 */
    border: none;
    /* 테두리 제거 */
    border-radius: 5px;
    /* 모서리 둥글게 */
    cursor: pointer;
    /* 커서 모양 변경 */
    transition: background-color 0.3s;
    /* 호버 효과를 위한 전환 설정 */
    text-align: center;
    /* 글자를 버튼 중앙에 위치 */
    display: inline-block;
    /* 인라인 블록 요소로 설정하여 자연스럽게 텍스트 중앙 정렬 */
    line-height: normal;
    /* 기본 라인 높이 설정 */
    vertical-align: middle;
    /* 수직 방향으로 중앙 정렬 */
    margin-top: 5px;
  }

  button:hover {
    background-color: #0056b3;
    /* 호버 시 배경 색상 변경 */
  }


  /* reset base styles */
  * {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
  }

  /* page header */
  header {
    margin-bottom: 2rem;
    padding-left: 6.5rem;
  }

  header h1 {
    font-size: 1.5rem;
  }

  header p {
    margin: .5rem 0;
    font-size: 1rem !important
  }

  main b {
    font-weight: 500
  }

  /* normal body content */
  h2 {
    font-size: 1.1rem;
    margin-bottom: 0.75rem;
    margin-left: 6.5rem;
    text-transform: uppercase;
  }

  h3 {
    border-bottom: 1px solid black;
    font-size: .9rem;
    margin: 1rem 0 .5rem 6.5rem
  }

  p {
    margin-bottom: 0.5rem;
  }

  a {
    color: inherit;
    /*#0000ee;*/
  }

  section {
    margin-bottom: 3rem;
  }

  /* misc */
  .pdf {
    font-size: .9rem !important;
    font-weight: 300;
    margin-left: 1.5rem;
    white-space: nowrap;
  }

  .pdf i {
    margin-right: .1rem;
  }

  .material {
    font-size: small;
    margin-left: .5rem;
  }

  :global(i) {
    padding-right: 4px !important
  }

  /* dated entries */
  .dated-entry {
    display: flex;
    flex-flow: row wrap;
    position: relative;
    margin-bottom: 1rem;
  }

  .dated-date {
    width: 6.5rem;
    text-align: right;
    padding-top: .15rem;
    padding-right: 1.5rem;
    font-size: .8rem
  }

  .dated-content {
    width: calc(100% - 6.5rem);
    font-size: .95rem
  }

  .oneline-entries {
    margin-bottom: 0.5rem;
  }

  .oneline-entries .dated-entry {
    margin-bottom: 0;
  }

  /* hide extra awards info for now, not sure what to include */
  #awards em {
    display: none;
  }

  .author-tooltip {
    font-weight: 400;
    font-size: .8rem !important;
    text-align: center;
  }

  /* on narrow displays, make the font smaller */
  @media (max-width: 480px) {
    html {
      font-size: 14px;
    }
  }

  /* when printing, make the font smaller and the page full-width */
  @media print {
    html {
      font-size: 12px;
    }

    main {
      margin-top: 0;
      max-width: 100%;
    }
  }

</style>

<script>
    document.addEventListener('scroll', function() {
      var sections = document.querySelectorAll('section'); // 섹션 선택자 수정
      var menu_links = document.querySelectorAll('.toc a'); // TOC 링크 선택자 수정
    
      var fromTop = window.scrollY;
    
      sections.forEach(function(section) {
        if (section.offsetTop <= fromTop && section.offsetTop + section.offsetHeight > fromTop) {
          menu_links.forEach(function(link) {
            if (section.getAttribute('id') && link.getAttribute('href').includes(section.getAttribute('id'))) {
              link.classList.add('active');  // 현재 섹션의 TOC 링크에 'active' 클래스 추가
            } else {
              link.classList.remove('active'); // 다른 모든 링크에서 'active' 클래스 제거
            }
          });
        }
      });
    });
    </script>
    

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">9. DNN Deep Neural Network (1)</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-05-24T00:00:00+02:00" itemprop="datePublished">
        May 24, 2023
      </time></p>
    <!-- 태그와 카테고리 표시 -->
    <div class="post-categories">
      
      <strong><span>📚 Categories:</span></strong>
      
      <a href="blog-categories-notes">Notes</a>
      
      
    </div>

    <div class="post-tags">
      
      <strong><span>🏷️ Tags:</span></strong>
      
      <a href="blog-tags-ml">ML</a>
      
      
    </div>


  </header>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>박운상 교수
Office: R-911
Tel: 705-8936
Email: <a href="mailto:unsangpark@sogang.ac.kr">unsangpark@sogang.ac.kr</a></p>

<h1 id="deep-learning">Deep Learning</h1>

<ul>
  <li>A machine learning subfield. Exceptional performance in learning patterns.</li>
  <li>Deep learning algorithms attempt to learn (multiple levels of) representation by using a hierarchy of multiple layers.</li>
  <li>If you provide the system tons of information, it begins to understand it and respond in useful ways.</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/0.png" alt="0" /></p>

<ul>
  <li>Manually designed features are often over-specified, incomplete and take a long time to design and validate</li>
  <li>Learned Features are easy to adapt, fast to learn</li>
  <li>Deep learning provides a very flexible, (almost?) universal, learnable
framework for representing world, visual and linguistic information.</li>
  <li>Can learn both unsupervised and supervised</li>
  <li>Effective end-to-end joint system learning</li>
  <li>Utilize large amounts of training data</li>
</ul>

<h2 id="history">History</h2>

<ul>
  <li>In ~2012, deep learning (DL) started outperforming other machine learning (ML) techniques, first in speech and vision, then NLP</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/1.png" alt="1" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/2.png" alt="2" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/3.png" alt="3" /></p>

<h2 id="structure">Structure</h2>

<p>어떻게 DL이 좋은 성능을 내는가?</p>

<ul>
  <li>Fat + Short vs. Thin + Tall Networks</li>
  <li>The same number of parameters</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/4.png" alt="4" /></p>

<ul>
  <li>Deep → Modularization</li>
  <li></li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/5.png" alt="5" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/6.png" alt="6" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/7.png" alt="7" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/8.png" alt="8" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/9.png" alt="9" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/10.png" alt="10" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/11.png" alt="11" /></p>

<ul>
  <li>Before 2006, deeper usually does not imply better performance.</li>
  <li><a href="/a8916a1612b345718745d8cf339ab58b">Geoffrey Hinton showed how to train deep network in 2006 [1]</a>
    <ul>
      <li>Learned layers one by one</li>
    </ul>
  </li>
  <li><a href="/a8916a1612b345718745d8cf339ab58b">Deep Neural Networks showed good classification performance with large image data set in 2012. [2] </a>
    <ul>
      <li>GPU</li>
      <li>Big data</li>
      <li>Better learning algorithms</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/12.png" alt="12" /></p>

<p><a href="http://www.gizmodo.com.au/2015/04/the-basic-recipe-for-machinelearning-explained-in-a-single-powerpoint-slide/">http://www.gizmodo.com.au/2015/04/the-basic-recipe-for-machinelearning-explained-in-a-single-powerpoint-slide/</a></p>

<ul>
  <li>Rectified Linear Unit (ReLU)
    <ul>
      <li>Fast to compute</li>
      <li>Vanishing gradient problem</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/13.png" alt="13" /></p>

<ul>
  <li><a href="/a8916a1612b345718745d8cf339ab58b">[Xavier Glorot, AISTATS’11] [3]</a></li>
  <li><a href="/a8916a1612b345718745d8cf339ab58b">[Andrew L. Maas, ICML’13] [4]</a></li>
  <li><a href="/a8916a1612b345718745d8cf339ab58b">[Kaiming He, arXiv’15] [5]</a></li>
</ul>

<h1 id="dropout">Dropout</h1>

<p><a href="/a8916a1612b345718745d8cf339ab58b">Dropout [6]</a></p>

<ul>
  <li>Each time before computing the gradients</li>
  <li>Each neuron has $p \times 100 \%$ chance to be dropped
    <ul>
      <li>The structure of the network is changed</li>
    </ul>
  </li>
  <li>Use the new network for training</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/14.png" alt="14" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/15.png" alt="15" /></p>

<ul>
  <li>Weights should be multiplied by (1-p) when testing</li>
</ul>

<h2 id="training-stage">Training stage</h2>

<p>Assume dropout rate is 50%</p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/16.png" alt="16" /></p>

<h3 id="testing-stage">Testing stage</h3>

<p>No dropout</p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/17.png" alt="17" /></p>

<h1 id="convolutional-neural-network">Convolutional Neural Network</h1>

<h2 id="fully-connected-layer">Fully connected layer</h2>

<ul>
  <li>Example: 200x200 image * 40K hidden units → ~2B parameters</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/18.png" alt="18" /><em>Slide Credit: Marc’Aurelio Ranzato</em></p>

<ul>
  <li>Waste of resources + we have not enough training samples</li>
  <li>Spatial correlation is local</li>
</ul>

<h2 id="locally-connected-layer">Locally Connected Layer</h2>

<ul>
  <li>Example: 200x200 image * 40K hidden units → ~4M parameters (Filter size: 10x10)
Page 20- Spatial correlation is local</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/19.png" alt="19" /></p>

<ul>
  <li>Waste of resources + we have not enough training samples</li>
  <li>Spatial correlation is local</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/20.png" alt="20" /></p>

<ul>
  <li>Statistics is similar at different locations
    <ul>
      <li>Share the same parameters across</li>
    </ul>
  </li>
  <li>different locations (weight sharing)
    <ul>
      <li>Convolutions with learned kernels</li>
    </ul>
  </li>
</ul>

<h2 id="convolution-operation">Convolution operation</h2>

<p>$F(m,n) = f * h = \Sigma_{l= -\frac w 2}^{\frac f 2}\Sigma_{k= -\frac w 2}^{\frac w 2} {f(m+k, n+l) *h(\frac w 2 -k, \frac w 2 -l)}$</p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/21.png" alt="21" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/22.png" alt="22" /></p>

<ul>
  <li>If a feature is useful in some locations during training, detectors for that feature will be useful in all locations during testing</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/23.png" alt="23" /></p>

<h2 id="pooling">Pooling</h2>

<ul>
  <li>By “pooling” (e.g., taking max) filter responses at different locations, we gain robustness to the exact spatial location of features.</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/24.png" alt="24" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/25.png" alt="25" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/26.png" alt="26" /></p>

<h3 id="max-pooling">Max-pooling</h3>

<p>$h_i^n(r,c) = max_{\bar r \in N(r), \bar c \in N(c), } h_i^{n-1} (\bar r, \bar c)$</p>

<h3 id="average-pooling">Average-pooling</h3>

<p>$h_i^n(r,c) = mean_{\bar r \in N(r), \bar c \in N(c), } h_i^{n-1} (\bar r, \bar c)$</p>

<h3 id="l2-pooling">L2-pooling</h3>

<p>$h_i^n(r,c) = \sqrt{\Sigma_{\bar r \in N(r), \bar c \in N(c), } h_i^{n-1} (\bar r, \bar c)}$</p>

<h2 id="convolution-kernel-filter-examples">Convolution kernel (filter) examples</h2>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/27.png" alt="27" /></p>

<ul>
  <li>
    <p>Examples of learned object parts from object categories</p>

    <p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/28.png" alt="28" /></p>
  </li>
</ul>

<h2 id="lenet-5-1998">LeNet-5 (1998)</h2>

<p><a href="/a8916a1612b345718745d8cf339ab58b">[7] Le-Net</a></p>

<ul>
  <li>Yann LeCun and his collaborators developed a really good recognizer for
handwritten digits by using backpropagation in a feedforward net with
    <ul>
      <li>Many hidden layers (at that time),</li>
      <li>3 convolution layer,</li>
      <li>2 subsampling (pooling) layer</li>
      <li>5*5 convolution kernels,</li>
      <li>~340,000 connections,</li>
      <li>~60,000 parameter</li>
    </ul>
  </li>
  <li>
    <p>Used for reading ~10% of the checks in North America</p>

    <p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/29.png" alt="29" /></p>
  </li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/30.png" alt="30" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/31.png" alt="31" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/32.png" alt="32" /></p>

<h2 id="backpropagation-in-cnn">Backpropagation in CNN</h2>

<ul>
  <li>Same color shares the same weight</li>
  <li>Compute the gradients as usual, and then modify the gradients so that they satisfy the constraints</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/33.png" alt="33" /></p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/34.png" alt="34" /></p>

<ul>
  <li>The 82 errors made by LeNet5</li>
  <li>Notice that most of the errors are cases that people find quite easy.</li>
  <li>The human error rate is probably 20 to 30 errors.</li>
  <li>LeNet uses knowledge about the invariances to design:
    <ul>
      <li>local connectivity</li>
      <li>weight-sharing</li>
      <li>Pooling</li>
      <li>~ 80 errors</li>
    </ul>
  </li>
  <li>Using many different transformations of the input and other tricks (Ranzato2008)
    <ul>
      <li>~ 40 errors</li>
    </ul>
  </li>
  <li>Using carefully designed extra training data (Ciresan 2010)
    <ul>
      <li>For each training image, they produced many new training examples by applying many different transformations</li>
      <li>~ 35 errors</li>
    </ul>
  </li>
</ul>

<p>[Ciresan 2010][8]</p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/35.png" alt="35" /><em>PyTorch implementation of LeNet-5 for MNIST, https://github.com/radsn/LeNet5</em></p>

<ul>
  <li>The top printed digit is the right answer.</li>
  <li>The bottom two printed digits are the network’s best two guesses.</li>
  <li>The right answer is almost always in the top 2 guesses.</li>
  <li>With model averaging they can now get about 25 errors.</li>
</ul>

<h1 id="from-handwritten-digits-to-3-d-objects">From Handwritten Digits to 3-D objects</h1>

<ul>
  <li>Recognizing real objects in color photographs downloaded from the web is much more complicated than recognizing hand-written digits:
    <ul>
      <li>Hundred times as many classes (1000 vs 10)</li>
      <li>Hundred times as many pixels (256* 256 color vs. 28* 28 gray)</li>
      <li>Two dimensional image of three-dimensional scene</li>
      <li>Multiple objects in each image</li>
      <li>Cluttered background</li>
    </ul>
  </li>
  <li>Will the same type of convolutional neural network work?</li>
</ul>

<h1 id="the-ilsvrc-2012-competition-on-imagenet">The ILSVRC-2012 Competition on ImageNet</h1>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/36.png" alt="36" /></p>

<ul>
  <li>[9] ImageNet
    <ul>
      <li>Over 15 million labeled high-resolution images</li>
      <li>Roughly 22,000 categories</li>
      <li>Collected from the web</li>
      <li>Labeled by human using Amazon’s Mechanical Turk crowd-sourcing tool</li>
    </ul>
  </li>
  <li>ImageNet Large-Scale Visual Recognition Challenge (ILSVRC)
    <ul>
      <li>Uses a subset of ImageNet</li>
      <li>1,000 categories</li>
      <li>1.2 million training images</li>
      <li>50,000 validation images</li>
      <li>150,000 test images</li>
    </ul>
  </li>
  <li>The classification task:
    <ul>
      <li>Get the “correct” class in your top 5 bets. There are 1000 classes.</li>
    </ul>
  </li>
  <li>The localization task:
    <ul>
      <li>For each bet, put a box around the object. Your box must have at least 50%
  overlap with the correct box.</li>
    </ul>
  </li>
  <li>Some of the best existing computer vision methods were tried on this dataset by leading computer vision groups from Oxford, INRIA, XRCE(XEROX), …
    <ul>
      <li>Computer vision systems use complicated multi-stage systems</li>
      <li>The early stages are typically hand-tuned by optimizing a few parameters</li>
    </ul>
  </li>
</ul>

<h2 id="examples-from-the-test-set-with-the-networks-guesses">Examples from the test set (with the network’s guesses)</h2>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/37.png" alt="37" /></p>

<p>Error rates on the ILSVRC-2012 competition</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>classification</th>
      <th>classification &amp; localization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>UToronto</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>16.4%</td>
      <td>34.1%</td>
      <td> </td>
    </tr>
    <tr>
      <td>UTokyo</td>
      <td>26.1%</td>
      <td>53.6%</td>
    </tr>
    <tr>
      <td>Oxford University Computer Vision Group</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>26.9%</td>
      <td>50.0%</td>
      <td> </td>
    </tr>
    <tr>
      <td>INRIA + XRCE</td>
      <td>27.0%</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>undefined- UToronto (deep learning - Alex Krizhevsky, AlexNet)</p>
<ul>
  <li>INRIA (French national research institute in CS) + XRCE (Xerox Research Center Europe)</li>
</ul>

<h1 id="a-cnn-for-imagenet">A CNN for ImageNet</h1>

<p>AlexNet[10]</p>

<ul>
  <li>Alex Krizhevsky (NIPS 2012) developed a very deep convolutional neural net of the type pioneered by Yann LeCun.</li>
  <li>7 hidden layers not counting some max pooling layers</li>
  <li>The early layers are convolutional, the last two layers are fully connected</li>
  <li>The activation functions are
    <ul>
      <li>Rectified linear units in every hidden layer. These train much faster and are more expressive than sigmoid.</li>
      <li>Normalization for better activation</li>
    </ul>
  </li>
  <li>Use “dropout” to regularize the weights in the fully connected layers</li>
  <li>224<em>224 patches are taken from the 256</em>256 images (10 different versions) and leftright reflections are used to get more data</li>
  <li>Used all 10 different patches at test time</li>
</ul>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/38.png" alt="38" /></p>

<h2 id="more-examples-from-alexnet">More examples from AlexNet</h2>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/39.png" alt="39" /></p>

<h2 id="hardware-for-alexnet">Hardware for AlexNet</h2>

<ul>
  <li>He uses a very efficient implementation of convolutional nets on two NvidiaGTX 580 Graphics Processor Units (over 1000 fast little cores)
    <ul>
      <li>GPUs are very good for matrix-matrix multiplies.</li>
      <li>GPUs have very high bandwidth to memory.</li>
      <li>This allows him to train the network in a week.</li>
      <li>It also makes it quick to combine results from 10 patches at test time.</li>
    </ul>
  </li>
  <li>We can spread a network over many cores if we can communicate the states fast enough.</li>
  <li>As cores get cheaper and datasets get bigger, big neural nets will improve faster than old-fashioned computer vision systems.</li>
</ul>

<h1 id="evolution-of-the-dnn">Evolution of the DNN</h1>

<ul>
  <li>Network depths and the performance</li>
  <li>
    <p>ILSVRC classification error (top-5 error)</p>

    <p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/40.png" alt="40" /></p>
  </li>
</ul>

<h1 id="fully-convolutional-networks">Fully Convolutional Networks</h1>

<ul>
  <li>
    <p>Fully connected layer constrains the input image size</p>

    <p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/41.png" alt="41" /></p>
  </li>
  <li>
    <p>Fully convolutional network structure has no constrains on the input image size</p>

    <p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/42.png" alt="42" /></p>
  </li>
</ul>

<h1 id="machine-learning-deep-learning-data-mining-big-data">Machine Learning, Deep Learning, Data Mining, Big data</h1>

<h2 id="big-data">Big Data</h2>

<ul>
  <li>기존 데이터의 크기 범주를넘어서는 규모(2010~)</li>
  <li>기존 데이터 처리 이슈 공유</li>
  <li>대용량 데이터 분산저장/처리 방법 필요</li>
</ul>

<h2 id="machine-learning">Machine Learning</h2>

<ul>
  <li>데이터의 속성을 일반적으로 분석하는 방법, 주로 분류/회귀 작업에 사용됨(1950~)</li>
  <li>지도학습/비지도학습/군집화</li>
  <li>체스 게임으로부터 발전</li>
</ul>

<h2 id="deep-learning-1">Deep learning</h2>

<ul>
  <li>심층 인공신경망 기술을 사용하는 기계학습 방법(2010~)</li>
  <li>기존 기계학습 방법의 성능을 뛰어 넘음</li>
</ul>

<h2 id="data-mining">Data Mining</h2>

<ul>
  <li>데이터에 내재된 속성을 분석 (1930~)</li>
  <li>기계학습과 유사하나 데이터 간의 규칙을 분석하는 측면으로 차별화</li>
</ul>

<h2 id="some-history">Some history</h2>

<ul>
  <li>
    <p>Frank Rosenblatt, Perceptron (1957, 1962): Early description and engineering of single-layer and multilayer artificial neural networks.</p>

    <p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/43.png" alt="43" /></p>
  </li>
  <li>
    <p>Kasparov vs Deep Blue, 1997</p>

    <p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/44.png" alt="44" /></p>
  </li>
  <li>
    <p>Lee Sedol vs AlphaGo, 2016</p>

    <p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/45.png" alt="45" /></p>
  </li>
</ul>

<h3 id="timelines">timelines</h3>

<ul>
  <li>1943: Neural networks</li>
  <li>1957-62: Perceptron</li>
  <li>1970-86: Backpropagation, RBM, RNN</li>
  <li>1979-98: CNN, MNIST, LSTM, Bidirectional RNN</li>
  <li>2006: “Deep Learning”, DBN• 2009: ImageNet + AlexNet</li>
  <li>2014: GANs</li>
  <li>2016-17: AlphaGo, AlphaZero</li>
  <li>Turing Award given for:
    <ul>
      <li>“The conceptual and engineering breakthroughs that have made deep neural
  networks a critical component of computing.”
  • Yann LeCun
  • Geoffrey Hinton
  • Yoshua Bengio</li>
    </ul>
  </li>
</ul>

<h2 id="limitations-of-deep-learning">Limitations of Deep Learning</h2>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/46.png" alt="46" /></p>

<ul>
  <li>Prediction from Rodney Brooks:
“By 2020, the popular press starts having stories that the era of Deep Learning is over.”</li>
  <li>2019 is the year it became cool to say that “deep learning” has limitations.</li>
  <li>Books, articles, lectures, debates, videos were released that learning-based methods cannot do commonsense reasoning.</li>
</ul>

<h2 id="statics-of-acceptance-rate-neurips">Statics of acceptance rate NeurIPS</h2>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/47.png" alt="47" /></p>

<h2 id="deep-learning-frameworktoolkits">Deep Learning Framework/Toolkits</h2>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/48.png" alt="48" /></p>

<h2 id="alexnet">AlexNet</h2>

<p>https://sushscience.wordpress.com/2016/12/04/understanding-alexnet/</p>

<p><img src="/assets/img/2023-05-24-9.-DNN-Deep-Neural-Network-(1).md/49.png" alt="49" /></p>

<hr />

<p>[1] Geoffrey Hinton showed how to train deep network in 2006</p>

<p>[2] Deep Neural Networks showed good classification performance with large image data set in 2012.</p>

<p><a href="/a8916a1612b345718745d8cf339ab58b">[3] [Xavier Glorot, AISTATS’11]</a></p>

<p>Deep Sparse Rectifier Neural Networks. <em><strong>Xavier Glorot, Antoine Bordes, Yoshua Bengio</strong></em>
 <em>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics.</em> PMLR 15:315-323, 2011.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- [https://proceedings.mlr.press/v15/glorot11a.html](https://proceedings.mlr.press/v15/glorot11a.html)
</code></pre></div></div>

<p><a href="/a8916a1612b345718745d8cf339ab58b">[4] [Andrew L. Maas, ICML’13] </a></p>

<p>Rectifier nonlinearities improve neural network acoustic models (2013) by Andrew L. Maas , Awni Y. Hannun , Andrew Y. Ng</p>

<p><a href="https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf</a></p>

<p><a href="/a8916a1612b345718745d8cf339ab58b">[5] [Kaiming He, arXiv’15] </a></p>

<p>Deep Residual Learning for Image Recognition📹
by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</p>

<p><a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></p>

<p><a href="/a8916a1612b345718745d8cf339ab58b">[6] Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></p>

<p><em><strong>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov</strong></em>; 15(56):1929−1958, 2014.</p>

<p><a href="https://jmlr.org/papers/v15/srivastava14a.html">https://jmlr.org/papers/v15/srivastava14a.html</a></p>

<p><a href="/a8916a1612b345718745d8cf339ab58b">[7] Le-Net</a></p>

<p><a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf</a></p>

<p>[8] [Ciresan 2010]</p>

<p>[9] ImageNet</p>

<p>ImageNet: A large-scale hierarchical image database</p>

<p>by Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei</p>


  </div><a class="u-url" href="/9.-DNN-Deep-Neural-Network-(1)" hidden></a>
</article>


      </div>
    </main><footer>
  <hr style="margin-left: 15px; color: #000 !important; background-color: #000 !important; opacity: 1 !important; width: 40px; height: .5px !important; margin-bottom: 2rem !important;">
  <div class="row m-0">
    <div class="col-12 col-md-3 col-sm-4 mb-4 mb-sm-0">
      <p>
        <a href="https://github.com/underthelights/underthelights.github.io" style="font-weight: 300">Kyuhwan Shim</a>
        <br>Up-to-date as of <span id="date">loading...</span></p>
      <i class="fas fa-location-dot mr-1"></i> Seoul is <img style="width: auto; height: 23px;" id="weather_icon">
      <span id="weather">Loading...</span>
    </div>
    <div class="col-12 col-md-9 col-sm-8">
      <p>
        <a style="font-weight: 300" href="https://linkedin.com/in/kyuhwan-shim"><i class="fab fa-linkedin" style="margin-right: 10.5px"></i>LinkedIn</a><br>
        <a style="font-weight: 300" href="https://fb.com/s.kyuhwn"><i class="fab fa-facebook" style="margin-right: 8px"></i>Facebook</a><br>
        <a style="font-weight: 300" href="https://www.instagram.com/s.kyuhwn"><i class="fab fa-instagram" style="margin-right: 10.5px"></i>Instagram</a><br>
        <a style="font-weight: 300" href="https://github.com/underthelights"><i class="fab fa-github" style="margin-right: 10.5px"></i>GitHub</a><br>
      </p>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.18.1/moment.min.js"></script>

<script src="/assets/js/weather.js"></script>
<script src="/assets/js/github_date.js"></script>
</body>

</html>

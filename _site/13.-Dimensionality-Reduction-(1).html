<!DOCTYPE html>
<html lang="en"><head>
  
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         TeX: {
         equationNumbers: {
             autoNumber: "AMS"
         }
         },
         tex2jax: {
         inlineMath: [ ['$', '$'] ],
         displayMath: [ ['$$', '$$'] ],
         processEscapes: true,
     }
     });
     MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
         alert("Math Processing Error: "+message[1]);
         });
     MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
         alert("Math Processing Error: "+message[1]);
         });
     </script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>13. Dimensionality Reduction (1) ✱ Kyuhwan Shim</title>

  <link rel="stylesheet" href="/assets/css/style.css">

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Barlow:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800&family=Noto+Sans+TC&display=swap" rel="stylesheet">

  <link href="/assets/fontawesome/all.min.css" rel="stylesheet">

  <!-- Bootstrap -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js'></script>

  <!-- GA -->
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VG8LB4J4EL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VG8LB4J4EL');
</script>


  <!-- favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/images/main/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/images/main/favicon/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/images/main/favicon/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/images/main/favicon/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/images/main/favicon/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/images/main/favicon/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/images/main/favicon/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/images/main/favicon/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/images/main/favicon/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/main/favicon/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/images/main/favicon/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/main/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/images/main/favicon/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/main/favicon/favicon-16x16.png">
  <link rel="manifest" href="">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/images/main/favicon/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  <!-- Opengraph -->
  <meta property="og:type" content="website">
  <meta property="og:image" content="/assets/images/main/favicon/og.png13.-Dimensionality-Reduction-(1).jpg">
  <meta property="og:title" content="13. Dimensionality Reduction (1) | Kyuhwan Shim">
  <meta property="og:description" content="Paper overview of  et al., ">

  <!-- Google scholar -->
  
  <meta name="citation_title" content="13. Dimensionality Reduction (1)">
  
  <meta name="citation_publication_date" content="">
  
  <meta name="citation_conference_title" content="">
  
  <meta name="citation_pdf_url" content="https://underthelights.github.io/assets/pdf/13.-Dimensionality-Reduction-(1).pdf">
  
</head>
<body><header class="page-content">
  <nav class="navbar navbar-expand-md navbar-light py-4">
    <div class="container-fluid">
      <button class="navbar-toggler ms-auto" type="button" data-bs-toggle="collapse" data-bs-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
        <ul class="navbar-nav ms-auto mt-4 mt-lg-0 navbar-nav-scroll">
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/news">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/publication">publication</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/project">Project</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/oconnect">oconnect</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/cv">CV</a>
          </li>
          <li class="nav-item dropdown">
              <a href="#" class="nav-link dropdown-toggle " data-bs-toggle="dropdown">fun</a>
              <div class="dropdown-menu">
                  <a href="/music" class="dropdown-item">music</a>
                  <a href="/artwork" class="dropdown-item">artwork</a>
              </div>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" target="_blank" rel="noopener noreferrer" href="https://underthelights.github.io/blog/">Blog<i class="fa-regular fa-arrow-up-right-from-square" style="padding-left: 5px;"></i></a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</header>

<style>
  .dropdown-toggle {
    background-color: transparent;
    border-color: #fff;
    border-style: solid;
    border-top: none;
    border-right: none;
    border-left: none;
    transition: color .15s ease-in-out, background-color .15s ease-in-out,border-color .15s ease-in-out;
  }
  .dropdown-toggle:hover {
    font-weight: 500
  }
  .dropdown:hover .dropdown-menu {
    display: block;
    margin-top: 0;
 }
 .dropdown-menu {
    --bs-dropdown-border-radius: 0 !important;
    padding-top: 0 !important;
    padding-bottom: 0 !important
 }
 .dropdown-item {
  font-size: .95rem !important;
  padding: .3rem .75rem !important;
 }
 .dropdown-item:active {
  background-color: #999 !important
 }
</style><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <style>
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    
      font-family: 'Arial', sans-serif;
      /* Use a modern font */
      margin-top: 20px;
      margin-bottom: 10px;
  }
  /* 포스트 헤더에 적용되는 여백 제거 */
  .post-header, h1, h2, h3, h4, h5, h6 {
      margin-left: 0 !important; 
      padding-left: 0 !important;
  }


  .post-title {
      font-size: 2em; /* 폰트 크기 증가 */
      font-weight: bold; /* 볼드체 */
      color: #333; /* 색상 변경 */
      box-shadow: inset 0 -20px 0 #bbb7e8;
      max-width: max-content;
    }

  /* Updated styling for h1 */
  h1 {
    font-size: 1.5em;
    /* Increase font size for main titles */
    box-shadow: inset 0 -10px 0 #bbb7e8;
    max-width: max-content; 
    font-weight: bold; /* 볼드체 */
    border-bottom: none;
    /* Remove bottom border */
    padding-bottom: 0;
    /* Adjust padding if needed */
  }

  h2 {
    font-size: 1.2em;
    margin-left: none;
    font-weight: bold; /* 볼드체 */
    max-width: max-content; 
    background-color: #cdcbe9;
  }

  h3 {
    font-size: 1.1em;
    max-width: max-content; 
    
    max-width: max-content; 
    background-color: #bbb7e8;
  }

  h4 {
    font-size: 0.75em;
    color: #888888;
  }

  h5 {
    font-size: 0.5em;
    color: #a6a6a6;
  }

  h6 {
    font-size: 1em;
    color: #bcbcbc;
  }

  /* Add some additional styling if needed */
  .post-content {
    margin-left: 20px; /* 왼쪽 여백 추가 */
    /* margin-right: 20px; 오른쪽에 TOC 너비 + 여백 만큼 추가 */
    line-height: 1.6;/* Improve readability */
    color: #333;/* Dark grey color for text */
  }
  /* Category Chart Style */
  .category-chart {
    font-family: 'Arial', sans-serif;
    /* width: 100%; */
    /* height: 100px; Adjust as needed */
  }

  .category-list li {
    display: inline-block;
    margin-right: 20px;
  }
  /* 이미지 스타일 */
  img {
    display: block; /* 이미지를 블록 레벨로 설정 */
    width: 50%; /* 또는 원하는 너비 */
    height: auto; /* 비율 유지 */
    margin-bottom: 5px; /* 이미지와 캡션 사이의 간격 */
    text-align: center;
  }

  /* 이미지 캡션 스타일링 */
  .em {
    color: #757575;
    font-size: 0.8em;
    
    /* margin-top: 5px; */
    display: block; /* 캡션을 블록 요소로 설정, 이미지 아래로 강제 배치 */
    text-align: center;
    margin-bottom: 5px;
  }
  /* Blockquote 스타일 */
  blockquote {
      margin-left: 20px;
      border-left: 1.5px solid #ccc; /* 좌측 선 */
      border-top: 1.5px solid #ccc; /* 좌측 선 */
      border-right: 1.5px solid #ccc; /* 좌측 선 */
      border-bottom: 1.5px solid #ccc; /* 좌측 선 */

      padding-left: 15px;
      color: #666; /* 글자 색상 */
  }



</style>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <!-- Table of Contents Container -->
<div class="toc-container">
    <nav class="toc">
      <strong>Table of Contents</strong>
      <!--  -->
      <!-- Jekyll TOC Liquid Code -->
      <ul><li><a href="#introduction-of-dimensionality-reduction">Introduction of ‘Dimensionality Reduction’</a><ul><li><a href="#dimensionality-reduction의-목적">Dimensionality Reduction의 목적</a></li></ul></li><li><a href="#problems">Problems</a></li><li><a href="#feature-extractiondimensionality-reduction-1">Feature extraction/Dimensionality reduction 1</a><ul><li><a href="#dimensionality-reduction의-목적-revisited">Dimensionality Reduction의 목적 (Revisited)</a></li></ul></li><li><a href="#data-compression">Data Compression</a><ul><li><a href="#reduce-data-from-3d-to-2d">Reduce data from 3D to 2D</a></li></ul></li><li><a href="#feature-extractiondimensionality-reduction-2">Feature extraction/Dimensionality reduction 2</a><ul><li><a href="#basic-principle">Basic Principle</a></li></ul></li><li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a><ul><li><a href="#calculation">Calculation</a></li><li><a href="#feature-extractiondimensionality-reduction-3">Feature extraction/Dimensionality reduction 3</a></li><li><a href="#covariance">Covariance</a></li><li><a href="#식">식</a></li><li><a href="#pca-example">PCA Example</a></li></ul></li><li><a href="#eigenface">Eigenface</a></li><li><a href="#sift-feature-visualization">SIFT feature visualization</a></li><li><a href="#pca-vs-lda">PCA vs. LDA</a><ul><li><a href="#pca-vs-lda-face-recognition-accuracy">PCA vs. LDA Face recognition accuracy</a></li><li><a href="#identification">Identification</a></li><li><a href="#verification">Verification</a></li></ul></li></ul>

      <!-- <ul><li><a href="#introduction-of-dimensionality-reduction">Introduction of ‘Dimensionality Reduction’</a><ul><li><a href="#dimensionality-reduction의-목적">Dimensionality Reduction의 목적</a></li></ul></li><li><a href="#problems">Problems</a></li><li><a href="#feature-extractiondimensionality-reduction-1">Feature extraction/Dimensionality reduction 1</a><ul><li><a href="#dimensionality-reduction의-목적-revisited">Dimensionality Reduction의 목적 (Revisited)</a></li></ul></li><li><a href="#data-compression">Data Compression</a><ul><li><a href="#reduce-data-from-3d-to-2d">Reduce data from 3D to 2D</a></li></ul></li><li><a href="#feature-extractiondimensionality-reduction-2">Feature extraction/Dimensionality reduction 2</a><ul><li><a href="#basic-principle">Basic Principle</a></li></ul></li><li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a><ul><li><a href="#calculation">Calculation</a></li><li><a href="#feature-extractiondimensionality-reduction-3">Feature extraction/Dimensionality reduction 3</a></li><li><a href="#covariance">Covariance</a></li><li><a href="#식">식</a></li><li><a href="#pca-example">PCA Example</a></li></ul></li><li><a href="#eigenface">Eigenface</a></li><li><a href="#sift-feature-visualization">SIFT feature visualization</a></li><li><a href="#pca-vs-lda">PCA vs. LDA</a><ul><li><a href="#pca-vs-lda-face-recognition-accuracy">PCA vs. LDA Face recognition accuracy</a></li><li><a href="#identification">Identification</a></li><li><a href="#verification">Verification</a></li></ul></li></ul> -->
    </nav>
</div>

<style>
    /* TOC 스타일 */
.toc-container {
  position: fixed;
  right: 10px;
  top: 100px;
  z-index: 1000;
  /* max-width: 50px; 가로 너비 조정, 필요에 따라 값을 변경하세요 */
  font-size: 0.75rem;
}

.toc {
  border: 1px solid #ddd;
  padding: 10px;
  border-radius: 5px;
  background-color: white;
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  max-width: 200px; /* 가로 너비 조정, 필요에 따라 값을 변경하세요 */
}


.toc strong {
  display: block;
  margin-bottom: 10px;
}

.toc ul {
  list-style: none;
  padding: 0;
  margin: 0;
}

.toc ul li a {
  text-decoration: none;
  color: #007bff;
  display: block;
  padding: 5px 0;
}

.toc ul li a:hover,
.toc ul li a.active { /* 추가된 active 클래스 스타일 */
  font-weight: bold;
  /* box-shadow: inset 0 -10px 0 #bbb7e8; */
  background-color: #bbb7e8;
}



  button {
    padding: 5px 10px;
    /* 상하 10px, 좌우 20px 패딩으로 텍스트 주변 여유 공간 추가 */
    font-size: 0.87rem;
    /* 글자 크기 */
    color: white;
    /* 글자 색상 */
    background-color: #007bff;
    /* 배경 색상 */
    border: none;
    /* 테두리 제거 */
    border-radius: 5px;
    /* 모서리 둥글게 */
    cursor: pointer;
    /* 커서 모양 변경 */
    transition: background-color 0.3s;
    /* 호버 효과를 위한 전환 설정 */
    text-align: center;
    /* 글자를 버튼 중앙에 위치 */
    display: inline-block;
    /* 인라인 블록 요소로 설정하여 자연스럽게 텍스트 중앙 정렬 */
    line-height: normal;
    /* 기본 라인 높이 설정 */
    vertical-align: middle;
    /* 수직 방향으로 중앙 정렬 */
    margin-top: 5px;
  }

  button:hover {
    background-color: #0056b3;
    /* 호버 시 배경 색상 변경 */
  }


  /* reset base styles */
  * {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
  }

  /* page header */
  header {
    margin-bottom: 2rem;
    padding-left: 6.5rem;
  }

  header h1 {
    font-size: 1.5rem;
  }

  header p {
    margin: .5rem 0;
    font-size: 1rem !important
  }

  main b {
    font-weight: 500
  }

  /* normal body content */
  h2 {
    font-size: 1.1rem;
    margin-bottom: 0.75rem;
    margin-left: 6.5rem;
    text-transform: uppercase;
  }

  h3 {
    border-bottom: 1px solid black;
    font-size: .9rem;
    margin: 1rem 0 .5rem 6.5rem
  }

  p {
    margin-bottom: 0.5rem;
  }

  a {
    color: inherit;
    /*#0000ee;*/
  }

  section {
    margin-bottom: 3rem;
  }

  /* misc */
  .pdf {
    font-size: .9rem !important;
    font-weight: 300;
    margin-left: 1.5rem;
    white-space: nowrap;
  }

  .pdf i {
    margin-right: .1rem;
  }

  .material {
    font-size: small;
    margin-left: .5rem;
  }

  :global(i) {
    padding-right: 4px !important
  }

  /* dated entries */
  .dated-entry {
    display: flex;
    flex-flow: row wrap;
    position: relative;
    margin-bottom: 1rem;
  }

  .dated-date {
    width: 6.5rem;
    text-align: right;
    padding-top: .15rem;
    padding-right: 1.5rem;
    font-size: .8rem
  }

  .dated-content {
    width: calc(100% - 6.5rem);
    font-size: .95rem
  }

  .oneline-entries {
    margin-bottom: 0.5rem;
  }

  .oneline-entries .dated-entry {
    margin-bottom: 0;
  }

  /* hide extra awards info for now, not sure what to include */
  #awards em {
    display: none;
  }

  .author-tooltip {
    font-weight: 400;
    font-size: .8rem !important;
    text-align: center;
  }

  /* on narrow displays, make the font smaller */
  @media (max-width: 480px) {
    html {
      font-size: 14px;
    }
  }

  /* when printing, make the font smaller and the page full-width */
  @media print {
    html {
      font-size: 12px;
    }

    main {
      margin-top: 0;
      max-width: 100%;
    }
  }

</style>

<script>
    document.addEventListener('scroll', function() {
      var sections = document.querySelectorAll('section'); // 섹션 선택자 수정
      var menu_links = document.querySelectorAll('.toc a'); // TOC 링크 선택자 수정
    
      var fromTop = window.scrollY;
    
      sections.forEach(function(section) {
        if (section.offsetTop <= fromTop && section.offsetTop + section.offsetHeight > fromTop) {
          menu_links.forEach(function(link) {
            if (section.getAttribute('id') && link.getAttribute('href').includes(section.getAttribute('id'))) {
              link.classList.add('active');  // 현재 섹션의 TOC 링크에 'active' 클래스 추가
            } else {
              link.classList.remove('active'); // 다른 모든 링크에서 'active' 클래스 제거
            }
          });
        }
      });
    });
    </script>
    

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">13. Dimensionality Reduction (1)</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-06-07T00:00:00+02:00" itemprop="datePublished">
        Jun 7, 2022
      </time></p>
    <!-- 태그와 카테고리 표시 -->
    <div class="post-categories">
      
      <strong><span>📚 Categories:</span></strong>
      
      <a href="blog-categories-notes">Notes</a>
      
      
    </div>

    <div class="post-tags">
      
      <strong><span>🏷️ Tags:</span></strong>
      
      <a href="blog-tags-ml">ML</a>
      
      
    </div>


  </header>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction-of-dimensionality-reduction">Introduction of ‘Dimensionality Reduction’</h1>

<ul>
  <li>Unsupervised learning에서 중요한 dimensionality reduction
    <ul>
      <li>PCA, LDA 방식으로 수행 : eigen equation을 생성하여 covariance 생성
        <ul>
          <li>Covariance를 구하고 data var이 큰 방향으로 방향 vector를 구함 : eigenvector</li>
          <li>분산 량 : eigenvalue</li>
        </ul>
      </li>
      <li>분산 큰 쪽으로 좌표축을 잡아 표현하는 방법 → dimension(data attribute) 줄이는 방법 (차원 축소)</li>
    </ul>
  </li>
  <li>PCA Principal Component Analysis - label X
    <ul>
      <li>unsupervised learning</li>
    </ul>
  </li>
  <li>LDA Linear Discriminat Analysis - label O
    <ul>
      <li>supervised learning</li>
    </ul>
  </li>
  <li>Semisupervised learning : unsupervised learning (부분적으로 Supervised)
    <ul>
      <li>물론 Clustering할 때도 일부 data에 label이 적용되어 알고 있을 수 있다 → label끼리 clustering할 때 정보를 활용하여 group화 할수 있음 (semi-supervised = unsupervised + supervised)</li>
    </ul>
  </li>
</ul>

<h2 id="dimensionality-reduction의-목적">Dimensionality Reduction의 목적</h2>

<ul>
  <li>1) Visualization 용이 : 3차원 이하로 Reduct하면 visualize 용이해짐</li>
  <li>2) Performace 향상 : 데이터 다루기 쉬워져 performance (acc) 향상,</li>
  <li>3) computation cost 감소 (time, computation 복잡도, memory)</li>
</ul>

<p>PCA, LDA</p>

<h1 id="problems">Problems</h1>

<ul>
  <li>Image에 있는 특정 Object검출 / 분류 / 영역 segmentation / caption / 임의의anomaly 검출 / Etc
    <ul>
      <li>image detection : window 를 지정하고 obj가 있는지 없는지 O,X로 검색</li>
      <li>image scan하며 반복 : 전체 Image 크기의 일정 비율만큼 되는지까지 반복 - 1초에 10장 이상의 Image window, 얼굴 하나만 검출해도 각 window에 대해 계속 반복해야 하므로</li>
      <li>computationally intensive → 연산량을 줄이기 위한 dimensionality reduction</li>
    </ul>
  </li>
  <li>Object Detection: Many detection windows
    <ul>
      <li>Each window is very high dimensional data</li>
    </ul>

    <p>→ computationally intensive (집약적)</p>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/0.png" alt="0" /></p>

<ul>
  <li>
    <p>General framework</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/1.png" alt="1" /></p>

    <ul>
      <li>고차원 Data</li>
      <li>Feature extracted된 것은 일반적으로 reducted된 형태</li>
      <li>Classifier / detection / segmentation</li>
      <li>input data 가 image / signal/ video/ audio/ text이건 General 하게 사용가능</li>
    </ul>
  </li>
</ul>

<h1 id="feature-extractiondimensionality-reduction-1">Feature extraction/Dimensionality reduction 1</h1>

<h2 id="dimensionality-reduction의-목적-revisited">Dimensionality Reduction의 목적 (Revisited)</h2>

<ul>
  <li>1) Visualization 용이 : 3차원 이하로 Reduct하면 visualize 용이해짐</li>
  <li>2) Performace 향상 : 데이터 다루기 쉬워져 performance (acc) 향상,</li>
  <li>3) computation cost 감소 (time, computation 복잡도, memory)</li>
  <li>It is impossible to process raw image data (pixels) directly
    <ul>
      <li>Too many of them (or data dimensionality too high)
        <ul>
          <li>data양이 너무 많음 : million cell을 넘어선 FHD, UHD 등 Resolution 이상 증가</li>
          <li>e.g. 1M - 2M - 4M pixel 증가하기에 그대로 쓸 수는 없음 : 그대로 DNN에 넣어 처리하지는 않고 500*500 =250K정도로 줄여서 사용함</li>
        </ul>
      </li>
      <li>Curse of dimensionality problem (차원의 저주)
        <ul>
          <li>Data dimension은 attribute로서 1d-2d-…-nd (data의 RGB, 길이, 크기 등) 많으면 많을수록 좋음 (더 많은 정보를 취합해서 처리할 수 있음)</li>
          <li>그러나 너무 많아지면 오히려 accuracy가 떨어지게 되는 현상</li>
          <li>RGB의 값과 무게 넓이를 측정하는데 Measured value 라는 것은 observed value로서 Boolean값이 아니기에 오차가 있을 수 있음.
            <ul>
              <li>(Boolean이 아닌, 오차가 포함된 observed value라면 값을 많이 추가하더라도 오히려 accuracy가 떨어지게 됨)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Process the raw pixel to produce a smaller set of numbers which will capture most information contained in the original data – this is often called a feature vector
    <ul>
      <li>Raw data 원본 data를 처리하여 smaller set으로 만듬</li>
      <li>d차원에서 임의로 10개 뽑아 정리하는게 아닌, 원본 data의 최대한 많은 정보를 뽑을 수 있도록 feature vector extract</li>
    </ul>
  </li>
  <li>Given data points in d-dimensions
    <ul>
      <li>Convert them to data points in $k(&lt;d)$ dimensions</li>
      <li>k의 약 10% 정도로 d를 다룬다</li>
      <li>With minimal loss of information</li>
    </ul>
  </li>
</ul>

<h1 id="data-compression">Data Compression</h1>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/2.png" alt="2" /></p>

<ul>
  <li>z_1 상에 proejction한 형태로 reduct dimensionailty</li>
</ul>

<h2 id="reduce-data-from-3d-to-2d">Reduce data from 3D to 2D</h2>

<ul>
  <li>어떤 방향으로 Data를 투영하느냐에 따라 분포 모양이 달라짐</li>
  <li>너무 뭉쳐서 투영되면 좋지 않은 reduction</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/3.png" alt="3" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/4.png" alt="4" /></p>

<h1 id="feature-extractiondimensionality-reduction-2">Feature extraction/Dimensionality reduction 2</h1>

<ul>
  <li>Suppose we have a population measured on p random variables x1, …, xd.
    <ul>
      <li>random variable</li>
    </ul>
  </li>
  <li>Note that these random variables represent the d-axes of the Cartesian coordinate system in which the population resides. Our goal is to develop a new set of k axes (linear combinations of the original d axes) in the directions of greatest variability: X2
    <ul>
      <li>cartesian coordinate system : 새로운 axis를 찾아내서 더 잘 Represent하도록 표현</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/5.png" alt="5" /></p>

<ul>
  <li>This is accomplished by rotating the axes 원래의 좌표축을 rotate하는 방법으로 진행
    <ul>
      <li>Rotates multivariate dataset into a new configuration which is easier to interpret</li>
      <li>Purpose
        <ul>
          <li>simplify data (잘 압축하여 간소화)</li>
          <li>look at relationships between variables , patterns of units (Data 간 관계, 패턴 분석 가능)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="basic-principle">Basic Principle</h2>

<ul>
  <li>From a raw data (vector) X of d-dimension to a new vector Y of k-dimensional (k &lt; &lt; d) via a transformation matrix A such that Y will capture most information in X</li>
</ul>

\[Y = AX\]

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/6.png" alt="6" /></p>

<ul>
  <li>원래 데이터 X에 변환 Matrix A를 곱하여 Y라는 변환된 matrix</li>
  <li>변환된 matrix : 회전된 좌표축에서의 data
    <ul>
      <li>→ X matrix를 raw matrix로 간주하고 d dimension to k dimension matrix</li>
    </ul>
  </li>
</ul>

<h1 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h1>

<ul>
  <li>Goal: find k-dim projection that best preserves variance</li>
</ul>

<h2 id="calculation">Calculation</h2>

<h2 id="feature-extractiondimensionality-reduction-3">Feature extraction/Dimensionality reduction 3</h2>

<ul>
  <li>We can compress the data by only using the top few eigenvectors
    <ul>
      <li>Corresponds to choosing a “<u>linear subspace</u>”
        <ul>
          <li>일정 constant를 곱한 형태이므로 linear</li>
        </ul>
      </li>
      <li>These eigenvectors are known as the <u>principal components</u></li>
    </ul>
  </li>
</ul>

<h2 id="covariance">Covariance</h2>

<ul>
  <li>각 차원 별 Relation (corelation)</li>
  <li>Variance and Covariance:
    <ul>
      <li>Measure of the “spread” of a set of points around their center of mass (mean)</li>
      <li>각 차원에서의 data가 얼마나 많이 분산되어 있느냐에 대한 정보 제공</li>
    </ul>
  </li>
  <li>Variance
    <ul>
      <li>Measure of the deviation from the mean for points in <u>one dimension</u></li>
    </ul>
  </li>
  <li>
    <p>Covariance</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/7.png" alt="7" /></p>

    <ul>
      <li>Measure of how much each of the dimensions vary from the mean with <u>respect to each other</u></li>
      <li>여러 차원에 걸쳐 분산을 계산하면 -&gt; covariance정보 얻을 수 있음</li>
      <li>Covariance is measured between two dimensions</li>
      <li>Covariance sees if there is a relation between two dimensions</li>
      <li>Covariance in one dimension is the variance</li>
    </ul>
  </li>
  <li>Positive: Both dimensions increase or decrease together
    <ul>
      <li>한 차원의 값이 증가하면 다른 차원의 값도 증가</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/8.png" alt="8" /></p>

<ul>
  <li>Negative: While one increase the other decrease
    <ul>
      <li>한 차원의 값이 증가하면 다른 차원의 값은 반대로 감소</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/9.png" alt="9" /></p>

<h2 id="식">식</h2>

\[\Sigma = \frac 1 N \Sigma_{i=1}^{N}{(x_i -\mu)(x_i -\mu)^T}\]

<ul>
  <li>$\Sigma v = \lambda v$
    <ul>
      <li>$\Sigma $ : Square Matrix</li>
      <li>$v $ : Eigenvector or characteristic vector (d)</li>
      <li>
        <p>$\lambda$ : Eigenvector or characteristic value (d*d)</p>

        <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/10.png" alt="10" /></p>
      </li>
    </ul>
  </li>
  <li>$\Sigma v = \lambda v$</li>
  <li>$(\Sigma -\lambda I) v=0$</li>
  <li>$\Sigma -\lambda I=0 \iff M = 0$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$</td>
          <td>\Sigma -\lambda I</td>
          <td>=0 \iff</td>
          <td>M</td>
          <td>= 0$ // Characteristic Equation</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>The zero vector cannot be an eigenvector</li>
  <li>The value zero can be an eigenvalue
    <ul>
      <li>Eigen solver - library 별로 Serving algorithm이 다름</li>
    </ul>
  </li>
  <li>다른 좌표계의 값으로 해석
    <ul>
      <li>차원 축소로 인한 info loss를 최소화시킴</li>
      <li>원래 좌표계에서 차원 축소하면 info loss가 너무 크기 때문에 변환하여</li>
    </ul>
  </li>
  <li>
    <p>From d original variables: $x_1, x_2, …, x_d$</p>

    <p>x 라는 좌표축을 y 좌표축으로 변환 (y-axis: uncorrelated)</p>

    <ul>
      <li>Produce k new variables: $y_1, y_2, …, y_k$ - Eigenvector의 개수를 k«d개로 표현할 수 있음.</li>
      <li>y좌표계에는 data diension간 correlation이 최소화</li>
      <li>다른 좌표축으로 새롭게 변환하며 dimension reduction
        <ul>
          <li>표현 방법은 다르지만 data 자체는 동일하게 d variables : y1~ y1d</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/11.png" alt="11" /></p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/12.png" alt="12" /></p>
  </li>
  <li>such that:
    <ul>
      <li>
        <p>$y_k$’s are uncorrelated (orthogonal)</p>

        <p>새로운 좌표축 yk는 서로 수직인 관계 (uncorrelated, orthogonal)</p>
      </li>
      <li>$y_1$ explains as much as possible of original variance in data set
        <ul>
          <li>data의 가장 큰 Variance 방향으로</li>
        </ul>
      </li>
      <li>$y_2$ explains as much as possible of remaining variance</li>
      <li>etc.</li>
    </ul>
  </li>
  <li>eigenvector / eigenvalue 대응 → characteristic equation으로 solve 가능
    <ul>
      <li>solver에 따라 solution이 다르게 나타나는데 eigenvector value가 잘 적합하게 구해졌는지 확인 (unstable하게 구해졌을 수가 있으므로)</li>
    </ul>
  </li>
  <li>$v_1 = \left[ \begin{matrix}v_{11} &amp; v_{12} &amp; …&amp; v_{1d} \end{matrix} \right]^T$ is the <u>1st Eigenvector</u> of covariance matrix, and coefficients of $y_1$ ( $v_1$ is the first principal component)</li>
  <li>$v_2 = \left[ \begin{matrix}v_{21} &amp; v_{22} &amp; …&amp; v_{2d} \end{matrix} \right]^T$is the <u>2nd Eigenvector</u> of covariance matrix, and coefficients of $y_2$ ( $v_2$ is the 2nd principal component)</li>
</ul>

<p>…</p>

<ul>
  <li>$v_k = \left[ \begin{matrix}v_{k1} &amp; v_{k2} &amp; …&amp; v_{kd} \end{matrix} \right]^T$is the kth Eigenvector of covariance matrix, and coefficients of $y_k$ ( $v_k$ is the kth principal component)</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/13.png" alt="13" /></p>

<ul>
  <li>eigenvector, eigenvalue가 나오게 되고 
2개의 eigenvalue의 크기 순서대로 eigenvector을 내림차순으로 정렬
    <ul>
      <li>→ 가장 크게 var이 되는 방향으로 정렬하게 됨</li>
    </ul>
  </li>
  <li>새로운 좌표계를 보면 제일 큰 분산 방향으로 분산을 표현해 낸 좌표축 v1, v2
    <ul>
      <li>첫 번째 eigenvalue에 해당하는 좌표축을 써서 data 표현해면 → info loss 최소화</li>
      <li>
        <p>더 분별력 잇는 방향으로 표현되나 data 자체는 동일하다.</p>

        <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/14.png" alt="14" /></p>
      </li>
    </ul>
  </li>
  <li>더 분별력 잇는 방향으로 표현되나 data 자체는 동일 (x1, x2) → (y1, y2)
    <ul>
      <li>그 변환 관계는 x좌표와 y좌표의 연관</li>
      <li>
        <p>d차원을 k차원으로 줄어들게 됨 : 가장 큰 eigenvector로 해서 좌표축을 변환해도 잘 date들이 표현이 됨을 확인할 수 있다. (최대 분산 방향이므로)</p>

        <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/15.png" alt="15" /></p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="pca-example">PCA Example</h2>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/16.png" alt="16" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/17.png" alt="17" /></p>

<ul>
  <li>Calculate the covariance matrix
    <ul>
      <li>$cov = \left[ \begin{matrix}.616555556 &amp; .615444444 \ .615444444 &amp;.716555556\end{matrix} \right]$
        <ul>
          <li>covariance : $\sigma _{12} = \sigma _{21}$이 같은 값을 가지는 symmetric - &gt; correlation이 positive :</li>
        </ul>
      </li>
      <li>since the non-diagonal elements in this covariance matrix are positive, we should expect that both the x and y variable increase together.
        <ol>
          <li>Zero mean data를 이용해 $XX^T$</li>
          <li>각각의 data에서 mu를 빼 주어 n으로 나누자:  $\frac 1 n (x-\mu)(x-\mu)^T$</li>
        </ol>
        <ul>
          <li>$\frac 1 n$으로 나누어주면 covariance</li>
          <li>안 나누어주면 scatter matrix
            <ul>
              <li>: 나누어주냐 나누어주지 않느냐는 상수를 곱해주는 것이기 때문에 cov로 하던 scatter로 하던 방향과 크기를 이야기하는 것이기에 같게 나옴.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Calculate the eigenvectors and eigenvalues of the covariance matrix</p>

    <p>characteristic equation을 구해 lambda를 구해 eigenvalue, vector</p>
  </li>
  <li>eigenvalues$= \left[ \begin{matrix}.0490833989 \ 1.28402771  \end{matrix} \right]$</li>
  <li>eigenvectors$= \left[ \begin{matrix}-.735178656 &amp;-.677873399 \ .677873399 &amp;-.735178656 \end{matrix} \right]$
    <ul>
      <li>크기 순서대로 정렬하게 되면 v2 / v1</li>
      <li>(-) (-) 북동/남서방향 : 서로 수직인 관계로 얻어짐</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/18.png" alt="18" /></p>

<ul>
  <li>eigenvectors are plotted as diagonal dotted lines on the plot.</li>
  <li>Note they are perpendicular to each other.</li>
  <li>Note one of the eigenvectors goes through the middle of the points, like drawing a line of best fit.</li>
  <li>The second eigenvector gives us the other, less important, pattern in the data, that all the points follow the main line, but are off to the side of the main line by some amount.</li>
  <li>
    <p>Reduce dimensionality and form feature vector</p>

    <p>차원 축소된 data : feature</p>

    <ul>
      <li>the eigenvector with the highest eigenvalue is the principle component of the data set.
        <ul>
          <li>원래 data dimension을 drop한 형태가 아닌 원래 거를 잘 조합해서 만든 feature vector</li>
          <li>eigenvalue를 크기 순대로 대응하여 첫 번째 component로 간주한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>In our example, the eigenvector with the largest eigenvalue was the one that goes through the middle of the data.</li>
  <li>Once eigenvectors are found from the covariance matrix, the next step is to order them by eigenvalue, highest to lowest. This gives you the components in order of significance.</li>
  <li>Eigen Feature Vector
    <ul>
      <li>
        <p>Feature Vector = (eig1 eig2 eig3 … eign)</p>

        <p>모든 eigenvector를 사용하면 원래 data 사용</p>

        <ul>
          <li>dim reduction은 없으나 correlation 최소되도록 좌표값 생성</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>We can either form a feature vector with both of the eigenvectors:
    <ul>
      <li>eigenvectors$= \left[ \begin{matrix}-.677873399 &amp; -.735178656 \ -.735178656 &amp;.677873399  \end{matrix} \right]$</li>
    </ul>
  </li>
  <li>or, we can choose to leave out the smaller, less significant component and only have a single column:
    <ul>
      <li>less significant 한 eigenvector순으로 drop시키면 차원의 수 = data info loss 최소화 하는 방향으로 drop</li>
      <li>eigenvalues$= \left[ \begin{matrix}.677873399 \ - .735178656 \end{matrix} \right]$</li>
    </ul>
  </li>
  <li>Back to our example: Transform data to eigen-space $(x’, y’)$</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/19.png" alt="19" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/20.png" alt="20" /></p>

<ul>
  <li>좌표축의 반시계방향 회전 (data의 시계방향 회전) 이라고도 볼 수 있음</li>
  <li>data를 eigenvector에 투영한다는 것은 :
    <ul>
      <li>Eigenvector를 원래 data point에 곱하는 것은
  새로운 좌표값에서의 값이 나오게 됨.</li>
    </ul>
  </li>
</ul>

<h1 id="eigenface">Eigenface</h1>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/21.png" alt="21" /></p>

<ul>
  <li>data 분석할 때 : 특히 video 처리할 때 image 100*100만 되도 dimension 10000
    <ul>
      <li>→ 처리:</li>
      <li>image column방향으로 이어붙여서 column vector륾 만들어 처리했는데 너무 dim이 컸음
  PCA를 통해 dimension reduction을 하려고 했음</li>
    </ul>
  </li>
  <li>When viewed as vectors of pixel values, face images are extremely high-dimensional
    <ul>
      <li>d = 100´100 image → 10,000 dimensions</li>
      <li>Slow and lots of storage</li>
    </ul>
  </li>
  <li>However, very few of 10,000 dimensional vectors are valid face images</li>
  <li>We want to effectively model the subspace of face images</li>
  <li>$X_{new} = \Sigma_i w_i x_i$</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/22.png" alt="22" /></p>

<ul>
  <li>Eigenspace 얼굴 이미지를 모아서 평균 face - PCA할 때 mean을 빼 주어 평균 얼굴을 빼 주어 수행</li>
  <li>10000차원 data를 이용해 covariance mat를 구하고 이로부터 eigenvector/value를 구할 수 있음
    <ul>
      <li>그 eigenvector는 10000차원임 (d = 10000) / data 개수 n = 100
        <ul>
          <li>
            <blockquote>
              <p>covariance matrix의 차원은 : 10000 * 10000</p>
            </blockquote>
          </li>
          <li>d*d로 pca하면 eigenvector d개 → 10000개 나오게 됨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>eigenvector : 10,000차원의 column vector를 100*100 이미지로 만들게 되면 eigenvalue모양이 얼굴 모양을 나타내게 됨.</li>
  <li>eigenvalue와 data를 곱하게 되면 이를 feature vector로 다섯 개로 줄어듦</li>
  <li>이런 식으로 feature vector로 나오게 된 것을 DT, bayesian에 넣는 것보다도 data 전체를 다 넣어서 NN에 넣는게 더 효율적이더라 :더 좋은 feature를 뽑아 classification도 잘 해줌</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/23.png" alt="23" /></p>

<ul>
  <li>$X_{\mu} = VY$</li>
  <li>$X = \mu + VY$</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/24.png" alt="24" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/25.png" alt="25" /></p>

<ul>
  <li>(1) 원래 data를 이용하는 것 vs (2) eigenvector를 이용하는 것 중 어느 것이 더 효율적일까? (효과적일까)</li>
</ul>

<p>→ eigenvector가 더 효율적이다.</p>

<p>이유)</p>

<ul>
  <li>original data X에는 많은 duplicate를 포함되어 있다.</li>
</ul>

<p>그러나</p>

<ul>
  <li>Eigenvector에는 그 duplicate가 제거된 correlation이 배제됨</li>
  <li>심지어 원래는 i 100개를 다 써야 하는데 eigenvector를 쓰면 10000 iteration까지도 갈 수 있음/ data correlation 제거했기 때문에 고유성을 가진 vector들은 적은 data를 사용해도 전체 데이터를 표현할 수 있다</li>
</ul>

<p>→ original data 100개 돌려서 쓰는 것보다 10개 해서 쓰는게 더 효과적인 결과가 나올 수 있다</p>

<h1 id="sift-feature-visualization">SIFT feature visualization</h1>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/26.png" alt="26" /></p>

<ul>
  <li>The top three principal components of SIFT descriptors from a set of images are computed</li>
  <li>data image pixel을 조합하여 image표현하는 방법</li>
  <li>등장 배경
    <ul>
      <li>고차원 data feature pixel(4k 해상도 등)을 개별적으로 처리하기에는 어려움이 큼</li>
      <li>→ data를 10*10으로 쪼개서 unit pattern의 조합 block으로 SIFT Descriptor를 뽑아 PCA를 수행시킴 → correlation이 배제된 상위 몇 개의 eigenvector 구해짐</li>
    </ul>
  </li>
  <li>상위 n개의 eigenvector를 가지고 원본 image로 mapping하기 위한 alpha값을 구할 수 있을 것.
    <ul>
      <li>Map these principal components to the RGB space
        <ul>
          <li>$\Sigma_i \alpha_i v_i$</li>
          <li>alpha value는 3개로 나타남 : 어떤 image의 patch이건 RGB</li>
        </ul>
      </li>
      <li>
        <blockquote>
          <p>image에 비슷한 경향을 나타나면 비스무리한 색으로 나타나게 된다.</p>
        </blockquote>
        <ul>
          <li>pixels with similar colors share similar structures 32</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>→ data에 어떤 내용이 포함 되어 있을지에 대한 연구를 수행할 수 있다.</p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/27.png" alt="27" /></p>

<ul>
  <li>임의의 차원의 data block을 원하는 차원으로 줄임
    <ul>
      <li>임의의 block에 PCA 분석을 수행시키고 eigenvector에 원하는 개수 만큼 수행하면 그 alpha값을 활용하여 dim reduction : unit patch에 대한 pca 분석</li>
    </ul>
  </li>
  <li>Divide the original 372x492 image into patches:</li>
  <li>Each patch is an instance that contains 12x12 pixels on a grid</li>
  <li>View each as a 144-D vector 33</li>
  <li>
    <p>PCA compression: 144-D → 60 D</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/28.png" alt="28" /></p>

    <ul>
      <li>144개 eigenvector 모두 : 원본 image와 동일한 quality
  60개 eigenvector : data 해상도가 떨어지지만 dim은 절반 정도로 줄어듬</li>
    </ul>
  </li>
  <li>
    <p>16 most important eigenvectors</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/29.png" alt="29" /></p>
  </li>
  <li>
    <p>6 most important eigenvectors</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/30.png" alt="30" /></p>
  </li>
  <li>
    <p>PCA compression: 144-D → 3-D</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/31.png" alt="31" /></p>
  </li>
  <li>
    <p>60 most important eigenvectors</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/32.png" alt="32" /></p>
  </li>
  <li>block의 대소에 따른 compression 정도의 차 :
    <ul>
      <li>compression 정도 - image quality 사이의 tradeoff 존재</li>
    </ul>
  </li>
  <li>
    <p>일정 크기의 patch에 대해 pca를 보고 상위 n개의 eigenvector</p>

    <p>→ 어떤 image건 12*12 pixel은 60개의 숫자로 표현되는 60dim</p>

    <p>Compression 많이 되면 잘 분간할 수 없을 정도가 될 수 있으므로 적절한 선에서 (1/10) 마무리하면 괜찮음</p>
  </li>
</ul>

<h1 id="pca-vs-lda">PCA vs. LDA</h1>

<ul>
  <li>
    <p>Linear Discriminant Analysis(LDA) considers class information</p>

    <p>LDA</p>

    <ul>
      <li>data label 이용:</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/33.png" alt="33" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/34.png" alt="34" /></p>

<h2 id="pca-vs-lda-face-recognition-accuracy">PCA vs. LDA Face recognition accuracy</h2>

<ul>
  <li>
    <p>Database 상 Face 중에서 어느 것과 가장 같은지 비교</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/35.png" alt="35" /></p>

    <ul>
      <li>eigenvector 너무 적게하면 정보 손실된 것이니까 잘 비교 안됨</li>
      <li>eigenvector(feature) 늘여보면 성능 올라감 / 떨어짐</li>
    </ul>
  </li>
  <li>PCA (eigenfaces): 80.0%</li>
  <li>LDA (fisherfaces): 93.2%</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/36.png" alt="36" /></p>

<ul>
  <li>FERET database</li>
  <li>PCA : dim reduction
    <ul>
      <li>얼굴 data 이용하여 : row 방향이 아닌 column vector 모양</li>
      <li>data에 대한 pca 분석 수행하면 eigenvector가 나오는데</li>
    </ul>
  </li>
  <li>y = V^T X로 y가 나옴
    <ul>
      <li>V eigenvector 10000개 중 k=100~200로 쓰면 100~200 dim의 data로 수행</li>
    </ul>
  </li>
  <li>사람의 수 = class 개수 → 수천만 개가 되면 class별 학습이 어려움
    <ul>
      <li>보통 얼굴인식할 때는 사람 수가 되게 많다 (수십 ~ 수백이나 폰에는 보통 한 명)</li>
      <li>출입 시스템에 등록된 사람 : 수십~수백</li>
      <li>CCTV block system : 수십 명</li>
      <li>운전면허, 주민등록 얼굴 : 수천만 명</li>
      <li>얼굴 인식할 때 등록되는 사람들 중 얼굴</li>
    </ul>
  </li>
</ul>

<h2 id="identification">Identification</h2>

<ul>
  <li>one-to-n matching</li>
  <li>k nearest neighbor를 사용하여 l2 distance가 가장 가까운 것을 새로운 얼굴에 대응되는 id라고 간주함</li>
</ul>

<h2 id="verification">Verification</h2>

<ul>
  <li>one-to-one matching</li>
  <li>사용자가 id를 이야기하고 맞는지 판단하는 경우-해당 id가 맞느냐 틀리느냐</li>
  <li>Attribute가 많아지는데 성능 떨어지면 curse of dimensionality / overfitting</li>
</ul>

  </div><a class="u-url" href="/13.-Dimensionality-Reduction-(1)" hidden></a>
</article>


      </div>
    </main><footer>
  <hr style="margin-left: 15px; color: #000 !important; background-color: #000 !important; opacity: 1 !important; width: 40px; height: .5px !important; margin-bottom: 2rem !important;">
  <div class="row m-0">
    <div class="col-12 col-md-3 col-sm-4 mb-4 mb-sm-0">
      <p>
        <a href="https://github.com/underthelights/underthelights.github.io" style="font-weight: 300">Kyuhwan Shim</a>
        <br>Up-to-date as of <span id="date">loading...</span></p>
      <i class="fas fa-location-dot mr-1"></i> Seoul is <img style="width: auto; height: 23px;" id="weather_icon">
      <span id="weather">Loading...</span>
    </div>
    <div class="col-12 col-md-9 col-sm-8">
      <p>
        <a style="font-weight: 300" href="https://linkedin.com/in/kyuhwan-shim"><i class="fab fa-linkedin" style="margin-right: 10.5px"></i>LinkedIn</a><br>
        <a style="font-weight: 300" href="https://fb.com/s.kyuhwn"><i class="fab fa-facebook" style="margin-right: 8px"></i>Facebook</a><br>
        <a style="font-weight: 300" href="https://www.instagram.com/s.kyuhwn"><i class="fab fa-instagram" style="margin-right: 10.5px"></i>Instagram</a><br>
        <a style="font-weight: 300" href="https://github.com/underthelights"><i class="fab fa-github" style="margin-right: 10.5px"></i>GitHub</a><br>
      </p>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.18.1/moment.min.js"></script>

<script src="/assets/js/weather.js"></script>
<script src="/assets/js/github_date.js"></script>
</body>

</html>

---
layout: post
date: 2022-06-12
title: "14. Reinforcement Learning (1)"
tags: [ML, ]
categories: [Notes, ]
use_math: true
---


# Characteristics of Reinforcement Learning 


![0](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/0.png)

- What makes reinforcement learning different from other machine learning paradigms?
	- supervised l. vs unsupervised l. vs. RL
		- supervised : label + data
		- Unsupervised : just use given data
		- RL : data + reward - Reward에 해당하는 추가적인 input이 존재함

	→ There is no supervisor, only a reward signal

- Feedback is delayed, not instantaneous
- Time really matters (sequential, non i.i.d. data)
	- 시간이 중요한 요소 중 하나
	- sequential : 전반의 선택이 후반의 선택에 영향
	iid = independent identically distributed - 상호 연관
- Agent’s actions affect the subsequent data it receives
	- agent action이 이후 data에 영향을 미친다.

## Examples of Reinforcement Learning

- Fly stunt manoeuvres in a helicopter
	- 헬리콥터의 비행 모형
- Defeat the world champion at Backgammon
	- backgammon 게임에서의 응용
- Manageaninvestmentportfolio
- Controlapowerstation
- Makeahumanoidrobotwalk
- Play many different Atari games better than humans
	- 로봇, 투자 포트폴리오, 아타리 게임에서의 학습

	![1](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/1.png)


	![2](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/2.png)


# Rewards

- A reward 𝒕 is a scalar feedback signal
- Indicate show well agent is doing at step t & The agent’s job is to maximize cumulative reward
- 각각의 시간에 얼마나 잘 행동 했는지 보고 reward 최대화되는 방향으로 행동하도록 학습
- Reinforcementlearning is based on the reward hypothesis
	- reward = 사람이 만든 기준
	ex. Atari game : target 별 최대한의 점수를 학습할 수 있도록 학습이 되기도 함. 점수가 많은 쪽을 더 빨리 얻을 수 있도록 학습시키는 양상이 생길 수 있다,
	- Reward hypothesis: all goals can be described by the m**aximization of expected cumulative reward**

## Examples of Rewards

- Fly stunt manoeuvres in a helicopter

	(+) : 원하는 궤적을 그리며 날아갈 때
	(-) : crashing 시 마이너스 ㅎ과

	- +ve reward for following desired trajectory
	- −ve reward for crashing
- Defeat the world champion at Backgammon
	- +/−ve reward for winning/losing a game
- Manage an investment portfolio

	(+) : 원하는 이익
	(-) : 손실

	- +ve reward for each $ in bank
- Control a power station

	(+) : 적절한 전력 공급
	(-) :

	- +ve reward for producing power
	- −ve reward for exceeding safety thresholds
- Make a humanoid robot walk

	(+) : 주어진 환경에서 target 물질을 확보에서 mission 잘 수행
	(-) : 넘어짐

	- +ve reward for forward motion
	- −ve reward for falling over
- Play many different Atari games better than humans

	(+) : 점수 얻거나
	(-) : 점수 잃거나
	-> 빠른 시간 안에 점수를 많이 얻는 방향으로

	- +/−ve reward for increasing/decreasing score

# Sequential Decision Making

- 현재의 action이 다음 턴 action에 영향을 미치는데, 오랜 turn에 대해 영향을 끼칠수도 있음.
- Goal: select actions to maximize total future reward
	- 일련의 행동에 따른 reward가 최대가 되도록 학습한다
- Actions may have long term consequences
	- state가 있고 action을 취해서 s1-(a1)->s2-(a2)->s3
- Reward may be delayed reward는 delay를 수반하여 주어질 수 있다
- 현재 action으로 인한 reward에 더 중점을 둘 것인지, 미래의 reward에 중점을 더 둘 것인지 : user setting할 수도 있고 학습 단계에서 어떻게 parameter를 설정했는지에 따라 / 학습이 잘 효과적으로 이루어질수 있는지를 고려하여 모수 조정
	- (greedy) 현재 reward에 초점을 맞추는 경우 - current reward
	- (optimal) 전체 reward에 초점을 맞추는 경우 - total reward
		- Itmay be better to sacrifice immediate reward to gain more long-term reward (greedy optimal)
- Examples:

	Ex.
	-투자 :당장은 손해가 나더라도 미래 시점에 수익
	-헬리콥터 주행 중 연료 주입 : crash하면 negative penalty하기에 현재로서는 reward 줄지만 optimal하게는 늘어나는 reward
	-체스에서 상대방 이동 : 본인 점수 취하는 것보다 상대방 방해가 전체적으로 더 이득일수도 있는 경우

	- A financial investment (may take months to be mature)
	- Refueling a helicopter (might prevent a crash in several hours)
	- Blocking opponent moves (might help winning chances many moves from now)

# Agent and Environment


![3](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/3.png)

- At each stept the agent: agent가 주변을 관찰하고, reward를 받아 action을 취함
	- Executes action At
	- Receives observation Ot
	- Receives scalar reward Rt

![4](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/4.png)

- The environment:
	- Receives action At
	- Emits observation Ot+1
	- Emits scalar reward Rt+1
- t increments at env. step

<agent, environment의 상호작용>
agent는 action을 취하고 state에 따라 Reward를 받게 됨
env는 action을 받아들여서 agent에게 주고 변환된 statement를 agent에게 줌

- t타임으로 이루어지는 요소들

action에 대해서 reward와 statement의 변화


![5](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/5.png)


# Major Components of an RL Agent 

- An RL agent may include one or more of these components:
	- Policy: agent’s behavior function 행동 정의
	- Value function: how good is each state and/or action 얼마나 좋은가
	- Model: agent’s representation of the environment  학습 모델

## Example - Maze 


![6](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/6.png)

- Agent: explores environment and gets reward
- Environment: agent 돌아다니는 환경 situation being explored by the agent
- States: 위치 - positions/locations in the environment
- Actions: 상하좌우 - allowed movements for the agent
- Reward: what agent gets as it moves

![7](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/7.png)

- For example, bomb has reward -10, germ has reward 10, every other move has rewards -1

	→ 불필요한 이동을 최소화시키기 위한 장치


s6 is blocked


![8](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/8.png)


![9](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/9.png)


# Bellman equation 


$$
V(s) = max_a(R(s,a) + \gamma V(s'))
$$

- $R(s,a) $: reward: state에서 취한 action에 따른 reward
- $V(s)$ : is the value function - value function:전체 reward 를 어떻게 표현할 것인가
- $\gamma$ : is the discounting factor
	- 현재-미래 reward중 어느 것에 초점을 맞출 것인지 중요도 맞추는 상수
- $s'$ : is the next state agent can go from
	- s : 현재 state, s’ : next state
- Bellman equation is used to calculate the value function 
→ 각 state에 대한 value function값으로 주어지게 됨 : 환경이 바뀌면 결과가 바뀌게 됨
R(s,a) : current reward
V(s’) : all futer reward
- 일반적인 규칙:폭탄,보석이 있고 env가 달려젔을 때 학습을 더 잘 할것인가->bellman eq로 value fn으로 하는거는 환경 바뀌면 다시 적용해야 함

![10](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/10.png)

- T calculate V(1) ,consider a path s1- s2-s3-s7-s11-s12

s1에 대해 가장 큰 state function의 결과를 만드는 값을 취하도록 했다.

- (assume $\gamma = 1$)
	- V(1) = R(s1, →) + V(2) = -1+V(2)
	- V(2) = R(s2, →) + V(3) = -1+V(3)
	- V(3) = R(s3, $\downarrow$) + V(7) = -1+V(7)
	- V(7) = R(s7, $\downarrow$) + V(11) = -1+V(11)
	- V(11) = R(s11, →) + V(12) = -1+V(12)
- Since V(12) = 10
	- We can get V(11)=9, V(7)=8, V(3)=7, V(2) = 6, V(1) = 5
- We can consider other path, s1-s2-s3-s4- s3-s7-s11-s12 to calculate V(1), in which case V(1) will be less than 5 14

![11](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/11.png)


$$
V(s) = max_a(R(s,a) + \gamma V(s'))
$$

- By calculating V(s) for all states
	- Agent can move to the state with larger state value
- 임의의 출발점에서 state function 커지는 쪽으로 action을 취하면 된다
→ equation을 이용해서 value funcition을 구한후 최적의 path를 구할 수 있다

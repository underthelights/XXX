---
layout: post
date: 2022-05-30
title: "11. "
tags: [ML, ]
categories: [Notes, ]
use_math: true
---


# Ensemble Learning


지금까지 배운 여러개의 classifier 각각 : parameter 변화하여 여러가지 model


한 가지 model보다는 여러 model을 활용하여 prediction


## Introduction


model 여러개를 써서 여러개의 model을 조합하여 최종 model을 결정

- Ensemble learning is a process that uses a <u>set of models</u>, each of them obtained by applying a learning process to a given problem. This set of models (ensemble) is integrated in some way to obtain the <u>final prediction</u>
- Aggregation of multiple learned models with the goal of <u>improving</u> accuracy
	- 목표 : 정확도 높이기 / classification, regression acc, clustering acc에서 좋은 성능 얻기
	- Intuition: simulate what we do when we combine an <u>expert panel</u> in a human decision-making process
		- 사람들이 결정 내릴 때도 한 두명 생각보다는 전문 집단 panel에 의해서 결정 내리면 좀 더 합리적이고 좀 더 좋은 결정을 내릴거라 생각하는 것처럼 결정

## Types of ensembles

- fusion
	- 다른 version의 dataset, algorithm을 갖고 종합해 최종 판단
	- 두 개의 용어는 바슷한 개념으로 사용되고도 있고 엄밀하게 분류하기 애매함
- ensemble
	- randomness : 한 쪽은 NN, SVM <- 2 model, no randomness
	- NN 사용 시 w randomly init : w 초기값이 다른 model을 여러 가지 만들면 randomness로 model 만든 ensemble

→ randomness 추가된 부분 : ensemble / Randomness excluded : Fusion (섞어서 사용하기도 함)


---

- Ensemble methods are used for:
	- Classification
		- 각 model들이 분류 결과를 만들어나면 이를 조합하여 최종 (average)
		- classification alg 바꾸기 or dataset 변화를 주어 다른 모델 만들 수 있음
	- Regression
	- Clustering (also known as consensus clustering)
		- clustering : 여러 version의 clustering에서 average를 취하여 어떻게 avg할 것인가가 차이
			- group의 category로 분류 : label이 학습할 때 주어진게 아님
			→ 1번 그룹 안 어떤 sample들이 동일한 그룹 안에 있다는 게 의미
			- 1번, 2번 group은 다른 group임이 의미
			-> clustering의 결과가 나타날 때 어떻게 합칠 것인가
- Ensembles can also be classifed as :
	- Homogeneous: It uses only one induction algorithm
		- 동일한 분류 algorithm의 경우에도 (induction algorithm = classifier)
			- Bayesian classifier에서도 pdf로 gaussian 사용 -> classifier의 구조를 바꾸면 다른 구조의 algorithm
			- 아예 다르게 바꾸게 되면 perceptron / svm/ decision tree 등 DNN/ CNN 자체도 구조는 차이가 있음
	- Heterogeneous: It uses different induction algorithms
		- algorithm의 구조에 차이가 있으면 heterogeneous

## Some Comments

- Combining models adds complexity
	- It is, in general, more difficult to characterize and explain predictions

		Model의 개수가 늘어나니까 전체 complexity 증가 → 설명하기도 어려워지고, 어떤 결과값이 나올지, 그 결과값 설명하기도 어려워짐

	- The accuracy may increase

		하지만 정확도는 일반적으로 향상됨

- Violation of Ockham’s Razor: **“Simplicity leads to better accuracy”**
	- simple decision boundary가 일반적으로 좋은 성능을 보인다
	- Identifying the best model requires identifying the proper “model complexity”
		- 좋은 성능을 내는 model을 찾으려면 complexity를 고민해야 하는데
		- occam에 거스르긴 하지만, 여러 model 사용하게 되면 개별 model은 복잡하지만 종합한 model은 smooth한 모양일수도 있음

		→ simple하게 만드는 과정이라 볼 수 있고, occam’s razor에 적합

			- simple boundary = simple alg이라고 생각했는데, complexity를 올림으로 db가 simple해질수도 있다!
	- Decision boundary may become simpler, eventually. E

## The Ensemble Learning Process


data가 주어지면 그에 대해 함수들을 생성 - 각각이 분류 함수들 k개의 model 생성→ pruning


→ 최종 함수 fk 도출


![0](/assets/img/2022-05-30-11.-.md/0.png)


# Methods to Generate Ensembles


![1](/assets/img/2022-05-30-11.-.md/1.png)


## Data manipulation


Data, algorithm에 대한 구분

- data : data의 src로부터 subset 추출

Data src / sensor (visible, thermal, near infrared)


빛의 파장이 가시광선, 적외선 등에 따라 영상이 다양히 나타나는데 개별적인 src로 판단할 수 있고 이를 조합하는 것 또한 ensemble이라 볼 수 있다

- 그런 식의 ensemble을 Fusion이라고 볼 수도 있다.
- Manipulating the input features

	![2](/assets/img/2022-05-30-11.-.md/2.png)


	movie data features : rating, actor, genre
	-> 흥행할것인지, 수익이 어느정도일 것인지, 그룹의 사람들이 좋아할지

	- 세 feature를 다르게 조합하여 각각의 경우가 모두 다른 classifier model
- Sub-sampling from the training set

	![3](/assets/img/2022-05-30-11.-.md/3.png)


	data의 분할 : subset들 조합하여 최종 결과


## Modeling process manipulation

- Manipulating the parameter sets
	- hyperparameter : network layer, 초기값을 어떻게 설정할지,
	node 개수, activation fn은 어떻게 설정할지

	![4](/assets/img/2022-05-30-11.-.md/4.png)

- Manipulating the induction algorithm

	![5](/assets/img/2022-05-30-11.-.md/5.png)


## How to Combine Models

- Algebraic methods : Score를 어떻게 처리할 것인가
	- Average
	- Weighted average
	- Sum
	- Weighted sum
	- Product
	- Maximum
	- Minimum
	- Median
- Voting methods
	- Majority voting
	- Weighted majority voting
	- Borda count

![6](/assets/img/2022-05-30-11.-.md/6.png)


![7](/assets/img/2022-05-30-11.-.md/7.png)


# Characteristics of the Base Models 

- The Base classifiers should be as accurate as possible and having diverse errors, while each classifier provides some positive evidences
	- diverse한 error가 나타나야 함. (Alg1, alg2 … 결과가 diverse)
	- 어느 정도의 정확도를 가지면서 정답의 다양성을 가져야 classify의 의미가 있다
	- 여러 version의 classifier를 ensemble하여 좋은 classifier
- The average error of the base learners should be as small as possible
- The variance (of the predicted values) of the base learners should be as small as possible
	- Variance : when alg1 is trained

	Random한 initial value에 의하여 train 여러번하는데


	Variance 정확도가 많이 변화한다면 좋은 classifier가 아닐 것


	diversity


	Stability : 한 알고리즘을 볼 때 다양하지 않은 결과


	bias : ansewr과의 차이


	Variance : 얼마나 정답이 다양하겠는가


	![8](/assets/img/2022-05-30-11.-.md/8.png)


## Popular Ensemble Methods


Bagging:

- Averaging the prediction over a collection of predictors generated from **bootstrap samples** (both classification and regression)

	bootstrap sample :trian data있으면 subset sampling

	- 각각 sampling으로부터 classifier 학습

		Random하게 sampling하며 다양한 model


Boosting:

- Weighted vote with a collection of classifiers that were trained sequentially from training sets given priority to instances **wrongly classified**

	Boosting : 여러 단계를 거쳐 classifier 학습

	- 이전 단계의 classifier의 오답에 초점을 맞춘다.

	오류가 나오는 data들을 모아 다음 stage에서 초점을 맞추어 학습하여 융합한다


RandomForest:

- Averaging the prediction over a collection of trees constructed using a **randomly selected subset of features**
	- tree를 randomly생성하여 randomly select해서 만든다.

Ensemble learning via negative correlation learning:

- Generating sequentially new predictors **negatively correlated** with the existing ones
	- 현재 classifier하고 negative corelation갖는 classifier를 학습하여 융합한다

Heterogeneousensembles:

- Combining a set of **heterogeneous predictors**
	- NN + SVM + DT 등 융합

# Bagging: Bootstrap AGGregatING


![9](/assets/img/2022-05-30-11.-.md/9.png)

- Analogy: Diagnosis based on multiple doctors’ majority vote

	여러 명의 의사들의 진단 결과를 융합하는 방법


	Ex. Max score, average score 등

	- 여러 model을 만들기 위해서 bootstrap sampling

![10](/assets/img/2022-05-30-11.-.md/10.png)


![11](/assets/img/2022-05-30-11.-.md/11.png)


# Boosting

- Analogy: Consult several doctors, when there are disagreements, we focus more attention on that case

	의사들의 의견이 갈릴 때 합치되지 않는 의견들에 대해 더 주의를 기울인다.


(data가 다시 sampling될 확률값 조정하는 방식 : adaboosting)

- M_i : weak classifier

## Adaboost 


![12](/assets/img/2022-05-30-11.-.md/12.png)

- Boosting algorithm can be extended for numeric prediction
	- Comparing with bagging: Boosting tends to achieve greater accuracy, but it also risks overfitting the model to misclassified data
- The diagram should be interpreted with the understanding that the algorithm is sequential: classifier $C_k$ is created before classifier $C_{k+1}$, which in turn requires that $\beta_k$ and the current distribution $D_k$ be available

	![13](/assets/img/2022-05-30-11.-.md/13.png)

	- sequential = 앞서 misclassified sample을 구해야 이를 바탕으로 추가 학습을 진행하기에
	ck는 ck+1보다 학습이 사전에 이루어져야 하며
	Beta k : 4th classifer의 error / 현재 data distribution을 알아야 다음 단계 classifer를 학습시킬 수 있음

### Comments


이런식으로 sample distribution에 의해 data update하여 학습하면
앞 쪽 단계 misclassified data들이 current stage에 학습

- hard sample에 편중하여 학습이 일어남 (undemocratic voting scheme)
Weighted majority voting : ensemble 대상 단위의 classifier들이 성능 상 큰 차이 존재가 있어 weight를 줄 수밖에 없고 성능 좋은 classifier에 대해 더 높은 weight를 주는 게 더 자연스러울 수 있고, 이런게 democratic하지는 않다.
- This distribution update ensures that instances misclassified bythe previous classifier are more likely to be included in the training data of the next classifier.
- Hence, consecutive classifiers’ training data are geared towards increasingly hard-to-classify instances.
- Unlike Bagging, AdaBoost uses a rather undemocratic voting scheme, called the weighted majority voting.
	- The idea is an intuitive one: those classifiers that have shown good performance during training are rewarded with higher voting weights than the others.

## 


# Random Forest 

- Two Methods to construct RandomForest:

	Random forest 만드는 두 가지 방법

	1. Random input selection : 각 node에서 attribute를 randomly selection하는 방법
		- Forest-RI (random input selection): Randomly select, at each node, F attributes as candidates for the split at the node. The CART methodology is used to grow the trees to maximum size
	2. Random linear combination: 기존 feature에 대한 linear combination 작업을 취하여 여러 개의 attribute에 linear combination을 바탕으로 tree 학습을 진행한다.
		- Forest-RC (random linear combinations): Creates new attributes (or features) that are a linear combination of the existing attributes (reduces the correlation between individual classifiers)
- adaboost와 유사한 특징을 가지지만 error / outlier에 더 robust한 특성을 보인다.
	- Comparable in accuracy to Adaboost, but more robust to errors and outliers
- 민감하게 반응하지는 않고 더 빠르게 실행 : decision tree 생성 과정은 학습 model 자체가 효율적으로 구성될 수 있기 때문에 (tree 구성)
	- Insensitive to the number of attributes selected for consideration at each split, and faster than bagging or boosting

# Model Selection

- Given a problem, which algorithms should we use?

# Statistical Validation

- Mixture of Experts
	- Combine votes or scores

	![14](/assets/img/2022-05-30-11.-.md/14.png)

- Stacking
	- Combiner f() is another learner (Wolpert, 1992)
	- adaboost도 최종 함수 : 개별 classifier에 근거하여 accuracy로부터 sequentially 생성

		Stacked generalization by [David H.Wolpert](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#!)[](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#aep-article-footnote-id1)


		[https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231)


	![15](/assets/img/2022-05-30-11.-.md/15.png)

- Cascading
	- Use next level of classifier if the previous decision is not confident enough

	![16](/assets/img/2022-05-30-11.-.md/16.png)


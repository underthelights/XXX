---
layout: post
date: 2023-05-26
title: "10. DNN Deep Neural Network 2 (1)"
tags: [ML, ]
categories: [Notes, ]
use_math: true
---

- 구조상 차이 : hidden layer 개수
fully connected layer -> conv layer
- Activation, normalization 하는 부분들
- weight update하는 부분들

여러 학습 알고리즘이 제안되며 더 좋은 성능을 가지게 됨


# Data Processing


input layer - hidden lyaer - output layer로 그려진 NN

- x, y, z (hidden lyaer 하나로만 두고 보면) y layer 의 첫 번째 node를 볼 때 각자 연결된다
- fc 구조가 기본적인 구조
- 앞 node의 weight combination + bias
- Consider what happens when the input to a neuron is always positive...

	![0](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/0.png)

	- i의 변화에 따라 x도 변화 → w가 모든 같은 부호
- w update slowly → learn slowly
	- 이상적인 w에 도달하지 않을 수 있다
	- 학습 시간에도 문제가 될 수 있다
- What can we say about the gradients on w?
- Always all positive or all negative!
- (this is also why you want zero-mean data!)
	- data normalization : $\mu = 0$
	- for input hidden layer

## Step 1: Preprocess the data


(Assume X [NxD] is data matrix, each example in a row)



![1](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/1.png)

- normalized data : $N(0, 1^2)$
	- learn faster

![2](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/2.png)


In practice for Images: Zero-center only


- e.g. consider CIFAR-10 example with [32,32,3] images
- Subtract the mean image (e.g.AlexNet)
	- (mean image = [32,32,3] array)
		- RGB 평균 구해서 average
- Subtract per-channel mean (e.g.VGGNet)
	- (mean along each channel = 3 numbers)
- Not common to normalize variance (for PCA or whitening)
	- w 효과적으로
	- PCA → data rotation, whitening → Cov to Identity Mat
	- if original data에 대한 학습 중 특정 w를 학습해야 한다면

# Weight Initialization


원래 w : randomly init.

- Q: what happens when W=0 init is used?

	![3](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/3.png)


f’(0) x_i → wi에 update됨


모든 w에 동일한 pattern으로 update


i가 달라짐에 다르게 but 같게 실행 : 지그재그로 수행되는 것보다 성능이 안좋을 수 있음

- First idea: Small random numbers
	- (gaussian with zero mean and 1e-2 standard deviation)

	```python
	W = 0.01 * np.random.randn(D,H)
	```

	- small w : hidden layer가 커질수록 → backprop되는 layer 적어짐
		- 뒤 layer로 갈수록 앞 layer update 작아짐 : 학습 잘 안됨
	- big w : vanishing GD : update 안될수록 f’ 작아짐
		- 
	- Works ~okay for small networks, but problems with deeper networks.
- Lets look at some activation statistics
	- 10-layer net with 500 neurons on each layer, using tanh
	- non-linearities, and initializing as described in previous slide.
- All activations become zero!
- Q: think about the backward pass. What do the gradients look like?
	- Hint: think about backward pass for a W*X gate.
- *1.0 instead of *0.01
- Almost all neurons completely saturated, either -1 and 1. Gradients will be all zero.

Deep Neural Networks
Weight Initialization



# Proper initialization is an active area of research

- Understanding the difficulty of training deep feedforward neural networks
by Glorot and Bengio, 2010
- Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe et al, 2013
- Random walk initialization for training very deep feedforward networks by Sussillo and Abbott, 2014
- Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification by He et al., 2015
- Data-dependent Initializations of Convolutional Neural Networks by Krähenbühl et al., 2015
- All you need is a good init, Mishkin and Matas, 2015 ...

# Batch Normalization


[Ioffe and Szegedy, 2015]


layer에 data를 넣을 때 전체 data를 randomly partition하여 mini batch 생성

- “you want unit gaussian activations? just make them so.”
- consider a batch of activations at some layer.

![4](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/4.png)

- Improves gradient flow through the network
- Allows higher learning rates
- Reduces the strong dependence on initialization
- Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe
- Note: at test time BatchNorm layer functions differently:
- The mean/std are not computed based on the **batch(test batch)**.
- Instead, a single fixed empirical mean of activations during training is used.
	- (e.g. can be estimated during training with running averages)

---


# —


# Model Ensembles

1. Train multiple independent models
2. At test time average their results (여러 model result average)
- Enjoy 2% extra performance
- single model : learning method 변경
	- ex. tanh → ReLU : 종합하여 result 추출

# How to improve single-model performance?


![5](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/5.png)


Regularization



# Regularization: Add term to loss


# Regularization: Dropout

- In each forward pass, randomly set some neurons to zero Probability of dropping is a hyperparameter; 0.5 is common

	![6](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/6.png)

- Srivastava et al, “Dropout: A simple way to prevent neural networks from overfitting”, JMLR 2014


Example forward pass with a 3-layer network using dropout


![7](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/7.png)


![8](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/8.png)

- How can this possibly be a good idea?
- Forces the network to have a redundant representation;
	- Prevents co-adaptation of features

	개별 node들이 학습에 적극적으로 사용되도록 강제화시키는 작업


![9](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/9.png)


### How can this possibly be a good idea?


randomly selection of node → network 구조 다양 - 종합적 사용

- Another interpretation:
	- Dropout is training a large **ensemble** of models (that share parameters).
	- Each binary mask is one model
	- An FC layer with 4096 units has 24096 ~ 101233 possible masks!
	- Only ~ 1082 atoms in the universe..

# Dropout: Test time

- Dropout makes our output random!

	![10](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/10.png)

- Want to “average out” the randomness at test-time
	- $y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)dz$ㅓㄹ
	- 여러 var mask  ㅇ - mu, sum
- But this integral seems hard ...
- Want to approximate the integral
	- $y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)dz$
- Consider a single neuron.

	![11](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/11.png)

- At test time we have:
	- $E[a] = w_1x + w_2y$
- During training we have:

	![12](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/12.png)

- **At test time, multiply by dropout probability**

![13](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/13.png)

- p to be dropped → *p : 값을 줄여나가는 식으로 진행
- At test time all neurons are active always
	- → We must scale the activations so that for each neuron:
	- output at test time = expected output at training time

## Dropout Summary

- drop in forward pass
	- train때 사용된 w값이 test 시점에도 반영
- scale at test time

	![14](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/14.png)


## More common: “Inverted dropout”

- train시 p로 나누어줌
- test time is unchanged!
	- test 단계에 적용 → test 에서는 X X
	- if v1 > p → test시점 (1-p)

	![15](/assets/img/2023-05-26-10.-DNN-Deep-Neural-Network-2-(1).md/15.png)


# Regularization: A common pattern


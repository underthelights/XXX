---
layout: post
date: 2022-09-08
title: "[ALG] 1.3. Order of Algorithms (1)"
tags: [Algorithm, ]
categories: [Notes, ]
---

- 제일 먼저 생각
	- Input Size
		- Problem을 풀고자 하는데 이를 sol하고자 하는 algorithm에서, 알고리즘에 들어오는 data의 크기는 어떻게 되는가.
			- 문제 상황에 따라 가로 n, 세로 m의 데이터가 들어오면 (n,m) n일수도 nm일수도 있다.
		- data size가 커짐에 따라 얼마나 시간이 걸리는가 분석 : 얼마난 크기의 데이터가 들어왔을 때, 얼마나 시간이 걸리는가.
		- 시간 분석의 기본 요소 : data의 수
	- Cost : $g(n)$
		- 문제가 있을 때 n에 대하여 어느 정도의 비용이 걸리는가.

# _O_ (Big $O$ Notation)


> 💡 for given two functions $f(n)$ and $g(n)$,  
>   
> $g(n) = O(f(n))\iff \exists c \in \mathbb{R}, N \in \mathbb{N}\quad g(n)\leq c\cdot f(n), \forall n \geq N$

- complexity를 따질 때, data size가 작을 때 보다는 커질 때 문제가 발생함을 확인하고 싶음.
	- → 모든 n일 필요는 없고, 그 N보다 큰 모든 input size에 대해서 이러한 조건을 만족하면.
- then we say that :$g(n)$ is big O of $f(n)$
- 예)
	- 코드의 비용을 분석해 봤더니
		- for loop 1, for loop n 자승만큼 돌게 된다.

		```c
		x = x + 1;
		for (i = 1; i <= n; i++)
			y = y + 2;
		for (i = n; i >=1; i--)
			for (j = n; j >= 1; j--)
				z = z + 1;
		```

- 비용 : $g(n) = c_0 + c_1 n + c_2 n^2$
- 예 : $g(n) = 5 + 6 + 7n^2 \leq 8n^2 \quad \forall n \geq 8$
	- $8 \cdot n^2 = c \cdot f(n), N = 8$
		- n이 커지면 g(n)이 압도적으로 다른 친구들을 누르게 된다.
		- 다시한번 이야기하지만, n이 커질 때, 내가 분석한 비용은 f(n)이라는 함수에 눌리게 되는 upper bound 개념
	- $g(n) = O(n^2)$
- $g(n) = O(n^{1000})$?
	- 정의에 의하면 맞음 : 그래프상 확인해보아도 맞음.
	- $f(n)\geq g(n) \cdot c$

## Notes for big O

- [Note 1] The big O puts an <u>**asymptotic**</u> <u>upper bound</u> on a function.
	- 복잡도를 따질 때 ‘몇 초'가 걸린다기 보다는 얼마나 효율적인가를 따지는 척도
		- PL, HW 상황에 대해서 implementation 관점에서 개인 차 발생
	- Asymptotic analysis (from Wikipedia)
		- asymptotic : 점근적인
		- data size가 커질 때, 이 알고리즘이 시간이나 필요로 하는 메모리 사이즈가 얼마나 나빠지는가? → 알고리즘이 요구하는 시간, 메모리 양 등이 얼마나 나쁜 형태로 변화하는지 ‘형태’

			> If $f(n) = n^2 + 3n$, then as n becomes very large, the term $3n$ becomes insignificant compared to $n^2$. The function f(n)f(n) is said to be "asymptotically equivalent to n^2n2, as $n→∞$". This is often written symbolically as $f(n) -> n^2$, which is read as "$f(n)$ is asymptotic to $n^2$".

	- 계산 비용이 $0.01n^2$ 과 $100n$ 알고리즘 중 어떤 것이 더 효율적인가?
		- 이론적인 관점에서 $100n = O(n), 0.01n^2 = O(n^2)$
		- input size $n=3$: $0.09 \quad 300$
		- input size $n=10^6$: $0.01 \cdot (10^6)^2 = 10^{10}$, $100 \cdot 10^6 = 10^8$

		→ 결국 $O(n^2)$

	- (Tight) upper bound
		- $37log n + 0.1n = O(n)$
			- n이 커지면 $\log n$은 상당히 작아짐
		- $n^2 + 10n = O(n^2)$
		- $4(\log n)^2 + n \log n + 100n = O(n \log n)$
			- $\log n$ vs $n$ → 당연하게 $\log n$
			- $n \log n > {\log n} ^2$
		- $n^2 + 10n = O(n^{200})$???
			- upper bound 맞아 틀린 말은 아니지만, 일반적으로 O Notation을 활용할 때에는 tight upper bound를 선택하여 표현한다.

		> 💡 Dominating Term   
		> - 지배하는 term을 찾아 Upper Bound를 찾는다.

			- 지배하는 term을 찾아 Upper Bound를 찾는다.
		- $\log _en $등 base는 왜 고려하지 않느냐 → 상수에 해당하므로 상관 없기 때문에.
			- $\log _2 n = \frac {\log_e n }{\log_e2}$

	### Growth Rates of Some Common Complexity Functions

	- 이론적으로는 $n^3$은 efficient하지만 현실적으로는 문제가 발생할 수 있을만한 복잡도
	- 감당할 수 없을 정도로 커지네 등 asymptotic 특성을 분석하기 위함

![0](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/0.png)

- [Note 2] Given a cost function g(n), how do you find the proper complexity function f(n) such that $g(n) = O(f(n))$?
	- Suppress lower-order terms and constant factors!
	- Example:
		- $10^3 + 10^3n + 10^-3 n^2 = O(n^2)$
			- then $lim_{n \to \infty} \frac{n^2}{n} = \infty$
		- $5n \log_3 n + 3(\log_2 n)^2 + n + 6n^2 = O(n^2)$
			- then $lim_{n \to \infty} \frac{n}{log_en} = lim _{n \to \infty} = \infty$
		- $3(log_2 n)^2+ 0.1n = O(?)$
			- Dominate Term이 무엇일까?
			- $\lim _{n \rightarrow \infty}{\frac {(\log n)^2}{n}}$ =$\infty, c, 0$
				- $\infty$ : $(\log n)^2$ , $0$ : $n$ , $c$ :
				- L’Hospital Theorem : $\lim _{n \rightarrow \infty}{\frac {f(n)}{g(n)}} = \lim _{n \rightarrow \infty}{\frac {f'(n)}{g'(n)}} $
					- $\lim _{n \rightarrow \infty}{\frac {(\log n)^2}{n}} = \lim = \lim _{n \rightarrow \infty} {\frac {2}{n}{}}0$
			- → Linear Time
		- $2^{n+5} = O(2^n)$ ??
		- $2^{5n} = O(2^n)$??

## Comparing Orders of Growth

- How do you compare orders of growth of two functions?
	- One possible way is to compute the limit of the ratio of two functions in question.
	- $x = lim_{n \to \infty } \frac{f_1(n)}{f_2(n)}$
		- if _x_=0, _f_1 has a smaller order of growth than _f_2
		- if $x=c$, $f_1$ has a same order of growth than $f_2$
		- if $x=\infty$, $f_1$ has a larger order of growth than $f_2$
	- Ex.1: $\log_2 n $ vs. $\sqrt{n}$
		- $lim_{n \to \infty} \frac{log_2 n}{\sqrt(n)} = lim_{n \to \infty} \frac{(log_2 n)'}{(\sqrt(n))'} = lim_{n \to \infty} \frac{(log_2 e)\frac{1}{n}}{\sqrt\frac{1}{2\sqrt(n)}} $
	- Ex.1.2.: $\log_2 n $ vs. $n^{0.0001}$
		- $lim_{n \to \infty} \frac{log_2 n}{\sqrt(n)} $
	- Ex.2: $n!$ vs $2^n$ - factorial vs. exponential
		- $lim_{n \to \infty} \frac{ n!}{2^n} = lim_{n \to \infty} \frac{\sqrt{2 \pi n} (\frac {n}{e})^n}{2^n}=lim_{n \to \infty }\sqrt{2 \pi n} \frac{({n})^n}{2^n e^n}$
		- stirling's formula : $n! \approx \sqrt{2 \pi n} (\frac {n}{e})^n$

# $Ω$ (Big Omega Notation)


→ Lower Bound

- for two given functions $f(n), g(n)$

> 💡 $g(n) = \Omega(f(n))g(n)=Ω(f(n)) ⟺ \exists c \in \mathbb{R}, ∃c∈R,$ and $N \in \mathbb{Z^+ \cup {0}}$, s.t. $g(n) \geq cf(n) \forall n \geq N$

- We say that g(n)_g_(_n_) is _ω_ of $f(n)$.
- The Ω puts an asymptotic lower bound on a function.
- Ex:
	- $37\log n+0.1n=\Omega(n)$
	- $n^2 + 10n = \Omega(n^2)$
	- $4(logn)^2 +nlogn+100n=\Omega(nlogn)$
	- $n^{200} +10n=\Omega(n^2)$
	- 

		![1](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/1.png)


# $Θ$ (Big Theta Notation)

- for two given functions f(n)_f_(_n_) , g(n)_g_(_n_)
	- 위에서 누르고
	- 아래에서 누르고

> 💡 $g(n) = \Theta(f(n))g(n)=Θ(f(n)) \iff⟺ g(n) = O(f(n))g(n)=O(f(n)) and g(n) = \Omega (f(n))$

- that is,

	> 💡 $g(n) = \Theta (f(n))g(n)=Θ(f(n)) \iff⟺ \exists c,d \in \mathbb{R}∃c,d∈R and N \in \mathbb{Z^+ \cup {0}}N∈Z+∪0 s.t. g(n) \geq cf(n)g(n)≥cf(n) \forall n \geq N∀n≥N$

- We say that g(n)_g_(_n_) is order of f(n)_f_(_n_).
- The \ThetaΘ puts an asymptotic bound on a function.
- $0.1n + 10n^2 = O(n^{1000}) / O(n^2)$
	- Big O 로 N 자승이다 하며는 tight upper bound를 이야기한다.
- Ex:
	- $37\log n+0.1n=\Theta(n)$
		- $O(n), \Omega(n)$
	- $n^2 + 10n = \Theta(n^2)$
	- $4(logn)^2 +nlogn+100n=\Theta(nlogn)$

> 💡 $\Theta(1)<\Theta(log n)<\Theta(n)<\Theta(n log n)<\Theta(n^2)<\Theta(n^3)<\Theta(n^j)<\Theta(n^k)<\Theta(a^n)<\Theta(b^n)<\Theta(n!)$

- for $k>j>3 , b>a>1$
- O(1) or _O_(_c_) : constant
	- $g(n) = 0.000001 \cdot n$
	- g(n) = 1000000_g_(_n_)=1000000
- Ref. Neapolitan Ex. (pp.42) 19, 24, 26, 28]

# Big O, Omega, and Order

- 

	![2](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/2.png)

- Ref._Ref_. [ Neapolitan Chapter 1.]

## Execution Times for Algorithms with the Given Time Complexities

- n의 k승, 이를 polynomial time (다항 시간)이라고 칭한다.
- n log n, log는 다항식이 아닌데 왜 polynomial이라고 하는가?
	- upperbound : n 자승보다 빠르니까 polynomial time에 들어가게 됨.
- 실제적으로 회사 가서 이 문제를 풀어주는 SW를 해서, 제출한다고 했을 때, n이 커짐에 따라 이러한 효율적인 알고리즘들은 그 피해가 덜한데, 뒤는 피해가 현실적으로 받아들일 수 없을 정도로 커진다. 우리가 이론적으로 구별은 무엇이냐 하면은 polynomial - nonpolynomial alg.를 구분
	- $n^6$은 이론적으로는 polynomial - 안 좋긴 하지만
- exponential, factorial algorithm은 inefficient, polynomial algorithm은 algorithm
	- 현실적으로는 cubic도 빡셈
- 알고리즘 :
	- 이론적인 측면에서 polynomial time 에 쓰이는가
	- 프로그램을 구현해서 돌릴 때, 당연히 cuvic보다는 효유적으로 돌아갈 것이다
- 현실적으루 n이 작을 때에는 그렇게 큰 지장이 없지만, n이 점차 커질 때 log, linear는 잘 버티는 데에 반해 exp, factorial
	- processor가 좋아진다 한들 풀고자 하는 문제가 더 커지기 때문에 시대적 needs라기 보다는 항상 우리 곁의 needs

	![3](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/3.png)

	- logarithmic, linear, n log n, quadratic, cubic << exp << factorial

![4](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/4.png)


# Worst-Case vs. Average-Case Time Complexity

- complexity : time, space complexity
	- 편의상 time complexity를 따짐
- **Expected value** (from Wikipedia)
	- let X_X_ be a random variable with a finite number of finite outcomes $x_1, x_2, ..., x_k$ occuring with probabilities $p_1, p_2, ... p_k$ respectively.
	- the Expectation of X is defined as :
		- $E(X) = \sum_{i=1}^{k }{x_i p_i} = x_1p_1+ x_2 p_2 + ... + x_k p_k$
	- since the sum of all probabilities $p_i$ is 1 (\sum_{i=1}^{k} {p_i}=1∑_i_=1_k__pi_=1) , the expected value is the weighted sum of the x_i_xi_ values, with the p_i_pi_ values being the weights
	- $a_1, a_2, a_3 \rightarrow b_1 , b_2, b_3$
		- input size n에 대해서 모든 가능한 input들의 집합을 S_n
		- 임의의 input $I$
		- $c(I) $ :
		- $p(I):$
- Worst-case complexity :
	- 모든 가능한 인풋중에 최악으로 가장 많은 시간이 걸리는 경우. 가장 시간이 많이 걸리는 경우
	- $T_W (n) = max \{ c(I)| I \in S_n \}$
- Average-case complexity
	- 모든 input에 대해서 일어날 확률 등에 대해 평균을 낸 것.
	- $T_A (n) = \sum_{I \in S_n} p(I) c(I)$
- Problem
	- Find the index of a given value _a_ in a givven array $(a_0, a_1, ...,a _{n-1})$. if _a_ doesn't exist in the array return −1
- Cost for a linear search algorithm
	- let P_i_Pi_ be the probability such that a= a_i_a_=_ai_
	- then the average cost is :

		$g(n) = 1 \cdot P_0 + 2 \cdot P_1 + 3 \cdot P_2 + ...+ n \cdot P_{n-1} + n (1 - \sum_{k=0}^{n-1} P_k)$


		$= \sum_{k=0}^{n-1} (k+1)P_k + n (1 - \sum_{k=0}^{n-1} P_k)=∑k=0n−1(k+1)Pk+n(1−∑k=0n−1Pk)$

		- Ex.1. $n = 10^9$, $P_0 + P_1 + ...+ P_{10^3} = 1$ so $g(n)=O(1)$
		- Ex.2. $n = 10^9$, $P_0 + P_1 + ...+ P_{\frac n {100} }= 1$, so g(n) = O(n)_g_(_n_)=_O_(_n_)

> 💡 배우게 될 내용, 중요

- [중요] **참고: Quick sort 알고리즘 →**
	- Worst-case : $O(n^2)$
	- Average-Case : $O(n \log n)$

# Reviews


## Summation

- Sums of powers
	- $\sum_{i=1}^{n} i = \frac {n(n+1)} {2}$
	- $\sum_{i=1}^{n} i^2 = \frac {n(n+1)(2n+1)} {6}$
	- $\sum_{i=1}^{n} i^3 = (\frac {n(n+1)} {2})^2$
	- $\sum_{i=1}^{n} i^4 = \frac {n(n+1)(2n+1)(3n^2+3n-1)} {30}$
	- $\sum_{i=1}^{n} i^s = \frac {(n+1)^{s+1}} {s+1} + \sum_{k=1}^{s} \frac {B_k} {s-k+1} {s \choose k} (n+1)^{s-k+1}$
		- $B_k$ is the $k^{th}$ Bernoulli Number.
	- $\sum_{i=1}^{n} i^{-s} = \prod_{p prime} \frac {1} {1 - p^{-s}} = \zeta(s)$
		- $\zeta_k$ is the Riemann zeta function
- Growth rates
	- $\sum_{i=1}^{n} i^c \in \Theta(n^{c+1})$
		- for real c greater than 1−1
	- $\sum_{i=1}^{n} \frac 1 i \in \Theta(log n)$
	- $\sum_{i=1}^{n} c^i \in \Theta( n \cdot log(n)^{c+1})$
		- for real c_c_ greater than 11
	- $\sum_{i=1}^{n} log(i)^c \in \Theta(n \cdot log(n)^{c})$ for nonnegative real $c$
	- $
	
	\sum_{i=1}^{n} log(i)^c \cdot i^d \in \Theta(n^{d+1} \cdot log(n)^{c})$ for nonnegative real $c, d$
	- $\sum_{i=1}^{n} log(i)^c \cdot i^d \cdot b^i \in \Theta(n^{d} \cdot log(n)^{c} \cdot b^n) $for nonnegative real $b>1, c, db>1,c,d$
- **Read** [_Summation_](http://en.wikipedia.org/wiki/Summation), [_Mathematical Series_](http://en.wikipedia.org/wiki/List_of_mathematical_series.)

## Run Time Analysis


What is the worst-case time complexity of each loop?

- 어디가 dominate한가 : SW를 개발 할 때 최적화를 해 주어야 함. program이 도는 것을 보면 어디가 bottleneck이 되어 비효율적인가
- (1) Matrix Addition : $O(n^2)$

	```c
	for (i = 0; i < N; i++)
	  for (j = 0; j < N; j++)
	    a[i][j] = b[i][j] + c[i][j];
	```

- (2)$O(n^2)$
	- `x+= i+j`가 i번 수행되고, $\Sigma_{i=1}^N i = \frac{N(N+1)}{2} = \frac {N^2} 2 + \frac N 2$

	```c
	x = 0;
	for (i = 1; i <= N; i++)
	  for (j = 1; j <= i; j++)
	    x += i + j;
	```

- (3) $O(n^2)$

	```c
	for (i = 1; i <= N; i++)
	        if (i % 2 == 0)
	            a[i] = 1;
	        else
	            a[i] = -1;
	    // N^2
	for (i = 1; i <= N; i++)
	        for (j = 1; j <= N; j++)
	            a[i][j] = i + j;
	    // N^2
	```

- (4) $O(n^3)$
	- $\frac N 2 \cdot N$ . $\frac N 2 N^2$ → $n^3$이 dominate

	```c
	for (i = 1; i <= N; i++)
	    {
	        if (i % 2)
	        {
	            for (j = 1; j <= N; j++)
	                a[i][j] = i + j;
	        }
	        else
	        {
	            for (j = 1; j <= N; j++)
	            {
	                a[i][j] = 0;
	                for (k = 1; k <= N; k++)
	                    a[i][j] += k;
	            }
	        }
	    }
	```

- (5)
	- $\Sigma_{i=1}^N \Sigma_{j=1}^i j= O(N^3)$

	```c
	x = 0;
	for (i = 1; i <= N; i++)
	  for (j = 1; j <= i; j++)
		//What if this is i*i?
			for (k = 1; k <= j; k++)
			x += i + j + k;
	```

- (6)$ \rightarrow O(N^4)$
	- $\Sigma_{i=1}^N \Sigma_{j=1}^{i^2} \Sigma_{k=1}^j k= O(N^4)$

	```c
	x = 0;
	for (i = 1; i<=N;i++)
	  for (j = 1; j <= i*i; j++)
	    if (j % i == 0) //j가 i의 배수이면
	      for (k = 1; k <= j; k++)
	        x++;
	```

	- n 이 작냐, 크냐에 따라서 10만이냐 100만이냐 할 때 어떤 속도로 나빠질 것인가? 얼마나 잘 유지될 것인가?
	- 항상 j가 1부터 i^2까지 도는데, j%i ==0 일 때 까지만 돈다 : j가 i의 배수인 경우
	- j가 i의 배수일 때 :
		- $i = 1,2, ... n$
		- $j = 1, i, 2i, ..., i^2$
		- $\Sigma_{i=1}^{N}{(1 + i + ... + i^2)}$
		- 

What is the worst-case time complexity of each loop?

- (1)

	```c
	// n = 2^k for some positive
	    // integer k
	    for (i = 1; i < N; i++)
	    {
	        j = n;
	        while (j >= 1)
	        {
	            // some O(1) computation
	            j = j / 2;
	        }
	    }
	```

- (2)

	```c
	// n = 2^k for some positive
	    // integer k
	    i = n;
	    while (i >= 1)
	    {
	        j = i;
	        while (j <= n)
	        {
	            // some O(1) computation
	            j = 2 * j;
	        }
	        i = i / 2;
	    }
	```

- (3) Could this be faster?

	```c
	//
	    float x[n][n + 1];
	    for (i = 0; i <= n - 2; i++)
	        for (j = i + 1; j <= n - 1; j++)
	            for (k = i; k <= n; k++)
	                x[j][k] = x[j][k] – x[i][k] * x[j][i] / x[i][i];
	```

- (4) Magic square : Could this be faster?

	```c
	// n: odd integer
	    for (i = 0; i < n; i++)
	        for (j = 0; j < n; j++)
	            s[i][j] = 0;
	    s[0][(n - 1) / 2] = 1;
	    j = (n - 1) / 2;
	    for (key = 2; key <= n * n; key++)
	    {
	        k = (i) ? (i - 1) : (n - 1);
	        l = (j) ? (j - 1) : (n - 1);
	        if (s[k][l])
	            i = (i + 1) % n;
	        else
	        {
	            i = k;
	            j = l;
	        }
	        s[i][j] = key;
	    }
	```

- (5)$ O(\log n)$

	```c
	// compute x^n (n >= 0)
	    m = n;
	    power = 1;
	    z = x;
	    while (m > 0)
	    {
	        while (!(m % 2))
	        {
	            m /= 2;
	            z *= z;
	        }
	        m--;
	        power *= z;
	    }
	```

- time complexity. : $c_0 + c_1 n + c_2 n^2 = O(n^2)$

	```c
			x = x + 1;
	    for (i = 1; i <= n; i++)
	        y = y + 2;
	    for (i = n; i >= 1; i--)
	        for (j = n; j >= 1; j--)
	            z = z + 1;
	```

- time complexity. : $c( ⌊{log_2 n}⌋+1) \cdot n^2 = O(n^2)$

	```c
	c = 0;// n > 0
	for (i = 1; i <= n; i++)
	  for (j = 1; j <= n; j++)
	    for (k = 1; k <= n; k = k*2)
	      c += 2;
	```

	- for k=1;k≤n;k=k*2
	- floor : 3.7 → 3, ceil : 3.7 → 4
	- $n=15 \rightarrow \lfloor log_2 15 \rfloor = \lfloor 3.*** \rfloor = 3$
	- 1 2 4 8 15
- time complexity. : $??= O( \sqrt n)$

	```c
			i = 1;
	    j = 1;
	    m = 0; // n > 0
	    while (j <= n)
	    {
	        i++;
	        j = j + i;
	        m = m + 2;
	    }
	```


---
title: "Deep Neural Net 2 (Chapter 10)"
excerpt: "Chap10.DNN2"
slug: "ML-10-DNN"
category: "ml"
lang: en
use_math: true
tags: ["ML", "AI", "Lecture"]
date: 2022-06-05T22:26:09-05:00
draft: false
---


# 10. DNN 2
- 구조상 차이 : hidden layer 개수
fully connected layer -> conv layer
- Activation, normalization 하는 부분들
- weight update하는 부분들

여러 학습 알고리즘이 제안되며 더 좋은 성능을 가지게 됨

# Data Processing

input layer - hidden lyaer - output layer로 그려진 NN

- x, y, z (hidden lyaer 하나로만 두고 보면) y layer 의 첫 번째 node를 볼 때 각자 연결된다
- fc 구조가 기본적인 구조
- 앞 node의 weight combination + bias

- Consider what happens when the input to a neuron is always positive...
    
    ![Untitled](10/Untitled.png)
    
    - i의 변화에 따라 x도 변화 → w가 모든 같은 부호
- w update slowly → learn slowly
    - 이상적인 w에 도달하지 않을 수 있다
    - 학습 시간에도 문제가 될 수 있다

- What can we say about the gradients on w?
- Always all positive or all negative!
- (this is also why you want zero-mean data!)
    - data normalization : $\mu = 0$
    - for input hidden layer

## Step 1: Preprocess the data

(Assume X [NxD] is data matrix, each example in a row)

![Untitled](10/Untitled_1.png)

- normalized data : $N(0, 1^2)$
    - learn faster

In practice, you may also see PCA and Whitening of the data

PCA : dimensionality reduction, clustering (unsupervised)

- decorrelated data
    - (data has diagonal covariance matrix)
    - axis방향으로 평행 : with x1 only (1dim)
    - from 2dim → 1dim compression (data info loss)
- whitened data
    - (covariance matrix is the identity matrix)
        
        $\Sigma =$
        
    - acc 변화했을 수도 있으므로 performance 체크

![Untitled](10/Untitled_2.png)

In practice for Images: Zero-center only

- e.g. consider CIFAR-10 example with [32,32,3] images
- Subtract the mean image (e.g.AlexNet)
    - (mean image = [32,32,3] array)
        - RGB 평균 구해서 average
- Subtract per-channel mean (e.g.VGGNet)
    - (mean along each channel = 3 numbers)
- Not common to normalize variance (for PCA or whitening)
    - w 효과적으로
    - PCA → data rotation, whitening → Cov to Identity Mat
    - if original data에 대한 학습 중 특정 w를 학습해야 한다면

# Weight Initialization

원래 w : randomly init.

- Q: what happens when W=0 init is used?
    
    ![Untitled](10/Untitled_3.png)
    

f’(0) x_i → wi에 update됨

모든 w에 동일한 pattern으로 update

i가 달라짐에 다르게 but 같게 실행 : 지그재그로 수행되는 것보다 성능이 안좋을 수 있음

- First idea: Small random numbers
    - (gaussian with zero mean and 1e-2 standard deviation)
    
    ```python
    W = 0.01 * np.random.randn(D,H)
    ```
    
    - small w : hidden layer가 커질수록 → backprop되는 layer 적어짐
        - 뒤 layer로 갈수록 앞 layer update 작아짐 : 학습 잘 안됨
    - big w : vanishing GD : update 안될수록 f’ 작아짐
        - 
    - Works ~okay for small networks, but problems with deeper networks.
- Lets look at some activation statistics
    - 10-layer net with 500 neurons on each layer, using tanh
    - non-linearities, and initializing as described in previous slide.
- All activations become zero!
- Q: think about the backward pass. What do the gradients look like?
    - Hint: think about backward pass for a W*X gate.

- *1.0 instead of *0.01
- Almost all neurons completely saturated, either -1 and 1. Gradients will be all zero.

Deep Neural Networks
Weight Initialization

“Xavier initialization” [Glorot et al., 2010]

```python
W = np.random.randn(fanin, fanout) / np.sqrt(fanin)
```

- Reasonable initialization. (Mathematical derivation assumes linear activations) with tanh fn
- 현재 layer node의 sqrt로 init
- but when using the ReLU nonlinearity it breaks. (0에 접근)

He et al., 2015 (note additional /2)

- error가 더 잘 감소됨을 확인할 수 있음

```python
W = np.random.randn(fanin, fanout) / np.sqrt(fanin/2)
```

# Proper initialization is an active area of research

- Understanding the difficulty of training deep feedforward neural networks
by Glorot and Bengio, 2010
- Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe et al, 2013
- Random walk initialization for training very deep feedforward networks by Sussillo and Abbott, 2014
- Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification by He et al., 2015
- Data-dependent Initializations of Convolutional Neural Networks by Krähenbühl et al., 2015
- All you need is a good init, Mishkin and Matas, 2015 ...

# Batch Normalization

[Ioffe and Szegedy, 2015]

layer에 data를 넣을 때 전체 data를 randomly partition하여 mini batch 생성

- “you want unit gaussian activations? just make them so.”
- consider a batch of activations at some layer.

- To make each dimension unit gaussian, apply:
    - $\hat x^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$
        - this is a vanilla differentiable function...
- dimension 단위 normalilze

1. compute the empirical mean and variance independently for each dimension.
    
    ![Untitled](10/Untitled_4.png)
    
2. Normalize
    - $\hat x^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$
- Usually inserted after Fully Connected or Convolutional layers, and before **nonlinearity.**

![Untitled](10/Untitled_5.png)

- Normalize:
    - $\hat x^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$
    - $\hat x \sqrt{Var} + E[x] = X$
    - $\gamma = \sqrt{Var} , \beta = E[x]$
    - $\hat x$ : normalized data, X : original data
- And then allow the network to squash the range if it wants to:
    - $\hat y^{(k)} = \gamma^{(k)} \hat x^{(k)}+\beta^{(k)}$
- Note, the network can learn:to recover the identity mapping.
    - $\gamma^{(k)} = \sqrt{Var[x^{(k)}]}$ (stretch)
    - $\beta^{(k)} = E[x^{(k)}]$ (이동)
    
- $X \rightarrow \hat X$ : normalize : $\gamma, \beta$로 scaling된 y value

![Untitled](10/Untitled_6.png)

- Improves gradient flow through the network
- Allows higher learning rates
- Reduces the strong dependence on initialization
- Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe
- Note: at test time BatchNorm layer functions differently:

- The mean/std are not computed based on the **batch(test batch)**.
- Instead, a single fixed empirical mean of activations during training is used.
    - (e.g. can be estimated during training with running averages)

---

# —

# Model Ensembles

1. Train multiple independent models
2. At test time average their results (여러 model result average)

- Enjoy 2% extra performance

- single model : learning method 변경
    - ex. tanh → ReLU : 종합하여 result 추출

# How to improve single-model performance?

![Untitled](10/Untitled_7.png)

Regularization

# Regularization: Add term to loss

$$
L = \frac 1 N \Sigma_{i=1}^{N} \Sigma_{j\neq y_i}{\max(0, f(x_i);W)_j - f(x_i);W)_{y_i} +1} + \lambda R(W)
$$

- Loss Fn : $\frac 1 N \Sigma_{i=1}^{N} \Sigma_{j\neq y_i}{\max(0, f(x_i);W)_j - f(x_i);W)_{y_i} +1}$
- Lambda weight term $+ \lambda R(W)$

![Untitled](10/Untitled_8.png)

- In common use:
    
    ![Untitled](10/Untitled_9.png)
    

## Regularization: Dropout

- In each forward pass, randomly set some neurons to zero Probability of dropping is a hyperparameter; 0.5 is common
    
    ![Untitled](10/Untitled_10.png)
    
- Srivastava et al, “Dropout: A simple way to prevent neural networks from overfitting”, JMLR 2014

Example forward pass with a 3-layer network using dropout

![Untitled](10/Untitled_11.png)

![Untitled](10/Untitled_12.png)

- How can this possibly be a good idea?
- Forces the network to have a redundant representation;
    - Prevents co-adaptation of features
    
    개별 node들이 학습에 적극적으로 사용되도록 강제화시키는 작업
    

![Untitled](10/Untitled_13.png)

### How can this possibly be a good idea?

randomly selection of node → network 구조 다양 - 종합적 사용

- Another interpretation:
    - Dropout is training a large **ensemble** of models (that share parameters).
    - Each binary mask is one model
    - An FC layer with 4096 units has 24096 ~ 101233 possible masks!
    - Only ~ 1082 atoms in the universe..
    

# Dropout: Test time

- Dropout makes our output random!
    
    ![Untitled](10/Untitled_14.png)
    
- Want to “average out” the randomness at test-time
    - $y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)dz$ㅓㄹ
    - 여러 var mask  ㅇ - mu, sum
- But this integral seems hard ...
- Want to approximate the integral
    - $y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)dz$
- Consider a single neuron.
    
    ![Untitled](10/Untitled_15.png)
    
- At test time we have:
    - $E[a] = w_1x + w_2y$
- During training we have:
    
    ![Untitled](10/Untitled_16.png)
    
- **At test time, multiply by dropout probability**

![Untitled](10/Untitled_17.png)

- p to be dropped → *p : 값을 줄여나가는 식으로 진행
- At test time all neurons are active always
    - → We must scale the activations so that for each neuron:
    - output at test time = expected output at training time

## Dropout Summary

- drop in forward pass
    - train때 사용된 w값이 test 시점에도 반영
- scale at test time
    
    ![Untitled](10/Untitled_18.png)
    

## More common: “Inverted dropout”

- train시 p로 나누어줌
- test time is unchanged!
    - test 단계에 적용 → test 에서는 X X
    - if v1 > p → test시점 (1-p)
    
    ![Untitled](10/Untitled_19.png)
    

# Regularization: A common pattern

- Cross Entropy Loss : $-\Sigma y_i \lim P_i$
    - yi : the answer
    - Pi : prediction
- Training: Add some kind of randomness
    - randomness - regularize
    - $y = f_W (x,z)$
- Testing: Average out randomness (sometimes approximate)
    - $y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)dz$
    - iteration별 여러 mu, sigma → average를 구하여 test시점에 적용한다
    - BN도 regularization작업
- Example:BatchNormalization
    - Training:Normalize using stats from random mini batches
    - Testing:Use fixed stats to normalize
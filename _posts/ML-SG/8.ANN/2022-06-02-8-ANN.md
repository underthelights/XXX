---
title: "Artificial Neural Net (Chapter 8)"
excerpt: "Chap8.ANN"
slug: "ML-8-ANN"
category: "ml"
lang: en
use_math: true
tags: ["ML", "AI", "Lecture"]
date: 2022-06-02T22:26:09-05:00
draft: false
---
# 8. ANN

# ANN Introduction

![h+ p ://cs231n.stanford.edu/slides/winter1516_lecture4.pdf](https://user-images.githubusercontent.com/46957634/188280599-fd556de5-9369-437d-b9da-ddbac640bd3e.png)

h+ p ://cs231n.stanford.edu/slides/winter1516_lecture4.pdf

- 

![© 2017, SNU BioIntelligence Lab, h+ p ://bi.snu.ac.kr/](https://user-images.githubusercontent.com/46957634/188280605-6b5b6d58-7520-47aa-b52f-bc527b096ea1.png)

© 2017, SNU BioIntelligence Lab, h+ p ://bi.snu.ac.kr/

- 

- Classification with a line, $y = ax + b$
    - single, multi layer perceptron with NN
    
    ![Untitled_2](https://user-images.githubusercontent.com/46957634/188280608-8fc02bb6-d702-4a2d-9787-24b564739e9d.png)
    ![Untitled_3](https://user-images.githubusercontent.com/46957634/188280609-111f213a-6d3f-40ec-9b44-0c154736abe1.png)
    
    
- Generalization
    
    ![Untitled_4](https://user-images.githubusercontent.com/46957634/188280610-1e1b8f1f-7146-4151-8992-beecbd21148e.png)
    
## Artificial Neural Networks (2 layers)

- NN 기본 구조 perceptron → hidden layer 쌓아 DNN
- Two-layer network
    - input layer + output layer (2)
    
    ![Untitled_6](https://user-images.githubusercontent.com/46957634/188280613-92493064-dc7e-4b5b-9e1c-eb139df14d42.png)

    
    - sigmoid 형태
    - no hidden layer → linearly separable prob only

![Untitled_5](https://user-images.githubusercontent.com/46957634/188280612-1c48ef53-13ac-4d4d-b8e3-249d3b14124f.png)

## AND


![Untitled_7](https://user-images.githubusercontent.com/46957634/188280614-20e30d3a-eea9-4ee1-9d30-5ad80c6ffdba.png)
![Untitled_8](https://user-images.githubusercontent.com/46957634/188280615-3a2d3b4f-3ebb-41c3-8990-69ed36044297.png)
![Untitled_9](https://user-images.githubusercontent.com/46957634/188280617-419dd254-73e0-4147-a75f-9fbe63c5ffd3.png)

## OR

![Untitled_10](https://user-images.githubusercontent.com/46957634/188280618-c0b0a7a2-586e-4934-838f-4dd35f0ad0b9.png)
![Untitled_11](https://user-images.githubusercontent.com/46957634/188280619-f0e88fd8-f90f-4dd0-bb97-2fcc32431c8e.png)
![Untitled_12](https://user-images.githubusercontent.com/46957634/188280620-a5af0e0b-c03f-4713-bc74-c328d09f25f2.png)

## XOR

![Untitled_13](https://user-images.githubusercontent.com/46957634/188280621-5b7cf2f0-6451-4b1c-9232-99f0db454138.png)
![Untitled_14](https://user-images.githubusercontent.com/46957634/188280622-ea215239-ba67-471a-815a-87601d488648.png)


- Two layer network cannot implement XOR.
    - perceptron 으로 분류 시 error가 크게 나타남

- Non-linear
    - two decision boundary (nonlinear)
    
    ![Untitled_15](https://user-images.githubusercontent.com/46957634/188280623-6edf3a68-4022-4a53-8d79-4a482826ac5a.png)
    
    ![Untitled_16](https://user-images.githubusercontent.com/46957634/188280625-b764e005-d90a-4d34-a441-a38c55250088.png)



# Multi-layer Networks

![Untitled_17](https://user-images.githubusercontent.com/46957634/188280626-1a22355d-ff83-467f-bb86-05e868a63b2d.png)


- input - hidden - output
- hidden layer 개수 → 몇 개인지에 따라 network 구조가 좌우됨
    - linearly nonsolvable 문제들도 풀 수 있게 된다

![Untitled_18](https://user-images.githubusercontent.com/46957634/188280628-c3a6a074-c967-466a-b133-ce3d9d311c94.png)
![Untitled_19](https://user-images.githubusercontent.com/46957634/188280630-bb888c8e-ba9f-4ed4-b184-3f37dee0a88d.png)
![Untitled_20](https://user-images.githubusercontent.com/46957634/188280632-3398c4bf-532a-43fc-849b-6f597ee07b14.png)

- hidden layer 1개 추가 → 많은 가중치 w들을 학습한다
- 수백개 hidden layer →  수천, 수만 w 학습


![Untitled_21](https://user-images.githubusercontent.com/46957634/188280633-8a876c51-60a8-488d-a21e-3d60d5bbf7da.png)
![Untitled_22](https://user-images.githubusercontent.com/46957634/188280635-184498eb-10dd-4446-a78e-25c1df09ee3f.png)
![Untitled_23](https://user-images.githubusercontent.com/46957634/188280636-97988193-22f4-4fa6-8b12-76a172f7e1b9.png)

- $wx = \Sigma_i w_i x_i$

![Untitled_24](https://user-images.githubusercontent.com/46957634/188280637-228af070-e1ab-4faa-a98a-31e4e128191c.png)

- feature space : input layer → hidden layer
- SVM과 유사한 효과 : high dimension
    - 원래 feature space: 원래는 not linearly separable하지만 high dimension phi fn을 활용
![Untitled_25](https://user-images.githubusercontent.com/46957634/188280640-d4713e9e-ab23-4794-9b84-8723cffdf9a2.png)
![Untitled_26](https://user-images.githubusercontent.com/46957634/188280641-0587860d-c9fc-4a66-a3a4-afd8d2b8b694.png)

- hidden layer를 추가하면 → 복잡한 boundary를 구할 수 있다


![Untitled_27](https://user-images.githubusercontent.com/46957634/188280643-c9e43cb8-8720-4718-86bd-48ffa1b8156d.png)
# ANN Training

- ANN training?
    - Estimate w by using training data
    
    ![Untitled_28](https://user-images.githubusercontent.com/46957634/188280645-267339ea-354b-4f80-8597-78065553e3f4.png)

    
    |  | training |
    | --- | --- |
    | perceptron | w |
    | SVM | w : 
    - margin, slack var, Phi fn |
    | Bayesian | G(mu, sigma^2) : prior
    - density modeling → likelihood
    
    - prior P → post P |
    | Decision Tree | Tree 구조 |
    | kNN | save train data
    - k value : train통해 결정
    - dinstance metric  : 
    L1 vs L2 vs Euclidean |
    | ANN | w
    NN Structure
    - I/H/O Layer개수 |
    | DNN |  |
    |  |  |
    - test data 분류 작업
    
    - regularizer
    - hyperparameter normalize

- By experiment ← train 중 parameter 결정
- data  : multimedia

## 1. Decide input layer’s node number

- By experiments, use domain knowledge

![Untitled_29](https://user-images.githubusercontent.com/46957634/188280647-6977fd58-709a-4a5a-89c4-7198e97e0c39.png)
## 2. Decide output layer’s node number

- By experiments, use domain knowledge


![Untitled_30](https://user-images.githubusercontent.com/46957634/188280648-ef85e47c-b544-4c90-b78c-a1052346482b.png)
![Untitled_31](https://user-images.githubusercontent.com/46957634/188280649-d3324505-f5bd-4df1-a18b-d51cb016fc29.png)

- 구조만 보면 class가 몇 개인지 모른다
- class 수 = node 수
    - classification : output layer node의 activation 여부 확인

## 3. Decide hidden layer’s node number

- By experiments, use domain knowledge


![Untitled_32](https://user-images.githubusercontent.com/46957634/188280650-380a553d-089d-4fdd-b59c-91c44476c173.png)
## 4. Find weight using training algorithm

- Use back-propagation algorithm. Supervised learning.


![Untitled_33](https://user-images.githubusercontent.com/46957634/188280652-89137d13-9776-4895-9188-edda2933dea3.png)

# Back-propagation Algorithm

- 등장 배경 : NN → SVM →DNN
    - image가 낮인지 밤인지 예견하는 문제
    - data를 잘 준비했어야 하는데 overfitting을 해결하지 못하여 SVM이 1990-2010년도 성행
- train with back propagation algorithm
    - w값 arbitraily initialize → feed forward :
        - **w : $\epsilon \downarrow$방향으로 update**
        - between prediction , ground truth value
- $y = f (\Sigma_i x_i w_{ij} + b)$
    - network의 구조가 결정되면 다음 layer의 node값에 bias를 더하는 함수
    - perceptron 문제와 유사하게 발생 :
        - local minima
        - many w → T 증가
        - iterative하게 w 구하는 과정
    
    ![Untitled_34](https://user-images.githubusercontent.com/46957634/188280654-7626b34b-fdb4-4169-a409-fb02a3a56ecf.png)

    
    모든 edge에 할당된 w에 대하여 backpropagation
    

![Untitled_35](https://user-images.githubusercontent.com/46957634/188280655-7390abaa-af14-4eb6-b7ac-a7f2e8393e48.png)

- w update하면 iteration 반복 많아짐
- hidden 많아지면 update bigger

- $E = \Sigma_k \frac 1 2 (z_k - t_k)^2$
    
    tk : ground truth
    
    - 제곱을 하고 2로 나누어준 이유 : perceptron처럼, 미분을 위해 제곱하고 1/2 *2하면 표현식이 간단해진다
    - error 미분 : wij, wji 미분하여 update

chain rule

- $\frac{\partial E}{\partial w_{kj}} = \frac{\partial E}{\partial z_{k}} \frac{\partial z_k}{\partial w_{kj}}$
- $\frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial z_{k}} \frac{\partial z_k}{\partial w_{ji}}  = \frac{\partial E}{\partial z_{k}} \frac{\partial z_k}{\partial y_j}\frac{\partial y_j}{\partial w_{ji}}$


![Untitled_36](https://user-images.githubusercontent.com/46957634/188280656-b3bbb82c-b51b-404b-983c-89b001b5b435.png)
![Untitled_37](https://user-images.githubusercontent.com/46957634/188280657-3d4e8155-0461-4529-aacb-4f22e646141d.png)
![Untitled_38](https://user-images.githubusercontent.com/46957634/188280658-060efecd-4e1a-4b11-afaf-857232a6c63e.png)
![Untitled_39](https://user-images.githubusercontent.com/46957634/188280659-753d7774-270d-4f3a-8b56-087a084f2726.png)


- big NN의 한계 : hidden layer 제한 → 다양한 성능 제한
- **hidden layer수 증가 감소 이유 :???**
    - output에서 input으로 backpropagate할 수록 더 많은 chain rule (Differentiation) 수행 → update양 감소
- To update weights far from the output layer, more nodes and their derivatives are involved. Due to the chain rule, the amount of update becomes smaller.

## Activation function: sigmoid

perceptron, SVM

- input data (weight) - f(x) update
    - 동일한 input에 대해 error 계산
    - error가 각 iteration별로 작아짐
- input data 바꾸면 - updated w도 new input data에 부적절할수도 있으니 fix해야 한다

![Untitled_40](https://user-images.githubusercontent.com/46957634/188280660-50fa1def-3a03-4b0c-9aec-17747f5d1e33.png)
- 부적합
- SVM에서, nondifferentiable fn : hinge loss → 구간 나누어 미분 → 복잡해지는 미분 계산 과정 itreation
    - 수학적으로 옳지는 않다??

![Untitled_41](https://user-images.githubusercontent.com/46957634/188280661-d687f5c3-18e8-4ab5-a137-6fed144b840a.png)
![Untitled_42](https://user-images.githubusercontent.com/46957634/188280662-b775a1fd-9428-4e7e-953e-688559836ae3.png)
- back propagation example
    - Ex) 1st error = 0.298371109 
    2nd error = 0.291027924
    ...
    ...
    10000th error = 0.000035085
    - output neurons after 10000th iteration : 
    0.015912196 (vs 0.01 target) 
    0.984065734 (vs 0.99 target)

![Untitled_43](https://user-images.githubusercontent.com/46957634/188280663-18f4a527-57ac-4a79-a965-7dc9bb2e8228.png)

- error : 역전파하면서 error 감소
    - 10000th로 갈수록 0으로 error 수렴
- output neuron : target에 근사

# Deep Neural Network

![Untitled_44](https://user-images.githubusercontent.com/46957634/188280666-bf6f4813-099e-47a9-bac8-5e1f65a9b1df.png)

- raw data in real world : 영상 획득, microphone 녹음 등 ..

DNN : Number of hidden layers 

- # of hidden layers $\leq 1$ → Shallow Neural Network
- # of hidden layers $\geq 2$ → Deep Neural Network

![Untitled_45](https://user-images.githubusercontent.com/46957634/188280667-12a4453c-311d-4c89-8b39-ade39e245887.png)

## DNN Training

### Vanishing gradient problem

Problem with non-linear activation

- As errors are back propagated, the gradient vanishes
- Derivatives of sigmoid are 0~0.25. As these are multiplied at multiple layers, they become smaller and smaller
- $w_{ji} = w_{ij} -\eta \frac {\partial E}{\partial w_{ji}}$: as $\frac {\partial E}{\partial w_{ji}}$becomes smaller, $w$ is not well updated, especially for the layers far from the output layer

![Untitled_46](https://user-images.githubusercontent.com/46957634/188280669-e2e0bd8c-ab14-42f7-8653-bc7ec91adc16.png)

- Typically requires lots of labeled data
    - Collecting data is time consuming and expensive (time, price)
    - but time 소모 증가, 비용 증가
        - data collection, tagging, labeling
    - 2010년도 DNN의 발전배경  :
        - Big data → 수집 용이
        - GPU (HW Support)
        - Algorithm적 발달
- ***Overfitting problem***
    - When training data is not sufficient, model only can handle the training data well (poor performance at test time)
    - data 변화 요인에 w 학습하고 , 다양한 test에 대해서도 적용
    
    ![Untitled_47](https://user-images.githubusercontent.com/46957634/188280670-892307f0-cbe3-4e9f-86e0-60f4836eb8b0.png)

    
- ground truth sampling → data에 fit하여 정답과 일치하도록
- overfitting : regression, classification 시 정답을 보지 않아도 train well됨을 확인함
- Get stuck in **local minima : iteration시 유의**
    - Problem even with enough training data
    - solution : 초기값을 randomly setting하고 train을 많이 하여 그 avearge를 추출한다
        - test set에 대해 일반적으로 성능 좋아짐
- Train Set 에서의 문
    - input data : raw data→ input layer : x1, x2
    - state of the nature $\in$ real world
    - 학습 data로 model을 만들지만 prediction-ground truth가 0이라고 하여도 좋은 model인지 아닌지 확신할 수 없다.
        - 사실 정답을 알 리가 없다 : 그 정답으로 추출된 제한된 관찰에 의해 얻어진 학습 data로 model을 구한 것이기에
    - sol **→ train data를 변화시키며 model이 얼마나 안정된 결과를 보이는지 (test data를 변화시키기도 해보자)**

- Vanishing gradient problem
    - ReLU(Rectified Linear Unit)
        - $\max(0,x)$ gradient update
    - Layer-wise training
        - 충분히 학습되었다면 건너뜀
            
            ![Untitled_48](https://user-images.githubusercontent.com/46957634/188280673-e07ecb47-7d3a-46dc-9a95-8923804b9d01.png)

            
    
- Requires **lots** of labeled data
- **Overfitting** problem
- Get stuck in **local minima**
    - Mitigated by increasing data and computation power

- 시간 소모 :
    - depend on performance of HW
    - w update : vanishing gradient → activation fn 변화
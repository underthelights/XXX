---
layout: post
date: 2023-06-02
title: "12. Clustering (1)"
tags: [ML, ]
categories: [Notes, ]
use_math: true
---


# Clustering

- Unsupervised learning
- Requires data, but no labels
	- label이 없다
- Detect patterns e.g. in
	- data pattern을 인식하려고 한다 : email / 검색 결과 / 쇼핑 패턴 / 이미지 리전으로부터
- Useful when don’t know what you’re looking for Clustering
	- 일단 clustering 수행하고 나면 해당 data가 어떻게 이루어지는지는 알 수 있다
- Basic idea: group together similar instances
- Example: 2D point patterns Clustering

![0](/assets/img/2023-06-02-12.-Clustering-(1).md/0.png)


![1](/assets/img/2023-06-02-12.-Clustering-(1).md/1.png)


![2](/assets/img/2023-06-02-12.-Clustering-(1).md/2.png)

- What could “similar” mean?
	- One option: small Euclidean distance

		‘ similarity’를 어떻게 판단할 수 있는가

		- 2 dim 공간 상 거리 -> Euclidean distance
		Similarity 는 distance에 반비례
	- Clustering results are crucially dependent on the measure of similarity (or distance) between “points” to be clustered

![3](/assets/img/2023-06-02-12.-Clustering-(1).md/3.png)

- Partition algorithms
	- K-means
	- Mixture of Gaussian
	- Spectral clustering
- Hierarchical algorithms
	- Bottom up : agglomerative
	- Top down : divisive

## Clustering Examples

<details>
  <summary>Image segmentation</summary>

- Image segmentation : 인접한 유사한 feature를 가진 pixel 끼리 group으로 분할
- Goal: Break up the image into meaningful or perceptually similar regions

![4](/assets/img/2023-06-02-12.-Clustering-(1).md/4.png)



  </details><details>
  <summary>Clustering gene data </summary>


Hierarchical clustering : gene group에 큰 두 개의 gruop이 있음을 판단할 수 있다.

- 나중에 들어오는 data에 대해서도 두 가지 class 대상으로 labeling할 수도 있고 두 가지 label로 supervised learning도 가능

![5](/assets/img/2023-06-02-12.-Clustering-(1).md/5.png)



  </details><details>
  <summary>Cluster news articles </summary>


![6](/assets/img/2023-06-02-12.-Clustering-(1).md/6.png)



  </details><details>
  <summary>Cluster people by space and time </summary>


![7](/assets/img/2023-06-02-12.-Clustering-(1).md/7.png)



  </details><details>
  <summary>Cluster languages </summary>


![8](/assets/img/2023-06-02-12.-Clustering-(1).md/8.png)


![9](/assets/img/2023-06-02-12.-Clustering-(1).md/9.png)



  </details><details>
  <summary>Cluster species (phylogeny) </summary>


![10](/assets/img/2023-06-02-12.-Clustering-(1).md/10.png)



  </details><details>
  <summary>Cluster search queries </summary>


![11](/assets/img/2023-06-02-12.-Clustering-(1).md/11.png)



  </details>
# K-Means Clustering


![12](/assets/img/2023-06-02-12.-Clustering-(1).md/12.png)


![13](/assets/img/2023-06-02-12.-Clustering-(1).md/13.png)


## K-Means Clustering: Example


![14](/assets/img/2023-06-02-12.-Clustering-(1).md/14.png)

- Pick K random points as cluster centers (means)
- Shown here for K=2

### Iterative Step 1

- Assign data points to closest cluster center
	- mean값에 대해서 다시 partition./ center값 할당하며 반복하게 되면
	K means clustering 작업에 의한 clustering 결과를 얻게 됨.

![15](/assets/img/2023-06-02-12.-Clustering-(1).md/15.png)


### Iterative Step 2

- Change the cluster center to the average of the assigned points

![16](/assets/img/2023-06-02-12.-Clustering-(1).md/16.png)


## Properties of K-means algorithm 

- Guaranteed to converge in a finite number of iterations

	k값이 대략적인 cluster개수와 일치해야지, 그렇지 않으면 만족하지 못할 결과가 나올 수도 있음

- Running time per iteration:
	1. Assign data points to closest cluster center : O(KN)

		k개의 point, n samples -> kn

	2. Change the cluster center to the average of its assigned points : O(N)

		Cluster center average -> n


## Example: K-Means for Image Segmentation


![17](/assets/img/2023-06-02-12.-Clustering-(1).md/17.png)

- Goal of image segmentation is to partition an image into regions each of which has reasonably homogenous visual appearance.

	K means clustering segmentation

	- clustering : space상에서 한 게 아니고 color값들이 r,g,b의 값을 갖는데 256^3 의 dimension 3 histogram
	- k개의 color로 clustering

# K-means algorithm - initialization 


K-means algorithm is a heuristic

- converging하기도 하고 완전 heuristic하지는 않고 완전 이상한 initial value로 하면 아예 못 구하기도 함
- Requires initial means
	- 초기 center값에 따라 최종 결과가 잘못될수도 있음
	- It does matter what you pick!
- What can go wrong?
- Various schemes for preventing this kind of thing: variance-based split / merge, initialization heuristics
- A local optimum

	![18](/assets/img/2023-06-02-12.-Clustering-(1).md/18.png)

- Would be better to have one cluster here

	![19](/assets/img/2023-06-02-12.-Clustering-(1).md/19.png)


# K-means algorithm

- Not able to cluster properly

![20](/assets/img/2023-06-02-12.-Clustering-(1).md/20.png)


![21](/assets/img/2023-06-02-12.-Clustering-(1).md/21.png)

- Changing the features (distance function) can help

# Clustering Hierarchical Clustering


Two main types of hierarchical clustering

- Agglomerative: 각각의 개별 sample로 시작해서 cluster로 간주하여 각각을 merge하여 최종적으로 1~k개의 cluster가 되기까지 merge
	- Start with the points as individual clusters
	- At each step, merge the closest pair of clusters until only one cluster (or k clusters) left
- Divisive:하나에서 개별 cluster로 divide
	- Start with one, all-inclusive cluster
	- At each step, split a cluster until each cluster contains a point (or there are k clusters)
- Traditional hierarchical algorithms use a similarity or distance matrix

	similarity와 distance는 서로 역수 관계

	- Merge or split one cluster at a time

		Merge/split하는 방식을 hierarchical clustering이라고 함


# Agglomerative clustering

- supervised : data label 제공 / unsupervised : data label 미제공
	- → labeling해서 classification, regression 수행
- clustering : hierarchical process로 data 분석하기도 하고

이상 전기 신호를 어떻게 구분할 것인가 (classification) 해야 하는데 이에 대한 label이 없음


몇 년치 data를 labeling하기도, 분석도 바로 못 들어감.


→ 평균치보다 크거나 작은 값을 군집화하여 이상신호를 보고 판단하고 labeling 수작업


정상과 비정상의 label에 대하여 이후 들어오는 data들에 대하여 수작업 (supervised)


# Agglomerative clustering algorithm


Data point distance metric계산하고 각각의 data point가 개별 cluster로 여겨지고 Closest cluster끼리 merge되고 (방법에는 최단/최대/평균)


Compute the distance matrix between the input data points Let each data point be a cluster Repeat Merge the two closest clusters Update the distance matrix Until only a single cluster remains

- Key operation is the computation of the distance between two clusters
- Different definitions of the distance between clusters lead to different algorithms Clustering
- Most popular hierarchical clustering technique
- Basic algorithm

## Input/ Initial setting


![22](/assets/img/2023-06-02-12.-Clustering-(1).md/22.png)


n Start with clusters of individual points and a distance/proximity matrix 


## Intermediate State

- After some merging steps, we have some clusters
	- 
	- clustering이 진행되면 group화 진행
	- 처음에 data point 12개여서 12_12
	-> clustering되어 5_5
	- data dimension이 커지면 거리 계산에 어려움
	- dimension별 scaling : K nearest neighbor에서 고려해야 함

![23](/assets/img/2023-06-02-12.-Clustering-(1).md/23.png)

- Merge the two closest clusters (C2 and C5) and update the distance matrix

	![24](/assets/img/2023-06-02-12.-Clustering-(1).md/24.png)

- How do we update the distance matrix?

	![25](/assets/img/2023-06-02-12.-Clustering-(1).md/25.png)


# Distance between two clusters


![26](/assets/img/2023-06-02-12.-Clustering-(1).md/26.png)


![27](/assets/img/2023-06-02-12.-Clustering-(1).md/27.png)

- Can handle non-elliptical shapes

	![28](/assets/img/2023-06-02-12.-Clustering-(1).md/28.png)

- Sensitive to noise and outliers
- It produces long, elongated clusters

![29](/assets/img/2023-06-02-12.-Clustering-(1).md/29.png)


![30](/assets/img/2023-06-02-12.-Clustering-(1).md/30.png)


![31](/assets/img/2023-06-02-12.-Clustering-(1).md/31.png)

- More balanced clusters (with equal diameter)

	Balance : separation이 조금 더 명확해졌고 cluster의 크기가 비슷하게 분류됐고, noise에 대해서 약간 덜 예민해짐

- Less susceptible to noise

![32](/assets/img/2023-06-02-12.-Clustering-(1).md/32.png)

- Tends to break large clusters
- All clusters tend to have the same diameter – small clusters are merged with larger ones

	큰 cluster를 작게 쪼개려고 하는 성질

	- cluster크기가 커지면 다른 sample들과 계산하건 거리가 크게 나옴 : 큰 덩어리가 분할되는 결과를 가져옴
	- 전체 cluster들이 비슷한 결과
	- 작은 cluster들이 큰 것을 흡수

![33](/assets/img/2023-06-02-12.-Clustering-(1).md/33.png)


# Average-link clustering: example 

- Proximity of two clusters is the average of pairwise proximity between points in the two clusters.

![34](/assets/img/2023-06-02-12.-Clustering-(1).md/34.png)


Average distance의 길이가 짧을수록 stable cluster


![35](/assets/img/2023-06-02-12.-Clustering-(1).md/35.png)


# Average-link clustering: discussion 

- Compromise between Single and Complete Link
- Strengths
	- Less susceptible to noise and outliers
- Limitations
	- Biased towards globular clusters 각각의 cluster를 구 형태로 만드려는 특성

# Hierarchical Clustering: Comparison 

- 최종 cluster는 동일
- 중간 cluster에서는 계산 방식이 다르기 때문에 다른 형태

![36](/assets/img/2023-06-02-12.-Clustering-(1).md/36.png)


# Divisive hierarchical clustering


Top-down : global (optimal)

- 한 번에 하나의 cluster를 split : which to divide
- calculation 많아짐

Bottom-up : local (heuristic, greedy)

- 개별 data를 각각의 cluster -> 점차 Merge하며 하나의cluster
- 

Divisive top down

- detailed cluster로 split recursively
- 사용할 dimension, variable의 개수 정도

monotheistic : 한 번에 한 개


polytheistic : 여러 dim/var을 한 번에


(Univariate / multivariate)

- distance measure metric : 수치 등의 기준

1 : n-1 (n)/ 2: n-2 (nC2)/ … 경우의 수를 따져볼 수 있고 최종적으로 하나의 Cluster

- 가장 적합한 split을 선택
- 모든 경우의수 따져보면 bottom up보다 더 많이 연산을 요구할 것인지는 차이가 있을 수 있음
- 일반적으로 agglomerative bottom-up방식 사용 : dendrogram방식으로 다양한 단계에서의 Clustering을 수행하며 정보를 얻고 취합하여 최종 Cluster 결과 선정 가능
- 현재 상태의 cluster에서 가장 가짜운 Cluster과 Merge : optimal보다는 heuristic, Greedy (전체 objective를 최적화하는 global의 개념은 아님-너무 연산 많아짐)
- Start with a single cluster composed of all data points
- Split this into components
- Continue recursively
- Monothetic divisive methods split clusters using one variable/dimension at a time
- Polythetic divisive methods make splits on the basis of all variables together
- Any intercluster distance measure can be used
- Computationally intensive, less widely used than agglomerative methods

![37](/assets/img/2023-06-02-12.-Clustering-(1).md/37.png)

- Initially, all points in the dataset belong to one single cluster
- Partition the cluster into two least similar cluster
- Proceed recursively to form new clusters until the desired number of clusters is obtained Clustering Divisive hierarchical 53

![38](/assets/img/2023-06-02-12.-Clustering-(1).md/38.png)

- Check the sum of squared errors of each cluster and choose the one with the largest value.
- In the dataset below, the data points are separated into 2 clusters
- For further separating it to form the 3rd cluster, find the sum of squared errors (SSE) for each cluster
- The cluster with the largest SSE value is separated into 2 clusters

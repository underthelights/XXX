---
layout: post
date: 2023-05-10
title: "7. k-Nearest Neighbor"
tags: [ML, ]
categories: [Notes, ]
use_math: true
---


# k-Nearest Neighbor Classifier

- perceptron : linear classifier
- SVM : linear, nonlinear classifier
	- given data → 분류하는 hyperplane를 구한다.
	- GD로 계산한다 : iteration 수천~수백 epoch 연산을 수행한다
	- (1) phi fn으로 고차원 mapping data 처리 가능
		- kernel trick ($\epsilon$→0)
	- (2) soft margin

![0](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/0.png)

- Store all training samples $(x_i, y_i)$ - 모든 데이터 저장
	- 학습 data 저장하면 끝이다 (extra data task 불필요)
- Nearest Neighbor classifier
	- Given a test sample $x_t$, locate the nearest training sample $(x_{n1}, y_{n1})$,
	- then assign $y_{n1}$as the class label of $x_t$
	- 
- k-Nearest Neighbor classifier
	- Given a test sample $x_t$, locate the k-nearest training samples $(x_{n1}, y_{n1}), ...,(x_{nk}, y_{nk}) $ then assign **majority class label** of $y_{n1}, ... y_{nk}$ to $x_t$
		- **majority class label**  : majority voting, nearst sample labeling algorithm
	- Take mean of , if they are real valued $\hat{f} = \frac 1 k \Sigma_{i=1}^{k} f(x_{ni})$
	- Also called as **Instance** based learning

![1](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/1.png)

- test 시점에서 data 들어오면 해당 data와 near sample 판단 : **sample간 거리 계산**
	- $L^2$-norm : $\sqrt{\Sigma_{d=1}^{D} |x_{td} - x_{nd}|^2}$
	- test data에 대해 거리를 계산할 때 test sample과 near한지 distance 계산한다.
- sample dimension이 증가하면 → calculation cost 증가
	- 모든 sample들끼리 거리를 계산하면 - dimension 증가되면

	(해결) kNN Optimization

		- 학습 시 계산 적게 storage 크게 : 어떻게 save하는가?
		- test 시 계산 많이 :  어떻게 줄이는가?
- When to consider
	- Vector features
	- ~Less than 20 attributes (=20 dim)
	- Sufficient amount of training data

![2](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/2.png)

- **Advantages**
	- <u>**Training is very fast :**</u> feature extraction & save
	- Learn complex target functions
	- Don’t lose information
- **Disadvantages**
	- Slow at query time ( ⇒ test time)
		- **train 에서의 연산이 많은 것이 좋을까?** 아니면 test 에서의 연산이 많은 것이 좋을까?
			- test하기 전 enough time으로 저장 (sample 그대로 저장)
			- train offline / test online,offline
			- **so→ test 시점 연산이 적은 것이 유리함**
	- Requires large storage
	- Not robust against irrelevant **attributes or outliers**
		- SVM, Perceptron : boundary에 크게 영향이 없음
		- kNN : test sampel decision에 변화 가능함

# Distance Metric

- Search operation is expensive with high dimensions

## Distance


![3](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/3.png)

- 모든 classify / regress에서 중요함
	- decision tree : nominal data → distance 불필요
	- 각 feature 값들이 서로 다른 w를 가질 수 있음
		- ex. age, height
- distance = 1/similarity

![4](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/4.png)


feature vector하나 dimension은 의미가 없으며, 전체가 모여 압축된 형태로 표현된다.


## equations of selected distance functions


![5](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/5.png)

- 

## Value difference metric (VDM)

- attribute class 통해 판단 → 동일 class 속하는 것이 관찰되면 attribute가 더 가깝다고 본다.
	- ex. R-B가 더 많이 같은 class를 가지고 있으면 → R-B > R-G
- Providing distance measurements for nominal attributes
	- $vdm_a(x,y) = \Sigma_{c=1}^C (\frac{N_{a,x,c}}{N_{a,x}}-\frac{N_{a,y,c}}{N_{a,y}})^2$
	- 𝑵𝒂,𝒙 : # times attribute a had value x
	𝑵𝒂,𝒙,𝒄: # times attribute a had value x and class was c
	𝑪 : # output classes
	- a : color / x : big / y : medium, small ??
- Two values are considered closer if they have more similar classifications, i.e., if they have more similar correlations with the output classes

# Problem with Euclidean distance

- High dimensional data
	- dimension : color, length, weight…등의 data attribute
	- 더 많은 attribute를 추가할수록 performance 상승
	- 그러나 너무 많이 feature/ attribute 증가시키면 dim이 너무 많이 증가하여 curse of dimensionality에 빠질 수 있다
		- data가 고차원일수록 취급이 어려움
		data에 관련하여 알아야 할 정보가 많은데 잘 활용치 못하면 classifier 성능 저하됨
	- (Number of samples - available info) 양이 많고 + 정확한 정보라면 dimension이 높아도 괜찮다.
- Can produce counter-intuitive results
- Shrinking density – sparsification effect
	- data dimension을 낮추어 처리하면 data가 sparse한 형태
- d가 같다고 data feature를 잘 반영하는지에 대해서는 잘 고려해봐야 함
	- 각 feature dimesion이 data significance를 나타냄 → feature거리를 측정하는 것이 유사성과 차이점을 덜 반영할 수도 있음
	- binary feature : 과연 d가 동일한 값이라고 data feature를 잘 나타내는 것일까?
	- 1 1 1 1 1 1 1 1 1 1 1 0
	0 1 1 1 1 1 1 1 1 1 1 1
	d = 1.4142
	- 1 0 0 0 0 0 0 0 0 0 0 0
	0 0 0 0 0 0 0 0 0 0 0 1
	d = 1.4142
- hamming distance
	- 각 bit 사이 같은/다른 bit 나타나는지 판단
- histogram intersection
	- $\Sigma_i \min(h_{1i}, h_{2i})$

		![6](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/6.png)


# Behavior in the limit

- 성능
	- kNN ($\epsilon$ up) < optimal classifier ($\epsilon$ down)
- $\epsilon^*(x) $ **:** Error of optimal prediction
- $\epsilon_{NN}(x) $ : Error of nearaest neighbor

Theorem : 


$$
\lim_{n\rightarrow \inf} \leq 2\epsilon^*
$$

- proof:
	- $p_+$ : data가 (+)일 확률, $p_{NN\in(-)}$ : nearest neighbor이 (-)일 확률
	- $\epsilon_{NN} = p_+p_{NN\in(-)}+p_-p_{NN\in(+)} = p_+(1-p_{NN\in(+) })+(1-p_+)p_{NN\in(+)} $
		- $\epsilon_{NN} \sim\epsilon_{+} $ : Nearest Neighbor과 optimal한 sample의 결과가 동일하다
	- $\lim_{n \rightarrow \infty} p_{NN\in (+)} = p_+$$lim_{n \rightarrow \inf} p_{NN\in (+)} = p_+$
	- $\lim_{n \rightarrow \inf} p_{NN\in (-)} = p_-$
	- $\lim_{n \rightarrow \inf} \epsilon_{NN} = p_+(1-p_+) + (1-p_+)p_+ = 2p_+(1-p_+) = 2\epsilon^*(1-\epsilon^*)\leq2 \epsilon^*$
		- $2p_+(1-p_+)$
			- prediction : (+) or (-)
			- $p_+$가 맞으면 $(1-\epsilon^*)$ 맞고 $\epsilon^*$ 틀림
			- $p_+$가 맞으면 $(1-\epsilon^*)$ 틀리고 $\epsilon^*$ 맞음
		- $\epsilon^* \in [0,1]$
	- NN Classifier can have up to twice as much error of **optical error**
		- = Bayesian Classifier
		- Sampling할 때 특정 d보고 sample → 전체 population know
		- → 전체 population 정보가지고 classifier를 생성하면 classifier error : optimal classification error이고
		- 우리가 얻을 최소의 error → optimal classifier
- Theorem :

	$$
	\lim_{n\rightarrow \infty, k \rightarrow \infty, \frac k n \rightarrow 0 } \leq \epsilon^*
	$$


# Standardization

- Transform raw feature values into z-scores $z_{ij} = \frac{x_{ij} - \mu_j}{\sigma_j}$
	- $x_{ij}$ is the ith sample and jth feature (dimension)
	- $\mu_j$ is the average of all for feature
	- $\sigma_j$ is the standard deviation of all for feature

![7](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/7.png)


# Efficient searching

- sample 주변을 확인해야 함 : 어떠한 data 주변에 존재할지
- KD trees
	- n개 KD Tree 구성하면 - $\epsilon \downarrow$ : 각 class score, $w_{sum}$계산

		![8](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/8.png)

- Choose dimension
- Choose pivot (median)
- Split data, repeat
- 

![9](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/9.png)


![10](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/10.png)


![11](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/11.png)


# Choosing k

- Choosing the value of k:
	- If k is too small, sensitive to noise points
	- If k is too large, neighborhood may include points from other classes
- Rule of thumb:
k = sqrt(N)
N: number of training samples
- Use N fold cross validation – Pick k to minimize the cross validation error
- For each of N test example
	- Find its k nearest neighbors
	- Make a classification based on these k neighbors
	- Calculate classification error
	- Output average error over all examples
- Use the k that gives lowest average error on the training data
	- 이론적으로 $k \uparrow$이면 smoother해짐

![12](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/12.png)

- Bayes-optimal boundary given true generating model
	- ideal case : $k, n \rightarrow \infty$, $\frac k n \rightarrow 0$, $\epsilon^* \leq \epsilon_{kNN}\leq2\epsilon^*$

	$$
	\lim_{n\rightarrow \infty, k \rightarrow \infty, \frac k n \rightarrow 0 } \leq \epsilon^*
	$$


	![13](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/13.png)

- As number of training <u>samples</u> $\rightarrow \infty$, and k becomes large, k-Nearest Neighbor classifier shows performance as good as that of Bayes classifier

# Cross-validation


![14](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/14.png)

- train error가 낮아지는게 무작정 좋지는 않을 수 있다 (overfitting)

# Condensing

- Aim is to reduce the number of training samples

	더 작은 dataset으로의 압축

- Retain only the samples that are needed to define the decision boundary
- Decision Boundary Consistent – a **subset** whose nearest neighbor decision boundary is identical to the boundary of the entire training set
	- 전체 original data와 동일한 boundary 구해짐
- Minimum Consistent Set – the **smallest subset** of the training data that correctly classifies all of the original training data
	- 가장 작은 Sample sel
- Original data

	![15](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/15.png)

	- decision boundary 구성 Sample만을 남겨 이 data만을 남긴다.
	- 2,3개 씩 저장해도 boundary가 유지된다.
	- 전체 Dataset을 활용해도 decision boundary는 동일하게 구해진다.
- Condensed data

	![16](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/16.png)

- Minimum Consistent Set

	![17](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/17.png)


## Condensed Nearest Neighbor (CNN)

- iteration 최소 data로 errorless 구하기??
1. Initialize subset with a single (or k) training example
	1. 하나의 Data를 임의로 추출
2. Classify all remaining samples using the subset, and transfer any incorrectly classified samples to the other subset
	1. 그 Data를 이용하여 NN → $\epsilon$이 나오는 임의의 Data 선택
3. Return to 2 until no transfers occurred or the subset is full
	1. data, error 구하기

![18](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/18.png)


→ 초기 Data 선택에 따라 decision boundary 변화

- kNN → decision boundary
	- 어떤 group에 test data 속하는지 확인

# Condensation

- Each cell contains one sample, and every location within the cell is closer to that sample than to any other sample.
- A **Voronoi diagram** divides the space into such cells.
	- 구획으로 나누고 boundary에 영향 없는 Sample을 제거하는 방향
- Every query point will be assigned the classification of the sample within that cell. The decision boundary separates the class regions based on the 1-NN decision rule.
- Knowledge of this boundary is sufficient to classify new points.
- The boundary itself is rarely computed; many algorithms seek to retain only those points necessary to generate an identical boundary.

![19](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/19.png)

- data 주어지면 그로부터 얻는 1NN boundary가 유일하게 결정됨
	- → train data 전부 저장하지 않아도 boundary만 저장하면 됨

## Voronoi Diagram 구성하는 방법


![20](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/20.png)

- **Delaunay triangulation (Delone triangulation)** for a given set P of discrete points is a triangulation DT(P) such that no point in P is inside the **circumcircle** of any triangle in DT(P)
- Circumcircle (circumscribed circle) of a polygon is a circle that passes through all the vertices of the polygon
- 어떠한 점도 해당 Circle 내 들어가지 않도록, 다각형을 둘러싼 원 형성
	- →세 점을 둘러싼 원, 원 내부에 주어진 점을 포함하면 안 되는 형태로
- Avoid sliver triangles. (maximize angles of triangles) 
예각삼각형을 피하고, 각도 최대화
	- Delaunay triangulation is not unique

		![21](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/21.png)

- Circumcenters of Delaunay triangles are the vertices of the Voronoi diagram
- If two triangles share an edge in the Delaunay triangulation, their circumcenters are to be connected with an edge in the Voronoi tesselation
- 방법 : delaunary triangle 만들고 → circumcircle의 가운뎃점 pt → centered point 연결
- Voronoi Diagram의 최종 목표 : 모든 영역 안에 임의의 지점은 그 안의 Sample point와 최단 거리를 갖는 영역으로 정의함

![22](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/22.png)


![23](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/23.png)


![24](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/24.png)

- 각 sample point의 class에 따라, 전체 영역의 class 결정

![25](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/25.png)

- test Data 들어오면, 이를 포함한 voronoi diagram이면 class 결정

![26](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/26.png)


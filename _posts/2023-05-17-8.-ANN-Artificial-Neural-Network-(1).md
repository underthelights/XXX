---
layout: post
date: 2023-05-17
title: "8. ANN Artificial Neural Network (1)"
tags: [ML, ]
categories: [Notes, ]
use_math: true
---


# ANN Introduction


![0](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/0.png)_h+ p ://cs231n.stanford.edu/slides/winter1516_lecture4.pdf_

- 

![1](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/1.png)_© 2017, SNU BioIntelligence Lab, h+ p ://bi.snu.ac.kr/_

- 
- Classification with a line, $y = ax + b$
	- single, multi layer perceptron with NN

	![2](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/2.png)


	![3](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/3.png)

- Generalization

	![4](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/4.png)


![5](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/5.png)


## Artificial Neural Networks (2 layers) 

- NN 기본 구조 perceptron → hidden layer 쌓아 DNN
- Two-layer network
	- input layer + output layer (2)

	![6](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/6.png)

	- sigmoid 형태
	- no hidden layer → linearly separable prob only

![7](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/7.png)


## AND


![8](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/8.png)


![9](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/9.png)


![10](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/10.png)


## OR



![11](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/11.png)


![12](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/12.png)


![13](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/13.png)


## XOR


![14](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/14.png)


![15](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/15.png)

- Two layer network cannot implement XOR.
	- perceptron 으로 분류 시 error가 크게 나타남
- Non-linear
	- two decision boundary (nonlinear)

	![16](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/16.png)


	![17](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/17.png)


# Multi-layer Networks


![18](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/18.png)

- input - hidden - output
- hidden layer 개수 → 몇 개인지에 따라 network 구조가 좌우됨
	- linearly nonsolvable 문제들도 풀 수 있게 된다

![19](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/19.png)


![20](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/20.png)


![21](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/21.png)

- hidden layer 1개 추가 → 많은 가중치 w들을 학습한다
- 수백개 hidden layer →  수천, 수만 w 학습

![22](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/22.png)


![23](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/23.png)


![24](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/24.png)

- $wx = \Sigma_i w_i x_i$

![25](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/25.png)

- feature space : input layer → hidden layer
- SVM과 유사한 효과 : high dimension
	- 원래 feature space: 원래는 not linearly separable하지만 high dimension phi fn을 활용
	- 

![26](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/26.png)


![27](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/27.png)

- hidden layer를 추가하면 → 복잡한 boundary를 구할 수 있다

![28](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/28.png)


# ANN Training

- ANN training?
	- Estimate w by using training data

	![29](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/29.png)


	|               | training                                                                            |
	| ------------- | ----------------------------------------------------------------------------------- |
	| perceptron    | w                                                                                   |
	| SVM           | w : 
	- margin, slack var, Phi fn                                                    |
	| Bayesian      | G(mu, sigma^2) : prior
	- density modeling → likelihood
	
	- prior P → post P          |
	| Decision Tree | Tree 구조                                                                             |
	| kNN           | save train data
	- k value : train통해 결정
	- dinstance metric  : 
	L1 vs L2 vs Euclidean |
	| ANN           | w
	NN Structure
	- I/H/O Layer개수                                                      |
	| DNN           |                                                                                     |
	|               |                                                                                     |

undefined	- test data 분류 작업
	- regularizer
	- hyperparameter normalize
- By experiment ← train 중 parameter 결정
- data  : multimedia

## 1. Decide input layer’s node number

- By experiments, use domain knowledge

![30](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/30.png)


## 2. Decide output layer’s node number

- By experiments, use domain knowledge

![31](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/31.png)


![32](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/32.png)

- 구조만 보면 class가 몇 개인지 모른다
- class 수 = node 수
	- classification : output layer node의 activation 여부 확인

## 3. Decide hidden layer’s node number

- By experiments, use domain knowledge

![33](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/33.png)


## 4. Find weight using training algorithm

- Use back-propagation algorithm. Supervised learning.

![34](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/34.png)


# Back-propagation Algorithm

- 등장 배경 : NN → SVM →DNN
	- image가 낮인지 밤인지 예견하는 문제
	- data를 잘 준비했어야 하는데 overfitting을 해결하지 못하여 SVM이 1990-2010년도 성행
- train with back propagation algorithm
	- w값 arbitraily initialize → feed forward :
		- <u>**w :**</u> $\epsilon \downarrow$<u>**방향으로 update**</u>
		- between prediction , ground truth value
- $y = f (\Sigma_i x_i w_{ij} + b)$
	- network의 구조가 결정되면 다음 layer의 node값에 bias를 더하는 함수
	- perceptron 문제와 유사하게 발생 :
		- local minima
		- many w → T 증가
		- iterative하게 w 구하는 과정

	![35](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/35.png)


	모든 edge에 할당된 w에 대하여 backpropagation


![36](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/36.png)

- w update하면 iteration 반복 많아짐
- hidden 많아지면 update bigger
- $E = \Sigma_k \frac 1 2 (z_k - t_k)^2$

	tk : ground truth

	- 제곱을 하고 2로 나누어준 이유 : perceptron처럼, 미분을 위해 제곱하고 1/2 *2하면 표현식이 간단해진다
	- error 미분 : wij, wji 미분하여 update

chain rule

- $\frac{\partial E}{\partial w_{kj}} = \frac{\partial E}{\partial z_{k}} \frac{\partial z_k}{\partial w_{kj}}  $
- $\frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial z_{k}} \frac{\partial z_k}{\partial w_{ji}}  = \frac{\partial E}{\partial z_{k}} \frac{\partial z_k}{\partial y_j}\frac{\partial y_j}{\partial w_{ji}}  $

![37](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/37.png)


![38](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/38.png)


![39](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/39.png)


![40](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/40.png)


![41](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/41.png)

- big NN의 한계 : hidden layer 제한 → 다양한 성능 제한
- **hidden layer수 증가 감소 이유 :???**
	- output에서 input으로 backpropagate할 수록 더 많은 chain rule (Differentiation) 수행 → update양 감소
- To update weights far from the output layer, more nodes and their derivatives are involved. Due to the chain rule, the amount of update becomes smaller.

## Activation function: sigmoid


perceptron, SVM

- input data (weight) - f(x) update
	- 동일한 input에 대해 error 계산
	- error가 각 iteration별로 작아짐
- input data 바꾸면 - updated w도 new input data에 부적절할수도 있으니 fix해야 한다

![42](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/42.png)

- 부적합
- SVM에서, nondifferentiable fn : hinge loss → 구간 나누어 미분 → 복잡해지는 미분 계산 과정 itreation
	- 수학적으로 옳지는 않다??

![43](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/43.png)


![44](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/44.png)

- back propagation example
	- Ex) 1st error = 0.298371109 
	2nd error = 0.291027924
	...
	...
	10000th error = 0.000035085
	- output neurons after 10000th iteration : 
	0.015912196 (vs 0.01 target) 
	0.984065734 (vs 0.99 target)

![45](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/45.png)

- error : 역전파하면서 error 감소
	- 10000th로 갈수록 0으로 error 수렴
- output neuron : target에 근사

# Deep Neural Network


![46](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/46.png)

- raw data in real world : 영상 획득, microphone 녹음 등 ..

DNN : Number of hidden layers 

- # of hidden layers $\leq 1$ → Shallow Neural Network
- # of hidden layers $\geq 2$ → Deep Neural Network

![47](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/47.png)


## DNN Training


### Vanishing gradient problem


Problem with non-linear activation

- As errors are back propagated, the gradient vanishes
- Derivatives of sigmoid are 0~0.25. As these are multiplied at multiple layers, they become smaller and smaller
- $w_{ji} = w_{ij} -\eta \frac {\partial E}{\partial w_{ji}}$: as $\frac {\partial E}{\partial w_{ji}}$becomes smaller, $w$ is not well updated, especially for the layers far from the output layer

![48](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/48.png)

- Typically requires lots of labeled data
	- Collecting data is time consuming and expensive (time, price)
	- but time 소모 증가, 비용 증가
		- data collection, tagging, labeling
	- 2010년도 DNN의 발전배경  :
		- Big data → 수집 용이
		- GPU (HW Support)
		- Algorithm적 발달
- <u>_**Overfitting problem**_</u>
	- When training data is not sufficient, model only can handle the training data well (poor performance at test time)
	- data 변화 요인에 w 학습하고 , 다양한 test에 대해서도 적용

	![49](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/49.png)

- ground truth sampling → data에 fit하여 정답과 일치하도록
- overfitting : regression, classification 시 정답을 보지 않아도 train well됨을 확인함
- Get stuck in **local minima : iteration시 유의**
	- Problem even with enough training data
	- solution : 초기값을 randomly setting하고 train을 많이 하여 그 avearge를 추출한다
		- test set에 대해 일반적으로 성능 좋아짐
- Train Set 에서의 문
	- input data : raw data→ input layer : x1, x2
	- state of the nature $\in$ real world
	- 학습 data로 model을 만들지만 prediction-ground truth가 0이라고 하여도 좋은 model인지 아닌지 확신할 수 없다.
		- 사실 정답을 알 리가 없다 : 그 정답으로 추출된 제한된 관찰에 의해 얻어진 학습 data로 model을 구한 것이기에
	- sol **→ train data를 변화시키며 model이 얼마나 안정된 결과를 보이는지 (test data를 변화시키기도 해보자)**
- Vanishing gradient problem
	- ReLU(Rectified Linear Unit)
		- $\max(0,x)$ gradient update
	- Layer-wise training
		- 충분히 학습되었다면 건너뜀

			![50](/assets/img/2023-05-17-8.-ANN-Artificial-Neural-Network-(1).md/50.png)

- Requires **lots** of labeled data
- **Overfitting** problem
- Get stuck in **local minima**
	- Mitigated by increasing data and computation power
- 시간 소모 :
	- depend on performance of HW
	- w update : vanishing gradient → activation fn 변화
